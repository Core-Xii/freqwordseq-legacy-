
Printable Version of Topic
Click here to view this topic in its original format
Game Maker Community _ Expert Users _ Creating Ai That Learns


Creating AI that learns from interactions with whomever is playing the game it is in. It sort of works such as a chat bot, taking information that the user has inputted and using it in responses.

Chat Bot Ex.:

AI: What is your name?
You: Chris
AI: Hi Chris!

However simple this may seem, I believe it could be applied to a game such as chess in order to learn opponent's strategies. When playing a game a of chess, the computer evaluates it's opponent's moved based SOLELY on what it's creator has programmed it to do. This, in short, is the downfall of chess AI. While a human opponent may learn it's mistakes, a chess board will continue making it.

But, let's say the AI is created in game maker, what if you create an external database in which all the AI's strategum's are confined. However, instead of being an isolated, unchangable map of what the computer can do, it would take in consideration each loss it encounters, by adding which move led to it's own checkmate (if course, still talking of chess, although this could be used in more advanced shooting games.)

For example, if the Queen, considered the most powerful of the chess pieces, is taken from our AI, but no other feasable moves could have been made during that turn, it must go back through it's moves (saved in the data file) and find which move led to it's queen's taking, and change the strategy in which it had to suit itself for the mistake.

I feel as though I should add I am somewhat of a chess connoisseur, and this is obviously very, very advanced, especially for most people who use game maker. However, the idea intrigued me after reading another post about creating standard chess AI (which is not as complicated as they made it seem, which I will explain later on in my post) and I decided that it could also be used not only for chess, but for top-down shooters, or if you are someone such as TGG, a master-if-nothing-else of the use of 3D in game maker, FPS (I will not mention 3rd person games, for they are quite complicated and not very developed in game maker as of now.)

Ways in which this may be implemented in games other than chess are:

-AI learning how the player fights.
-If a punch, kick, upper cut combo is used mutliple times, the AI may learn to block at a certain time and devise a counter-offensive.
-In FPS, if the player seems to aim for the head shot, learn to duck
-Any other practical explanation.

I dare not bore you with more examples, because I am sure if you have read this far you have many in mind.


Explaining simple chess strategies.


It would take a long time to devise your own hand crafted list of moves, but that is how it must be done. If a player moves a pawn to a certain point, and you have programmed in a counter-offensive (I say counter-offensive to illustrate the reasoning that "defense" in chess leads to your opponent controlling the board) that makes the AI move their piece to a certain position. Of course, though, then you would have to make a strategy for every single variation that a chess board could take on. Obviously this very layman's way of explaining an AI's chess board strategy, and it is so long it is impractical, which is why I suggest allowing the AI to build it's own database.

Allowing the AI to forumulate it's own database from scratch


As proposed before, the AI learns new strategies from the player it is playing against. However, in this situation, the computer starts out making random moves. This would take pathfinding, such as checking if a piece is in the way, and would be quite simple to create. Each time the computer inevitably loses, it stores it's game information in it's external database, and each time you replay the computer, it reloads it's cumulative strategy report, and uses it's information to devise strategies against you. Suppose you try and go for the 4-move check mate. (link for those who care, it's the top strategy: http://www.avlerchess.com/strategies.html) The computer would presumably be defeated in 4 turns provided it doesn't accidentally counter it. But, the next time you go to do this same thing, the Ai searches it's database, recognizes the moves, and relays it back to do something different. This would, of course, take along time to develop, due to the fact that there are so many different variations in the game of chess for each strategy. It's be much simpler to do this with Shooting games, due to the fact that it would be a lot less information to deal with.


You often hear people say that AI canâ€™t learn. I can understand how they would think this. When they think of an AI acting in such a way that it is learning (ie. if it always dies from head shots it starts ducking) then they think of a code such as:
CODE
if (deaths_from_head shot > 10) {script_duck_tactics()}

Which is often how this type of "learning" is done, and i agree; this is not learning.

However one must take a think about what learning, in humans actually is. Itâ€™s more or less recognizing and remembering patterns to control *cause and effect*. In my opinion true learning is not some magical holly grail of programming, its simply allowing the AI to recognize patterns, and the any *cause and effect* associated with it.

I would approach it like this:
Have the AI track EVERY variable that is possible for a human to recognise on their screen. These would be pre-programmed. I hear that "But your starting to program what the AI learns" bell ringing, but remember: a human playing the game can only watch and learn from what he sees on his/her screen, and what he/she sees on the screen is predefined (or programmed if you will) by the game makers, and yet, players still learn.
Variables that the AI could track could include the following for example.

Kills
Enemy position
Enemy weapon
My weapon
My position
ect...

Now in order to learn it needs to have a goal. (ie. maximise its score, minimise the enemyâ€™s score) Also you must pre-program small scale goals and small scale patterns, ones that a human would realize before even playing (ie the a human is programmed to know before it starts playing) such as to get allot of kills, you need to get lots of individual kills, or even more basic, you need to aim at enemy to kill them. Basically you make the bear bones of a standard "non learning" AI

Then at specific times, and say whenever it gets a kill, or some other definite *effect* it records the *cause* - the variables at that time. Over time it will eventually have recorded a lot of sets of variables for the one *cause*, at which time it can start to recognize patterns and similarity in variables at a specific *cause*

These would become its secondary level goals, to learn how to cause the variables to be at the values it has associated with desired *causes* and how to force the variables to avoid the values that they were at when undesired *causes* happen.

IE. The AI might recognize that when every time it dies in a fight, it always seems to be standing still - so it begins to try to not stand still in a fight.
or
It might realize that a high number of its deaths occur when it was at the top of the sniper tower - so it tries to avoid its position variables from equalling "top of sniper tower"

And for controlling the acquisition of these second level variables it would go though the same process of learning as it did for the first set and so forth, the amount of levels of deepness that it could be limited by you, and in a humans case is limited by how smart they are.

Then you just let the AI play its self, play and play and play. You could probably set the room speed of the game to really high, because it doesnâ€™t effect AI. It would be an interesting process to watch.

Of course this would take an extremely long amount of time unless you help it out a bit. You could let it know how to acquire the variables it wants.
ie. say it learns that it wants to be holding a certain weapon (it wants weapon_type=5 or whatever) you could give let it know that this weapon spawns at this point on the map, and lets its path finding take care of the rest. It could learn this from guess and check by "pressing" the pick up weapon button everywhere until it learns were it spawns but you can speed it up.

Of course you would have to program a bit of randomness into its actions (or human error) so it tries new things, and decides if their out comes are good or not.

Once the AI is good enough itâ€™s ready for the game, but you would of course leave all its learning routines going.

The examples I have written here are for say an advanced FPS, so it would be a much simpler process for a simpler game. I might make an example of an extremely simple soccer game with AI that learns the way I have been describing.



The idea itself is pretty promising, took me a few minutes to come up with an actual concept (fitting for a shooter for example)...

Normal AI works it's way around a map via waypoints, players do it different: They get to know the map and find their personal favorite and weak spots in a map through time. An AI is able to do this, too. My idea is a data grid for that.

Example of a map:

#####
#####
#####
#####
#####

Now we assign every place a value, to keep it simple I'd say we don't assign every coordinate a single value, instead we use one value for let's say each 32x32 tile.

Okay, we begin with a neutral value for every spot in the grid. Let's say 100 means "perfect place to go", 0 means "stay the **** away". So on a new map, we start with 50 for each, okay?

Now if the AI is getting hurt or dies on a certain spot, we decrease its value in the grid. If it doesn't get hurt, we increase the value. Also, if we pick up a bonus item or something, we increase the value at that spot (item spawn points are generally useful, we see people flocking the quad damage and rocket launcher spots in Quake3 all the time, eh?)

So if the AI plays for a while, it will find good and bad spots on the map and change them dynamically when players use other strategies.

Example:

#####
#####
#####
#####
#####


(One might want to set solid areas like walls to 0 from the beginning on to keep the AI from running into walls. Heh.)



Greetz, Atlantis


AI for a RTS game could also be something like:

if the number of player_infantry>player_antitank
create 10 tanks

or

if player anti air nnext to factroy<5
send 3 bombers to bomb factory


(of course this isnt code, i just didnt feel like spending the time to write actual code)



AI for a RTS game could also be something like:

if the number of player_infantry>player_antitank
create 10 tanks

or

if player anti air nnext to factroy<5
send 3 bombers to bomb factory


(of course this isnt code,  i just didnt feel like spending the time to write actual code)
*


No, that'd again be limited. It only reacts with a certain, preprogrammed behavior and that is what we wanted to avoid.



Greetz, Atlantis



yes what they are saying is to make a way so that the computer can solve its own probelms by examining where it went wrong the first time...not just programming specified actions based on the player's.


hmm, then maybe like for a turn based strategy game:

if units killed in last move>5
{dont repeat last move}

is that what you mean?


something along those lines, yet more complicated...for example, in a chess game, suppose a knight starts with 2 options for movement (he can go 1 of two places) and either place he goes will open up say 2 more possible movements. Well, say he gets taken out of the game, well the AI would analyze what moves led to the knights capture. Now, the AI, the next time it encounters a similar situation, will respond differently (a.k.a. learning from its mistakes). It's something that not even the programmer can 100% accurately anticipate. Whereas what you (Finland Games) are suggesting the programmer will know for sure what the AI will do. And this is why it becomes complicated...



The idea itself is pretty promising, took me a few minutes to come up with an actual concept (fitting for a shooter for example)...

Normal AI works it's way around a map via waypoints, players do it different: They get to know the map and find their personal favorite and weak spots in a map through time. An AI is able to do this, too. My idea is a data grid for that.

Example of a map:

#####
#####
#####
#####
#####

Now we assign every place a value, to keep it simple I'd say we don't assign every coordinate a single value, instead we use one value for let's say each 32x32 tile.

Okay, we begin with a neutral value for every spot in the grid. Let's say 100 means "perfect place to go", 0 means "stay the **** away". So on a new map, we start with 50 for each, okay?

Now if the AI is getting hurt or dies on a certain spot, we decrease its value in the grid. If it doesn't get hurt, we increase the value. Also, if we pick up a bonus item or something, we increase the value at that spot (item spawn points are generally useful, we see people flocking the quad damage and rocket launcher spots in Quake3 all the time, eh?)

So if the AI plays for a while, it will find good and bad spots on the map and change them dynamically when players use other strategies.

Example:

#####
#####
#####
#####
#####


Greetz, Atlantis
*


That is actuallly a great idea. However, it should be modified quite a bit still.
*If it notices that it kills alot of people in one spot, it's value should go up.
*If it notices that other people are having good luck in a spot, it should go up.
*If something goes up, the area around it should go up slightly.
*It should keep track of where the player is usually. If the player always goes in a spot, and the enemy finds a spot with a good shot on that area, that place should go up alot.
I think that you're idea could be very expanded upon, Atlantis
, and I think someone could go far with it.



If you truly wanted an AI that could learn, it would have to create and modify its own code. This could be done using external files, and execute string, but it seems quite difficult to accomplish.


Yes, exactly Atlantis_base. Thatâ€™s a good way of implementing it.

Can some one make a basic top down multiplayer shooter, that we can try to implement this type of AI onto?




If you truly wanted an AI that could learn, it would have to create and modify its own code. This could be done using external files, and execute string
*


Or, you could use add_event for user defined events, and then when you run out of room in those, you could save events are variables and keep having them replace each other and stuff.

TO TRANSLATE MY RAMBLE:

Ok, so you know add_event or whatever functions, right? Well, there are user defined events, 15 of them. Then, you can just have the AI create code for these events. But, let's say you get over 15 reactions that the AI has built, right? Well, it saves number 15 as the variable userdef15. The next as userdef16, and so on. Then, when use event 15 is called, you need to set userdefevent to 15, then it makes user defined event 15 to the variable userdef15, then executes it, then changes it back. Get it? It's confusing, but, with lots and lots of work, but it's possible to impliment this with the system that atlantis thought of and make awesome AI



I dont understand the use of having new code written in game would help, or be a prerequisite to learning. Remember, were not creating Frankensteinâ€™s monster, just something that can adapt. I just think it save infomation to a file containing what its learned. And this is added to and updated every time it plays.

A basic version of that file might look something like this.

Kills in grid square------Kills with weapon type
1,1 : 5 ------------------ 1 : 5
1,2 : 3 -------------------2 : 4
1,3 : 5 -------------------3 : 16
1,4 : 6 -------------------4 : 9
1,5 : 26 ------------------5 : 20
1,6 : 24
2,1 : 3
2,2 : 1
2,3 : 4
2,4 : 6
2,5 : 20
2,6 : 26 -------------------ect...

What we need is a really simple multiplayer game that some one has allready made, that we can try and write learning AI for.


Creating this kind of AI for a computer is not easy, since we all agree that the computer will have to recognice paterns between Cause and event. The human brain is a true master in that and computers tend to have problems linking 2 actions together perfectly. But i've seen some good concepts here and it all looks promising.

Its reminded me of a documentary i saw about the futere of the world (lots of SF) but there was a part about some programmers who made a program with animals created out of blocks. The animals would then learn by themselves how to move and could evolve into later generations. They made a lillte game from it so the animals where put to test against eachother to first capture a block that was in the middle. After a few generations something spectacular turned oud, suddenly one of the animals atacked the other animal before they even reached the block; it had completely created this behavior out of scratch without any preprogramming. The programmers were astonished since they never anticipated how effeciently the animal had reprogrammed itself just like the human brain. If only we could achieve that here biggrin.gif

When we would achieve something we could always test it eaily: Let 2 computer chars compete against eachother. If the system works good they should tie after a while smile.gif

Edit: Made my post somewhat nicer and added something (don't mind my spelling pls smile.gif)

Regards Hellavi


I had a thought a while back that was like this.

If you make an external file called memory or something then create the code to write the players move and the computers reaction then read the file and compare the reactions
in order to Â¨create Â¨ a new reaction by the computer.

pseudocode-if(computer_shoots)
{
//write to file comuter shoot code//
}
if (player_ducks)
{
//write to file player ducking code//
}
if(computer shooting code is executed a number of times)
{
//read the memory file//
}
//adjust the height/direction the computer shoots to react to the players
previous reactions//
}

well at least it looked good in my thoughts



I dont understand the use of having new code written in game would help, or be a prerequisite to learning. Remember, were not creating Frankensteinâ€™s monster, just something that can adapt. I just think it save infomation to a file containing what its learned. And this is added to and updated every time it plays.

A basic version of that file might look something like this.

Kills in grid square------Kills with weapon type
1,1 : 5 ------------------ 1 : 5
1,2 : 3 -------------------2 : 4
1,3 : 5 -------------------3 : 16
1,4 : 6 -------------------4 : 9
1,5 : 26 ------------------5 : 20
1,6 : 24
2,1 : 3
2,2 : 1
2,3 : 4
2,4 : 6
2,5 : 20
2,6 : 26 -------------------ect...

What we need is a really simple multiplayer game that some one has allready made, that we can try and write learning AI for.
*


If you were to give the AI the ability to write its own code in response to interactions, then it would truly be an AI that learns. It could remember what tends to work in its favor, then code a specific response to a situation. It would be much more flexible than the method you suggest, but also more difficult.



I agree with the "Cause and effect" method. We definatly want to avoid preprogrammed "avoid this if human does this". There is always a little glitch; and once a human finds the glitch, the computer knows no way how to avoid the effect.

MY EXAMPLE

Perhaps, before writing an AI base for an RTS for example, we would tell the computer what is good to do, and what is not. At first, the computer for an RTS game builds lots of useless buildings randomly or more than once. This is resource-eating and is something that the computer should avoid, unless very neccessary. Every move the human and computer does, is stored in an external database. Now let's say the human creates a huge strike force and crushes the computer. Of course, losing is a bad thing for the computer.

The computer now goes back into it's database and tries to find the cause for the bad effect it achieved. Perhaps if it saw that the human created a Training Hall building Terablasters to crush them, he might decide to go for that tactic instead. But to get the resources he needs, he needs to stop building useless buildings. He does so, but again the human crushes him, because the human had a bigger attack force.

The computer wonders again "What did I do wrong?" and searches back through the database the differences between how the human acted and how he did. He finds that the human used some of his resources to build Collectors in order to achieve more resources over time. Next game the computer tries this as well. He finds that this time he loses once again because he built too many Collectors instead of Terablasters. Next time, he tries a few less. So we get a balanced point somewhere.

When the human loses, and the computer thinks he has the ultimate strategy, the human changes tactic. The human monitors the computer's activity with some cheap Observers and finds he is building a force of Terablasters. The human knows that a equal force of Incemirators will easily kill the Terablasters. Suddenly, as the computer loses, he wonders what was the difference in the two player's actions. He finds that the Incemerators kill the Terablasters easily, and tries to achieve the same result as the player. Eventually the computer would come to a balanced point of excellent learning and strategy.

I think that this is the way a computer can learn how to handle a human.

FPS STRATICS

Ok, let's see here. Let's pretend that in our game there are two spots you can shoot at, either head or body. Head kills the player immidietly, while body might have to take about 20 shots before dying. Let's pretend we have three diffrent "players" in the game.

GAME MASTER A human that knows all the rules and tactics.
UNKNOWING HUMAN A human knowing nothing about the rules.
CPU The learning computer system, knowing no rules.

And the rules:

1. Headshot kills immidietly.
2. Bodyshots kill after 20 shots.
3. Ducking prevents headshots if such a shot is made.
4. Ducking while body shooting leads to a headshot tongue.gif

MATCH 1: UNKNOWING HUMAN VS. GAME MASTER
The master knows all rules and thus win the first matches easily. The unknowing human learns from his mistakes with a extremly powerful brain (compared to computer) and eventually might learn the tactics and win against the game master.

MATCH 2: CPU VS. GAME MASTER
Again, the CPU loses the first times against the master. Eventually he notices through his external database that the headshot is a very effective way of shooting. He begins to try to make all shots a headshot, but the Game Master knows that ducking will prevent that easily. As the computer loses again, because the vital shot misses, the computer will try to make headshots and duck if he is shot by a bullet himself. But if the game master notices that the CPU ducks all the time, he can simply aim at the body and get a headshot as so. Again, and again, the computer will learn the balances of strategy and analyzing.


jonah.schreiber, your ideas were pretty good, but I noticed that they were mainly about cpying the player move for move. I believe copying some of the player's actions is good, but there must be more to make an actual learning AI instead of a copycat. I'll try to make an example using some of the suggested methods.

FLS


I tried testing some of the suggestions in this thread. Sorry for the messy code and lack of comments, will improve them later.

http://www.geocities.com/snorrevestli/ai.zip

Move using up,down,left,right.
Quit using esc.

When you move, the AI moves at the same time, in a random direction determined by the color of the adjacent blocks. After moving, both you and the AI automaticaly shoot straight forward. After 10 hits you die, score beeing displayed in the top left corner. if the ai hits you, the square it is in gets one notch better, if you hit the ai, the square it is in gets one notch worse.
the game autosaves when you quit.

You can also manipulate the ai by clikcking with your right or left mouse button on any square.

edit: To add something to the debate, i think that if you add multiple "layers" of square preferance, for example relative to the player (including the players direction), or relative to itself, you could get more advanced and faster lerarning. i'll try to incorporate it into my model later.

edit2: i'll also look into making it two AIs instead of AI and player (two AIs with sepparate grids, of course).


Znorre, I'll try your example as soon as I get home. Writing at school.

Freelancer: I agree, although the computer is merely copying to understand how to react to a certain situation. The part which I describe in the RTS is simply the person making the game teaching the computer the basics of playing. Otherwise as you say, copying is good at some times but should be avoided at others to avoid symmetry.


I would like to post my first expert users post aaskull.gif
Well anyway I look at human learning as said here...observe things and remember things. Now some here seem to say that a program gotta reprogramme itself witch I don't agree with, as everyone has the basics imprinted from the time they are born, these are called reflexes. Reflexes could be simulated by making simple code that handles the keys, do this to move one up and such, then the AI would check what happens when it does a thing. A child copies its parents, why would it otherwise want to stand up?, so why wouldnt AI? tongue.gif

That little ramble was a effort to explain that I think a "copycat"-AI is very much a learning-AI.

Now for the thing I would want to add smile.gif
maybe planing made by the AI, meaning that it wont only react on the current situation but also want to try new things. Maybe not even have a ultimate goal that it strives for without any thought about new things, maybe consisting of new troop-combinations in RTS's. And then some factors of "panic" maybe when the AI is in a position of bad bad and bad move it would maybe try a more primitive approach.


Maybe have some kind of script for either experimenting or running. meaning that if a AI-animal would encounter snow for the first time it could first be afraid, try to dodge and stay away, fail and notice snow isn't of immediate danger, this would lead to the AI-Animal trying different things on the snow, maybe first checking for reactions of the snow when moved toward, touched attacked, shouted at, smell, warmth, each experiment the AI-animal would do would then filter through the old info (hmm this smells a litle like that other thing, that other thing tasted bad and I had a food poisoning out of it. This is bad-->stay away/be careful)
the experiment would also have safety values indicating what to try first, reflexes(pre-programing) could include dodging and attacking and then have more complicated variations added on basis of older info.
After the AI-Animal have noticed snow isn't dangerous for it it would more or less ignore it, but as soon as the snow would do something unexpected the experiment actions be triggered, but if the snow stops doing something unexpected the AI-Animal would get confused, witch would lead to some kind of awaiting for something to happen, if the thing is dangerous(that does the unexpected thing) the AI-animal would get scared rather than just confused and would run, attack or what ever goes with the AI-Animals personality.

So one would have a AI with pre-programed basic stuff, then the reactions would be based on observations of things from older games/events.
the pre-programed stuff would need to include basic stuff for every thing the AI can do, yes but The AI could advance learn and upgrade itself and its tactics. so basically you would give the AI a basic training, in moving, attacking, eating, teaching others(in AL tongue.gif). and so on



Honestly, we have to make a template for us to modify and play around with. I presume all posters here have a registered version of Game Maker, so that we can play around with the objects and functions.

I think that we should make a top-down template. It is the simplest way of controlling our in-time objects. Perhaps we could do as Erska suggests, an animal or something representing our main AI learner, and some objects it could find in the world. Such as snow wink1.gif

And let's remember to keep the AI as rooted as possible. We mustn't program every action the AI wants to take at a certain point. That would be almost impossible. For example:

CODE

if (killed_by_headshot > 10)
  {
   start_duck();
  }


Which is not rooted at all. We need to group all bad things and good things together, and either we preprogram what they are (eating dirty water is bad) or let the computer learn (I ate dirty water. It made me feel bad). This list needs to be put in an external database, from where it can be pulled up again. So lets say our AI animal finds some clear water. We might have something like this:

CODE

if (FoundObjectThroughDatabase("ext_database.lib",obj_water) == false)
  {
   // Experiment with the object
   WriteObjectToDatabase("ext_database.lib",obj_water));
   iResultFrighten = AttemptFrightenObject(obj_water);
   WriteStringToDatabase("ext_database.lib","obj_water frighten " + String(iResultFrighten));
   iResultSmell = AttemptSmellObject(obj_water);
   WriteStringToDatabase("ext_database.lib","obj_water smell " + String(iResultSmell));
   // And so on and so forth...
  }

It's a bad example, and not GML code (or any other code for that reason) but we might have to take this into account and see what results we can get.



Created an example using two AIs.
http://www.geocities.com/snorrevestli/AI_two_AIs.zip

change speed with enter.
switch ai view between players with space.

I left this example running for seven hours at max speed, resulting in this:

top player - lowest score
user posted image

bottom player - highest score
user posted image


What does it do? I see bouncing arrows go around and occasially leaving behind a brown spot... what is the AI in this?



What does it do? I see bouncing arrows go around and occasially leaving behind a brown spot... what is the AI in this?
And I quote: "I left this example running for seven hours". Why don't you release this as open source? This is an experts forum and in order for people to make better games, we need to see source code.



There is a player vs AI version on the first page, in case you are posting in a thread you havent read.

I will eventually release the source, but it's currently noncomented and full of sloppy/redundant code, and i dont want that pinned to my name.


I see now smile.gif

Not unexpected, for both players in all corners the color is completely red biggrin.gif


have you ever played quake? well search google for the "reaper bot". now that is some killer AI!!


Here is a link to read about it: Sounds pretty nice biggrin.gif http://www.bluesnews.com/guide/bots.htm


Yet another one! this time each square has four arrows pointing to its middle, representing how attractive it is to move into that square from that direction.




http://www.geocities.com/snorrevestli/ai_2_005.zip


Controls:
Enter: Change room speed. 1-9999
M: DisplayMode. Choose which players AI to display.
R: "Reward". How much the value of a move increases when a player hits its enemy.
P: "Punishment". How much the value of a move decreases when a player gets hit by its enemy.

I also noticed that a positive effect of this model is that when most of a players AI grid is red (because it's losing), every change matters more, thus ensuring faster evolution for pressed players.

Edit: if someone could leave one on with a low reward/punishment rate (like 1 or 2), and the report the results, that would be great.


Problem with all of your examples, znorre, is that they keep resetting themselves every few seconds. Not the grid, but the speed and learning settings. So I can't let it run with low setting, but I'll run it with standard settings for a few more minutes.



Greetz, Atlantis


I noticed the speed reset,but i have absolutely no idea what's causing it. Are you sure it also affects learning settings? Help would be appreciated.

Edit: I just realised that when the room restarts (after one of the players have been hit 10 times), the speed resets. However, theres no reason that should reset the learnspeed. are you sure it resets?

Edit2: Download updated with fix for speed reset.


I tried lowering the learning settings and it resets them a few steps afterwards. As long as reward and punishment are the same numbers, though, it doesn't matter which one it is.

EDIT: Results so far. They didn't change too much in the last few minutes so I decided to stop.

user posted image
Player 1

user posted image
Player 2



Greetz, Atlantis


Her is how I would do it (and I plan up some mean AI - Not that I have many games to put them in) and it has been proven to work.

First of all, you must set up your sensory input

Second, you've got to get some sort of memory - dosent loast long, just can hold some true/false values, an array even.

Then, set up a system that can create rules, based on maybe event_user, or a script that runs. Example:

Rule:

if sensor_left = 1 then blah blah blah.

The robot must have things that are bad (get hit by bullet, run into wall) in its programmed thing.

Each rule will then have a score.

The rules score will go down if something bad happens because of it (when bad thing happens, just see what rules got it into that bad position, and demote those rules's scores.)

The rules score will go up if the opposite occurs.

The rules need to be able to read and write to the memory, memory cannot stay around long, or the bot will get confused (so clear it every 5 steps or so)

After all that is in place, the bot just needs to check, every 30 seconds or so, the scores of the rules.

If a rules score is below a certain ammount (difficulty can maybe be set by increasing and decreasing this ammount) it is replaced by a new rule. The new rule will be a combination and a mutation of the top 2 rules.

Over time, the bad rules will be weeded out, and your bot will get increasingly better.

That is what to do (in my opinion)



The


Sparkworker, the idea that you put forward sounds a bit like AI equivalent of natural selection. Random rules are tried, and the good/successful ones are kept, and the no good ones are forgotten. I like it smile.gif

EDIT znorre, in your program, make your learning AI play against a non learning AI.


QUICK NOTE: My mate at school is a wiz at Delphi and stuff (went to a 'world youth programming olympics' or sumthing), and yesterday he was trying to make a fake virus with GML.
Ignoring common sense, he started throwing Delphi and (i think) C+ (or C# or one of them), and it worked!
Hes had to make AI and things in these other languages before, and it turned out rather good.

Edit: The relevance of this is that if you know another programming language that would work better for AI, then it wouldn't hurt to give it a shot.



he started throwing Delphi and (i think) C+ (or C# or one of them)

neh it was propl LISP or smalltalk....

*sigh*

learning is a very complex 'train of events' process.
For that reason the only accepted 'real-learning' in comp-science is with neural networks
(actually wrote betworks, but that would then be an semi intelligent casino..:)
Neural networks (NN) can be implemented as graphs
eg 'node-grids' where the indiviuel descission (on-node) are verry simple but the combined picture -have the posibillity- (!) to become complext (but dont need to)
The descission of going from node a to node n' is weiged in as egdes of the graph.. But! this needs to be a dynamic weight value
..like
if npc is situated at node h and the cost of going to node i is 10 then this cost should be altered to ..say 67 if the player or another npc placed a 'trap' on the path h-i
'our' npc would then , at pos node h , weigh the cost against the benefit og going from h-i or the longer h-j-w-i
(btw This algoritm is the backbone of routing protocols)
In theory at least this is feasable..
in practice we will find that a compext NN would get diffycult to manage rather fast. Another problem with gm is that we lack structs and as such nodes need to be full objects (overhead)
Again it IS posible because gm alows us to add -objects- in runtime
BUT!!
adding -objects- always come with a litle hickup
For that reason a NN i gm as such is interesting i theory, but not as an implemetation of a real game
...this post just got deprecated ;)

a shortcut would be to limit the conditions to a fixed number -then it could be posible to implement the individal nodes as -instances-
The tradeoff is that the initial system is limited and hence no real 'learning' is posible
..And learning was the threads topic.. ;)





To begin with i must say bearSoft is that Iâ€™m not sure I understood your post fully. Not to say that whatever you were saying was wrong, or that your typos were too distracting, just it was too hard for me. smile.gif I couldnâ€™t understand how the AI you talked about actually learned.

Also, Iâ€™m not sure what you said about levels of complexity but I believe the following. It doesnâ€™t matter how deep an AIs learning is, itâ€™s still strictly learning. So long as the programmer didnâ€™t directly program a learnt behaviour such as [if (death_from_headshot > 10) scipt_duck()] it is still learning. Also every aspect of the AI doesnâ€™t have to improve for it to have learnt, it call still do hell brainless moves, and have learnt regardless. Take for example znorre's program; the AI has no future past planning, no comprehension of the likeness of the enemies moves ectâ€¦ but in its learning.

As for our ideas following some master rule for 'real-learning' or whatever, i donâ€™t think it matters. WE are trying to make AI that for all concerned (the player of our future games) adapts and appears to learn how to behave in a given situation, with out us have directly and systematically programmed each situation before hand.



Ok, i made an AI using Sparkworkers surgestion.

gm6 file: http://www.rocketsoft.gm-school.uni.cc/uploads/AI_learn_ball.gm6

All the AI does is deciding what the speed of a ball is that bounces around the room. Now the playerâ€™s goal is to click on the ball, and the AI's goal to avoid being clicked. Now we already know that the smartest thing for the AI to do is make the ball go at its maximum aloud speed (which I set to 20). However, I have not pre-programmed into the AI any knowledge of this. When ever the player tries to click on it and misses (or the AI successfully avoids the player if you will), it records what speed it was going at the time onto the end of a growing array of its good memories. If the player clicks and hits (or the AI is unsuccessful in dodging) it doesnâ€™t remember a thing.

After the player clicks (wether they hit or missed) a new random speed is selected, and then averaged with all of its good memories, then another random element is added to give the next speed that it will test.

You can also manipulate the AI by holding space when you click to automatically hit it.

In theory, over time it will remember the faster speeds, and start producing faster speeds more regularly. So it has learnt to move fast.

Let me know what you think.


Good example biggrin.gif

Inspires me to do one of my own...

I'm thinking of doing a platformer where an object has to jump as far up in the air as possible.


THE_oldy: What i think would make that example better, is if you have the ball record other things too, like initial direction and movement patterns (You could for example have a var called curve, and change the direction by it each step). One of the most important aspects of learning ai is letting it select the best soloution even when you don't know what it is.


znorre: I wouldnâ€™t use this type of programming to program some thing so straight forwardly obvious as in the last example, but I wanted something really straight forward that we could predict the right outcome of the AI, to see if it worked.

Also I think it would be advantageous for only the newest say 10 remembered variables in the array to be used in the calculations, so if the players playing style changes, to AI doesnâ€™t take an eternity to adapt.

I think it will be straight forward to write a script that could handle for this type of learning for every variable that the AI needs to learn about, so I might write one.



Maybe instead of having just the ten newest memories count, i think we should have the older ones have less effect, like making "number ten" be the average of all the older tries.


oldy you cpuld make it the closer you get to cliking it it sould also take that to acount.

so if it is at a low speed and you click right beside it it knows it was lucky.


personly i dont think it is posible to program the ai to learn.

however if you were to get the program to come up with every posible move it could take in a chess game. then tell it to take the best posible move that will leat to its victory.

so in a chess game the computer would probibly try the 4 move check mate first because that would be the quickest way to win

im shure you get the idea.


I have a friend who deals with creating cognitive engines and has been successful to an extent. He does not disclose much information on it, so I will stop there.

For a simple artificial intelligence you could store "important" events into some kind of database and have it recall its memory on a certain event. So you would have something like:

You: Hello.
Chatbot: (Recalls that hello is a greeting)
Chatbot: Hi there! (The chatbot randomly picks something in its greetings database)
You: My name is Halospree13.
Chatbot: (Detects is, and attempts to piece together the sentence. Finds action name = Halospree13)
Chatbot: Thanks for telling me, Halospee13.


The last statement would involve some kind of grammar proccessor, which is not impossible. (The same friend mentioned above says it was actually somewhat trivial.)

An example of what I'm talking about can be found here: http://start.csail.mit.edu/
That is a question answering program that has been collecting data for years.

I hope this information was useful.
-Halo

Edit: I would like to point out, after re-reading the topic post, that many games with dynamically changing difficulty are being produced. This is basically described exactly as Solidus states it.

Posted by: St. Patrick May 2 2006, 02:49 AM


However one must take a think about what learning, in humans actually is. Itâ€™s more or less recognizing and remembering patterns to control *cause and effect*. In my opinion true learning is not some magical holly grail of programming, its simply allowing the AI to recognize patterns, and the any *cause and effect* associated with it.


The problem with this is there are only so many "causes" that have been hardwired into the AI, "learning" would mean not only being able to recognize different causes and divises a solution (or "effect") but being able to learn new causes to avoid linear AI that can only recognize so much. I suppose that when the player makes a move, it will run a series of checks through different hardwired responses to different moves, and if that move is not on that list, it will either: A ) depending on what your game is you may be able to devise a solution on the fly or B ) make a pursposeful mistake, analyzed what happened, and save a solution.




I have a friend who deals with creating cognitive engines and has been successful to an extent. He does not disclose much information on it, so I will stop there.

For a simple artificial intelligence you could store "important" events into some kind of database and have it recall its memory on a certain event. So you would have something like:

You: Hello.
Chatbot: (Recalls that hello is a greeting)
Chatbot: Hi there! (The chatbot randomly picks something in its greetings database)
You: My name is Halospree13.
Chatbot: (Detects is, and attempts to piece together the sentence. Finds action name = Halospree13)
Chatbot: Thanks for telling me, Halospee13.


The last statement would involve some kind of grammar proccessor, which is not impossible. (The same friend mentioned above says it was actually somewhat trivial.)

An example of what I'm talking about can be found here: http://start.csail.mit.edu/
That is a question answering program that has been collecting data for years.

I hope this information was useful.
-Halo

Edit: I would like to point out, after re-reading the topic post, that many games with dynamically changing difficulty are being produced. This is basically described exactly as Solidus states it.
*




For the name part, you could make it so that it askes for a name, and the player types his name say Mike, then the AI askes if thats his name, the player says "yes/yeah/yup" then something like global.name='Mike', and then everytime it says a certain sentence like 'Hey there global.name, what else do you do?' Because global.name='Mike' it would say mike and and.....man a beginner at GML is NOT! the person to explain this, I just thought of this off the top of my head, it sounds like it would work, would it?



(St. Patrick @ May 2 2006, 02:49 AM)
The problem with this is there are only so many "causes" that have been hardwired into the AI, "learning" would mean not only being able to recognize different causes and divises a solution (or "effect") but being able to learn new causes to avoid linear AI that can only recognize so much. I suppose that when the player makes a move, it will run a series of checks through different hardwired responses to different moves, and if that move is not on that list, it will either: A ) depending on what your game is you may be able to devise a solution on the fly or B ) make a pursposeful mistake, analyzed what happened, and save a solution.
*


C) Try to come up with an answer and make a mistake and record it.




For the name part, you could make it so that it askes for a name, and the player types his name say Mike, then the AI askes if thats his name, the player says "yes/yeah/yup" then something like global.name='Mike', and then everytime it says a certain sentence like 'Hey there global.name, what else do you do?' Because global.name='Mike' it would say mike and and.....man a beginner at GML is NOT! the person to explain this, I just thought of this off the top of my head, it sounds like it would work, would it?
*



Yes, but it would not be very advanced. Simply storing your name isn't enough, said bot needs to be able to proccess english (considering it is chat based). Most AI will work similarly.

-Halo



I came up with a very basic example of learning ai.

Here it is:http://andylhansen.googlepages.com/learningai.gm6

You control the blue ship with the arrowkeys and shoot bullets with space. Try to shoot the red ship. The red ship learns(sort've) as you keep playing.

I'm sure this can be expanded on.


Interesting, but comments in the code would be nice. Also, I'm not sure that having the AI simply follow you counts. Could you explain what it is that makes it learn?

-Halo


There is a reason this is not seen often. Its not that programmers don't know how, its that AI can act wierd enough as it is. Start adding sliders based on the player's actions and suddenly so many things can go wrong. Also, bugs are harder to work out because you can't really recreate the situation that they occured in (unless the person who found the bug somehow knows all of the AI's "variables.") In short, I'd recommend against it. Instead make something simple like an AI that bases its accuracy on the player's skill or something.



There is a reason this is not seen often. Its not that programmers don't know how, its that AI can act wierd enough as it is. Start adding sliders based on the player's actions and suddenly so many things can go wrong. Also, bugs are harder to work out because you can't really recreate the situation that they occured in (unless the person who found the bug somehow knows all of the AI's "variables.") In short, I'd recommend against it. Instead make something simple like an AI that bases its accuracy on the player's skill or something.
*


i agree... but... couldnt you add a limit as to what the ai learns?



This is somthing very basic that I started out with. All the computer does is when it is hit, it stores in how much the human is pointing at him and the distance. In the ai script it checks to see if the human is pointing at him the same way as it was in any of the past situations. It moves to a different side of the player, the greatast distance that you stored previously. When the cp hits the human it stores the angle of the human+180 and the distance from the computer and the human. It trys to move to that point to shoot the player again. So if the computer hit the human from behind, it would try to hit it from behind again.


AI-chatbot:
http://www.jabberwacky.com/ wink1.gif

It knows english, swedish, german, finnish, french... ohmy.gif


that's not what this is about I guess....
that oponent never learned anything out of scratch!
from scratch.. not forimplanted codes!
when people tell you stuf you want to know you remember...
so you LEARN it!
because you never knew that before???
so should an AI be?
if you show it how you attack it should make stratagies to overcome that....
so the longer you play... the stronger the oponent gets...
every time you make an attack it should think a way up how to avoid it or if it's good to try the same...
at least something like that....

if being attacked = true
{
read_file(defenses)
if !attackstyle_exists(current_attack)
perform_basic_attack
else
perform_array[2]


open_write_file(attackmodes)
>oponent attack me from 2 sides at once, he made 1 distraction and 1
huge attack..
best defense style was to split up my army and try to destroy as many forces as possible<
}

if attacking = true
{
load_newest_attackstyle()
//attack style contains;
attacking from 2 sides... 1 distraction and 1 strong army
going through west and south
}


Sorry guys. All your examples, well, they sort of stink. No offense, but You are just writing an Ai, then eating some supplement that makes you insane, and in your insanity, you say your AI "learnt" how to play the game. THat is not learning. It may be, but you have already coded the AI to play. Learning must start out with absolutely nothing already. Try Again.


Well, to use your own words, you argument stinks. If that is your definition of learning, then humans don't do so (or any other living being with a central nervous system). We all come "pre-programmed" with certain knowledge, upon which we build our knowledge. Also, learning requires a way of knowing the basic "rules of play", so that you can understand if an experience was positive or negative.

What your post also seemed to miss, was the fact here that "true" learning AI is not what we are trying to acomplish, because if that was something we were able to understand/program, we would probably currently be paid to do it, and we would not be doing it in gamemaker. what we are trying to do is making a form of AI that will improve as it plays, without preprogramming what improvement is, but by giving it a way of evaluate its actions and use that knowledge to avoid mistakes in the future. This semblance of learning can be done many ways, some of which we have tried making simplified versions of.


Ai that learns is almost impossible

Learning isn't really intelligence I mean if the guy on your left gets shot in the head your going to duck and if he gets blow to bits with a gernade your going to retreat simple

Also generally for the time an enemy exists it has little time to learn and essentially shouldn't be able to pass on those traits to its allies




Defenetly not true. Learning AI is very good possible, only you're thinking way to complicated. I think good basics of a learning ai will always be:

- put some positions in a .txt file while playing
- if the AI isn't defeated by the player erease the notitions

- if while playing the positions are about the same as in the .txt file do the things the player did

The only thing is it's rare that positions are exactly the same. The only solution is playing lots and lots of times against the pc so it gets a larga database (this is something they do in robotsoccer, creating a laaarge database). I'll post an example tomorrow.


All I was saying is the AI cant learn if it is already wired too well. If there is nothing to improve on, it cant have a reason to improve.


Ah, you don't see the point in doing this, well me neither. Not with gamemaker. But I can remember quite a few times I made a LED connected to one chip blink, the other chip would learn it's tempo of blinking with a light detector and starts blinking in the same tempo. I also just could programmed them to blink both at 120 BPM, but that's boring. It's very funny to make a complicated AI, even if there's nothing to win. huh.gif



AI-chatbot:
http://www.jabberwacky.com/ wink1.gif

It knows english, swedish, german, finnish, french... ohmy.gif
*



That thing hardly thinks for itself...
("Conversation with the chatbot thing")
It: Have you ever wondered why we are here?
Me: Perhaps.
It: Perhaps?
Me: Yes.
It: THAT'S your test?
Me: Huh?
It: I like your name.


Yeah, WTF?

-Halo

Edit: On a note of SparkWorker: Without a way to proccess information, your AI is just a blank gm6/cpp/java/whateverelse file. For a chat based AI, you need at the bare minimum a way to proccess a language and its grammar system, or some kind of chat pattern matching. From there, it can learn how to spell words (even if it's incorrectly), communicate and think somewhat for itself. A human baby knows how to proccess information given to it, but it doesn't learn how to react on this information until someone or something tells it.

Edit2: Friend mentioned in earlier post says the JabberWacky thing is a Markov chain generator, much like a script he wrote for Tribes 2 called Psycho Tycho.



What U need is a system that can determine good and bad things, and determine it's own rules. After that, it must combine these two skills to make and constantly improve it's rules. THis wont work without sensors - The 2 examples I have seen do not do any of this. Maybe I will make an example myself...



AI-chatbot:
http://www.jabberwacky.com/ wink1.gif

It knows english, swedish, german, finnish, french... ohmy.gif
*


Just a note, it speaks estonian too.



If you're actually trying to do this for a game, then you don't have to let the computer decide which effects are good/bad, as there ar a finite number of effects. All you have to do is keep track of the causes. (Probably this was stated before, but just to reiterate.)

Of course the tough part is dealing with "acceptable losses" i.e. if the AI is damaged, but kills the player. Perhaps you could assign each effect a "magnitude" corresponding to how good/bad the action is. For example harming an opponent has a value of +5, but being killed yourself has a value of -10. Therefore if these events happen in close proximity to each other, the computer will know that the overall outcome is bad.

However, to represent this accurately you would need to log a complete game to examine the player's entire strategy and learn how to counter it.

The most thorough way to do this if you're trying to create actual intelligence:
1. Discover a Unified Field theory of theoretical physics that discribes every interaction at a fundamental level.
2. Discover what the universe was like at the moment of creation.
3. Make steps 1 and 2 into a GML program.
4. Run the program. Wait about 30 trillion years for a race of superintelligent beings to develop it the universe you've created on your computer, then use their programming to control the AI in your games.
Sadly, we'll have to wait for a while to implement that solution... biggrin.gif



4. Run the program. Wait about 30 trillion years for a race of superintelligent beings to develop it the universe you've created on your computer, then use their programming to control the AI in your games.
Sadly, we'll have to wait for a while to implement that solution... biggrin.gif
*



The real trick is seperating the beings from the rest of the program wink1.gif


The closest thing I've seen to a learning AI in any game was the creature in Black & White, but it hardly learned in a way that you're describing. It only had set things it could learn, and set ways it would learn them.

I have heard about virtual neural networks in video games though, if you could set one up, and constantly re-adapt it to new information (that part alone would probably make it too slow to work) you could have something similar to what you're looking for.

I didn't take a particularly close look at http://www.uh.edu/~jbutler/anon/anonnn.html but it seems relevant. lol, actually you can probably ignore that site completely, all of the links seem to be broken.



the whole ai talk is pretty interesting but there is no one universal way of implementhing it. different games might have to adopt different learning methods. non-abstract games might pose a serious challenge in imp[lementing but so far we know what the ai is supposed to learn. eg in teennis you are always getting lobbed, stay away from the net. in soccer your team is weak and they score the first goal keep defending the lead. in chess its rather different you can say

CODE

if(ai_pieces > other_pieces)
{behaviour=cautious();}


but this kills the spirit of the cpu to go after another queen or setup more challenging senarios. also learning is limited to the creator and the player, meaning if someone else plays the game with a different approach he might find it a bit too easy.

learning ai is going to be too hard to implement in a gm game cos the engine is quite limited. Random/variable ai is wat ppl should employ at the moment.

games would be no fun if cpu never makes a mistake.

hope u enjoyed my rather pointless banter



Ok heres what i think.

now when u say it rewrites its code thats like creating a human that can evolve in seconds. if it was perfect it would create a virus that infected all computers.
even the pentagon, why because it would infintly see us as a threat to its existence, and it would realise it would be infinte so it would stop the loop and take over the first computerised bombers robots mechs and any thing it can controll and kill us with.

thats stupid dont u think.

a human now they dont rewrite there brain to stop there heart from beating because they had a heart attack and telling it to stop will prevent it.

now way we have things we cant change in are mind predfined rules to keep us alive.

thats like basic ai code that people wouldent consider learning ai because the computer dident figure that out we told it that. well those lines of code keep its heart beating or what ever the ai would have to keep it working so dont think thats a bad thing.

now also we store memory and call apon it as some of you had said. thats basicly the topic titles ai that learns.

for us to learn we realise this happened befor we want it more or less so we think of what we can do to avoid it.

well then just have the ai do that dont have it rewrite it self to be evlove into a god.

i dunno where u got off thinking of that u should have thought it through.


mabey this post will keep people from saying OHHH WE SHOULD MAKE IT REWRITE IT SELF YEA IF IT CAN PROGRAM IT SELf USING EXTERNAL DLLS IT WILL LEARN no its evolving into a freak of nature.


evolving and learning are difrent things.

the reason i compare the ai to a human is because people for years have been thinking what if a computer could work like a human brain and less like a computer(thats where ai that learns to be better comes in)


Um, @sonix: Don't worry. AI isn't gonna evolve into a god, whether we want it to or not.

Anyways, 2 Random Contributions:
1) Look down a few topics. There's one called Intelligent Chatbots. We've covered tons of the chatbot stuff there, and while it's unbelievably long, it's certainly worth reading through (in small chunks at a time). At any rate, it's probably not worthwhile to bring it all up again in another topic since that one still gets attention.

2) I assume that this topic exists in spite of the other in order to focus on a behavioral learning aspect... Like enemy AI. So, I'll point your attention to a recent post I put up in the extending GM section about http://forums.gamemaker.nl/index.php?showtopic=209473&view=findpost&p=1457194. I think it would really, really help with this sort of thing, since it allows your definition of random to change in order to accomodate trends.



Ok heres what i think.

now when u say it rewrites its code thats like creating a human that can evolve in seconds. if it was perfect it would create a virus that infected all computers.
even the pentagon, why because it would infintly see us as a threat to its existence, and it would realise it would be infinte so it would stop the loop and take over the first computerised bombers robots mechs and any thing it can controll and kill us with.

thats stupid dont u think.

a human now they dont rewrite there brain to stop there heart from beating because they had a heart attack and telling it to stop will prevent it.

now way we have things we cant change in are mind predfined rules to keep us alive.

thats like basic ai code that people wouldent consider learning ai because the computer dident figure that out we told it that. well those lines of code keep its heart beating or what ever the ai would have to keep it working so dont think thats a bad thing.

now also we store memory and call apon it as some of you had said. thats basicly the topic titles ai that learns.

for us to learn we realise this happened befor we want it more or less so we think of what we can do to avoid it.

well then just have the ai do that dont have it rewrite it self to be evlove into a god.

i dunno where u got off thinking of that u should have thought it through.


mabey this post will keep people from saying OHHH WE SHOULD MAKE IT REWRITE IT SELF  YEA IF IT CAN PROGRAM IT SELf USING EXTERNAL DLLS IT WILL LEARN no its evolving into a freak of nature.


evolving and learning are difrent things.

the reason i compare the ai to a human is because people for years have been thinking what if a computer could work like a human brain and less like a computer(thats where ai that learns to be better comes in)
*



Uhh, it's impossible for AI to do that, unless programmed that way. Even then it would take years for it to do anything harmful. And by the way, the AI would require network access to take over the Pentagon or anything else. What we are talking about is we give the AI parameters to follow, for example learning from the enemy.

In chess,

*game start*
move = move + 1
....
...
..
.
*move #10*
if checkmate = 1
then study previous moves
save possible solutions

Later,

*game start*
load solutions file
**player uses same strategy**
move pawn instead of queen
if move = 10 and checkmate = 0
save strategy


I hope this cleared some confusion up...



@sonix: In theory, since a human is not perfect, human creations cannot be perfect. Therefor it cannot become the all-powerful human ruler. Besides, as you see, humans evolve slowly. The AI will evolve slowly too as it sees fit for its existence, and if created by humans will be as human-like as possible, causing it to blend in with humans.

However, I think I'm going to completely drop this topic.

-Halo


I've twiddled with the idea of AI, i've attempted to put integrate it into my games, but the is a result of et another AI that i tried to make learn but instead left more and more of the AI out until finally it just does what i tell it to. Do you know any way i might integrate "learning" into the following example? what could i allow the computer to learn, and how would i get it to use the info it retrieves? here is the example: http://www.rocketsoft.gm-school.uni.cc/uploads/AIlearn.gm6
use the arrow keys for strafing and movement, use the mouse for directional help, use left click for fire, right click for Ghoso, and middle click for reload with clip, and when colliding with a clip on the ground use space to pick up.


I've created this example of AI learning and writing its own code into a .ini file. It basically writes every co-ordinate in the room into a file and when the AI is hit, it records that the current co-ordinate is not safe. Within a few minutes it will almost always stay behind the walls. http://www.savefile.com/files/2815606. It runs very fast and use very few lines of code. I'm currently working on a more advanced version that takes many more factors into account, and uses multiple .ini files.

NOTE:
-use the first room to write the co-ordinates, and the second to watch the action!
-to run the rooms just D&D the rooms into the desired order
-I've given more info in the Game Information
Posted by: >Leroy222< May 21 2006, 10:24 AM

You cant really make AI that learns in a human sense. It only can respond to preprogrammed variables and events.

You can make AI that appears to be learning by making it store variables and use different functions accordingly.

e.g. if it notices it wins more often with one weapon, it will use that weapon.


How do you think our brains work then?



How do you think our brains work then?
*


No, he's right. Unless you CAN simulate the brain in a programming language, you can only respond to preprogrammed events. Our brains take input, and based on later input, or input as a response to output, and tell you if the input is pleasurable or not.

For instance...
Output - Go to bed 2 hours late
Input - Tiredness
Later Input - Unhappy mood
Later Input - Boss gets mad and fires you

Decision: Go to bed on time. Getting fired isn't fun.

Later thought: What will my wife think?
Input from wife: YOU GOT FIRED?!
Output: Well.. yes...
etc etc.

-Halo



Most people, when they think of AI, think of simulating human intelligence. Don't you think programmers should simulate the instinctual actions of basic life forms before moving on to the more complex systems of mammals?

I wouldn't say our brains work on an I/O system. Most things we do are based on memory. When we recite the alphabet, we think back to when we first learned it... or understood it. So in essence, we only have pre-programmed responses. It's just the fact that there are a seemingly infinite number of responses that we may think we are really analyzing the situation.

So in creating an AI program, which would be simulating life- since all life is in some form intelligent- we would need to simulate all the necessary elements of life. This came from Wikipedia:

1. Organization - Living things are comprised of one or more cells, which are the basic units of life.

2. Metabolism - Metabolism produces energy by converting nonliving material into cellular components (synthesis) and decomposing organic matter (catalysis). Living things require energy to maintain internal organization (homeostasis) and to produce the other phenomena associated with life.

3. Growth - Growth results from a higher rate of synthesis than catalysis. A growing organism increases in size in all of its parts, rather than simply accumulating matter.

4. Adaptation - Adaptation is the accommodation of a living organism to its environment. It is fundamental to the process of evolution and is determined by the individual's heredity.

5. Response to stimuli - A response can take many forms, from the contraction of a unicellular organism when touched to complex reactions involving all the senses of higher animals. A response is often expressed by motion: the leaves of a plant turning toward the sun or an animal chasing its prey.

6. Reproduction - The division of one cell to form two new cells is reproduction. Usually the term is applied to the production of a new individual (either asexually, from a single parent organism, or sexually, from two differing parent organisms), although strictly speaking it also describes the production of new cells in the process of growth.

So by simulating these functions, the program would truly be "living" but in a different "dimension."

And some previous posts mentioned the AI "evolving" itself. Wouldn't that be impossible? I mean, a computer, when it copies something, doesn't make any mistakes... usually. So there would be no evolution. But since that is the driving force of life, I think it would be necessary to make a "mutation" script, which would periodically make deviations in the source code of the AI.


http://forums.gamemaker.nl/index.php?showtopic=100083&view=findpost&p=1141069


Basically, all learning can be narrowed down to being able to:
1. Connect actions <--> consequences and consequences<--> other consequences
2. Evaluating the concequences (good<-->bad).
3. Remebering the action, and its corresponding consequence
4. Performing the remebered actions only if the consquence is desirable.

So to make an ai, you need it to tell it what consequences ultimately are desirable (killing the player) and undesirable (getting killed), and make it able to connect events.

In a game, you can make a simplified version of this by making a list of preprogrammed actions and consequences (each conseqwuence with an assigned good/bad value), and vriting a simple script that will let it remember if an action <--> consequence or a consequence<--> other consequence often happens close together in time.

If you want a true (or "truer") form of learning A.I. you also have to let it be able to recognise not preprogrammed actions and not preprogrammed concequences, and almost more importantly, you need to make it able to change its own learning algorithm (its way of evaluating wether an action/consequence is connected, and its reward/memory system).

The one we want to focus on is the top method, and leave the bottom one to the MIT scientists which are currently working on learning ai.


@Halospree13
A beeing can easily create another more advanced beeing. How? By creating a simple basis and giving it self improving mecanisms like evolution or learning. the monkeys created us. Are we not smarter than them?

@Sonix
A learning A.I. operating in a game, will only be able to perform game actions. You don't want it to learn to change the players life- variable, you want it to learn how to execute it's preprogrammed set of possible actions (moving, shooting and so on) in the best way possible. Even if you let it generate its own code, you would not be able to take over your computer/the internet/the world, unless you give it the neccesary tools to do so. Also, you would often run such a program in a so-called virtual computer, and outside of this sandbox, it can't do anything.
Also, this is the GMC not AIM, so please Write full words, spellcheck and punctuate. It isn't hard, it makes you seem smarter, and it makes it very much easier to understand what you are trying to tell us.
Posted by: >Leroy222< May 21 2006, 07:09 PM

Although learning AI is an intresting topic, it is more practical to make pre programmed AI with so many variables and set responses that it feels real.


What this tread really is aiming for is to create some semblance of learning, because a learning ai can learn things you could not have preprogrammed, and it will be able to adapt to a players style.


I was just on an idea a had for learning AI before I foundthis thread. My idea was to make an AI that determined whether or not it was afraid of something by deciding whether it damaged it or not. for example:

Prey AI walks into green wall > AI is damaged > AI Decides to stay away from SOLID, GREEN, WALLS

Prey AI is chased by pre-programmed predator > predator hits AI >AI is damaged > AI Forms a fear of that particular predator and maybe decides to run from anything that tries to chase it. Which would protect it from all predators.

Predator AI sees runs into another prey animal > predator gains health > predator tries to run into small aanimals
Etc.

This could be adapted into shooters also

Blackviper91


Wouldn't that mean that ultimately, the enemy learns to fear everything?
Easier to cut to the chase and tell him to run from everything from the start.

The harder-- and perhaps more intelligent seeming-- AI would try to find ways to accomplish whatever objective it had without getting hurt.
eg. If it was trying to get to someplace on the other side of the wall, go around w/out touching.
So the hard one is: If the enemy was trying to hurt the player, find a way to do it w/ out getting hurt so much. And yet, without pre-programmed strategies to try.


Hey, i was thinkin that we could make some AI with some neural networks. I tried to impliment it into gamemaker, but I couldn't. Here is a link to a site about neural networks http://www.cs.stir.ac.uk/~lss/NNIntro/InvSlides.html. They are interesting. If one was to put one in their game, it would probably take a while to learn though.



Wouldn't that mean that ultimately, the enemy learns to fear everything?
No.
The limitation of blackviper's system is that it's based purely around a single variable, whereas you could implement other criteria (for example, damage dealt to the player). This way you would have antagonistic pairs of criteria where the AI will have to find a way to strike a balance between damage dealt to himself whilst killing off the player.
You (tsa) suggested something along those lines, but I think it's important to have more criteria and have opposites.



Hey, I made a "Simple Perceptron". It is as simple as simple gets. You give it the desired output, and you have a choice to set the starting weight, and then it learns the desired number after a certain amount of trials. I don't know if you would actually call it learning, but I just tried making this to see what it's like. You can read about the Simple Perceptron on the website I posted above.


Here is the link to my simple perceptron: http://www.rocketsoft.gm-school.uni.cc/uploads/Simple%20Perceptron.gm6



Really, for this you would have to preprogram (or allow the user to choose) "personality" traits for the AI. Perhaps one decision, the computer has found to be relativly safe from negative effects, but dosent offer many positive effects either. Another spot may offer a higher ammount of negative effects, but there might be a bigger payoff if the AI's 'personality' is willing to take that risk. Or mabye the AI likes to play it safe, and stay in the defensive zones? or mabye its aggressive, and goes for the kill? Just learning alone kind of limits the AI to a set pattern that eventually develops. If you were to, say, randomly set the personality at the start of each game (in a balance; ex. 100 'personality' points to be split between defensive and aggressive behavior, for instance), the AI could work its way arround the game board/map/whatever in the way that most satisfies its personality. This would allow for different AI every game that is played, not unlike human players, where we are all different.

...The only thing is that the AI wont "try something new". It wont try to find a new place, and its somewhat limited to radomly comming across a good spot, where a human could look at something, think creatively ("Hey, that looks like a good spot to snipe from"), and test the theory. Even if the AI is set to "adventurous" and seeks new areas, it cannot analyze an entirely new area and think in a creative way how to take advantage of it.

....Which brings another thing up... As humans, we learn those patterns that permit certain things. We can recongnize any high ledge as a potential good sniping point. The trick would to program this into the AI as well, to compare different terrain and find similarities. When it is found to be similar to certain known terrain, it is rated as similar, by how much it compares to each of the other terrain types. Such anaylsis would bring increased intelligence to the system, but it still would not be able to think creatively.


like animals humans have instincts things that are hardwired

basic principles be hardwired into the ai in order to ensure it wil attempt a predecided move that ignores all decisions previosly made according to a certain condition

for humans u weigh the odds of ur instincts against ur conscious decisions,
im hungry i want to eat, but i cant until i finish feeding my hamster.

since hamster feeding is a pretty worthless task u mite out weigh it with
im hungry im going to eat and completely ignore this hamster until i am done eating

all base objects that affect decision should be given a value and then added and subtracted from other base and decision values
in the end the base instinct is a certain value after being checked against choices, it will run the instinct if the value exceeds the minimum limit

goal=self preservation
shoots gun, runs out of ammo, runs reload instinct, ai then makes a choice of wich ammo to get and where to get it from

but the instinct stopped it from fighting and made it go search because it was a logical instinctual decision as self preservation is key

these r things that must be done always no matter wat and the ai cannot go for long periods without performing these actions



emotion also hinders or enhances judgement, dying 10 times may frustrate a human and cause them to die another 10 times before realizing that they could have gone the other way and not died at all
would this be desirabel for an ai?


Go here: http://www.biologic.com.au/bugbrain/

That will give you hands on learning about neural networks. At the end, you learn about learning brains. blink.gif


I've been following this thread with interest and wanted to add my thoughts. This blueprint in particular got me thinking about how to go about implementation:


Basically, all learning can be narrowed down to being able to:
1. Connect actions <--> consequences and consequences <--> other consequences
2. Evaluating the consequences (good<-->bad).
3. Remembering the action, and its corresponding consequence
4. Performing the remembered actions only if the consequence is desirable.

So to make an ai, you need it to tell it what consequences ultimately are desirable (killing the player) and undesirable (getting killed), and make it able to connect events.

In a game, you can make a simplified version of this by making a list of preprogrammed actions and consequences (each consequence with an assigned good/bad value), and writing a simple script that will let it remember if an action <--> consequence or a consequence <--> other consequence often happens close together in time.


Say actions are denoted by letters A,B,C... and consequences by numbers 1,2,3... Let the computer keep a timestamped listed of actions and consequences. For simplicity's sake assume time is discrete. A possible list would be

A,3,B,3,4,E,B,2,C,E,A,3,D,1,4,A,3,3,B,B,2,A,3,B,2

To deal with 'part 1' (Connect actions <--> consequences) the computer needs an algorithm for spotting patterns in the list. I've purposively put two patterns in this list: A is always followed by 3, and 2 only ever appears after B. These patterns could be examples of say, "standing next to the player always gets me shot" and "the player only ever dies from the collapsing floor when I pull the lever".

The computer of course should of course be noticing these patterns in a more probabilistic way, "75% of the time when I stand in the middle of the arena I get shot". If there are N actions and M consequences and if the AI is to learn patterns of the first kind (A is followed by 3) then the first idea that comes to mind is to keep track of a N by M matrix with the nth,mth entry being the number of times that pattern occurred. Spotting patterns of the second kind (2 only after B ) is another N by M matrix. If you want to spot longer patterns it'd be an even bigger matrix.

Obviously enumerating every possible pattern and looking for them (the matrix idea) isn't the way it should be implemented. Number one, patterns need to be a little more fuzzy than an absolute sequence like "A,3" or "D,1,F,A,3". The pattern "A --> 3" might very strong pattern, but each occurrence might have lots of junk data in between the two. Number two, in the real game time is not discrete so the computer needs to deal with this too. My mathematical instincts tell me that there should be a nice way to handle all this...

I'll assume just that for the moment, that is I'll assume if you tell the computer to specifically look for "A --> 3" that it has an algorithm to come up with meaningful statistics about that pattern. The next problem is that the computer can't enumerate every possible pattern and just start looking for all patterns at once. It needs a selective, dynamic way of choosing what patterns to look for and what patterns to stop taking data for. For example if a human player gets killed the same way twice within a short time period he is more likely to think about what lead up to that (as opposed to if the player died that way merely once in a while). Humans also cement things down once patterns are established, for example there is no need to know that a grenade launcher beats the AK-47 91.46182....% of the time. The computer could stop keeping a close eye on that pattern after the strong correlation was found. (At the same time the computer needs to guard itself against change. If the human develops new tactics to avoid grenades eventually the computer needs to catch on. In the spirit of the first human-like behavoir the computer might have a tendency to reexamine old patterns if it encouters "streaks" of unlikely results). To create a computer algorithm I would probably try to use a lot of these human-like heuristics but also devote a percentage of search time to more or less random searches.

Phew, and that was just part 1. I don't know how this is post is going to be received but as my prof once said "Sometimes you just gotta run it up the flagpole and see if anyone salutes", so add, expand, criticise, etc. Also if anyone wants to start writing on parts 2, 3, and 4 be my guest.



Certainly interesting.

The problem is that to work out that the player was close when the AI was shot needs alot of variable need to be included: health decreased, distance to player, the things that injured it (bullet, fists ect). If you wanted to recall this at a later stage you'd also need to have the position in the room, and you'd also have to do this with almost every room. Another problem is that in a normal sized room of say 640 by 480 there'd be 307200 potential positions for the enemy. Even with my 80 by 80 room, the .ini file that stored all the places the AI was killed, took up about 70kb. If my calculations are corrct a .ini file to store 307200 postions would take up about 4MB of space, which would take a long time to trawl through every step.

Plus to add other variables like distance to player, if the health decreased, distance to health pick-ups, ammo and many more, your looking at files over 10mb.

One idea which I had would be to instead make multiple files which act as a kind of memory storage system, and the AI checks only the relevant files.

Obviously even that would be difficult. I think people should start actually making examples instead of talking about the human brain works, or evolution.


I've been reading this topic over and over again and I find one very important aspect missing. I've heard all of the aurguments for determining what weopon to use and where to attack from but, where is all the learning. If you created the ai and it killed an object at one spot wouldnt it just go right to that spot. What about the random learning through trial and error.

An important aspect of the ai is trying out new things. The best ai doesnt mean always going to one spot to get the kills because the human will adept and find a way around it. The ai needs to use its knowledge to not only work on a current stratigy but to develop new ones as well. Well, thats my 2 cents.

~spelling edit


People seem to be ignoring me around here, has sombody checked out the bug brain software?


I will check it out right now.

Edit: Wow, that is pretty cool, I couldn't figure out how to do the new direction one foor the lady bug though.


The problems are pretty hard, I had to look at the solutions, at the end they demonstrates some learning nn's. It could be applied here.

Also, I found another article on learning nn's. Here is it: http://www.geocities.com/neuralbug/neural_networks.htm


ive always wanted learning characters in my SB2SB game...


Wow, interesting topic guys.

Well... me and my family once played this "game", which a friend of ours tought us. She's a dog trainer, and has trained several rescue dogs. The game itself is simple:

Two people are "chosen", one is the Game Master, and one the Target. The Game Master, chooses an "action". The target then has to do the action. This action can be anything, from coughing, to standing on an object on one leg while clapping. Whatever. The fun part, which makes it worth it for the veiwers, is that the Target dosn't know what he has to do. All he gets are "claps" whenever he does something even remotely right. Thus, if he goes closer to the thing he has to stand on, the Game Master would clap, but the Game Master wouldn't clap, if you didn't do anything right at all. Slowly, the Target will be able to guess what he has to do by simply trying random things. The game then ends after the target has figured out what to do.

Analyzing the behavior generated from the game, learning is when one figures out what to do, and what not to do , based on reactions OR pregenerated factors (in animals, these factors are set by birth).

A lion, by birth, knows that eating is good and getting hurt is bad, among a thousand other things. Thus using this knowledge, and the knowledge its friends and family teaches it, it will at some point learn how to hunt and start a family and whatever.

To put this into a game, can, as seen in some of the examples above, be extremely simple. It can also be franatically hard. The pregenerated factors are the most safe to use, as counting on the player to play "good" is not really the best action. But if you want the AI to react differently from time to time, these pregenerated factors would have to change.

In my oppinion, i think it's possible to implement, but there aren't many games where this is really usable. Atleast not simple quick game maker games. On the other hand, a game where you could interact with the learning of something (an animal perhaps) and teach it right from wrong might be in itself, very interresting. Sort of like The Sims, or those things all kids had some years back, that you had to take care of.

I'll try to think of a way to make an example of AI...


He i found a game that actually learned somewhere, made in Java i think, by using genetic algorithms and player's strategies to figure out how to beat it. If i find it i will post it. It was one of those games where you have a smoke trail that you draw behind you and you cannot crash into the other player's smoke trail, your smoke trail, or the walls. The one who doesnt wins. It looked pretty kewl, but i found it at school, where i could not play it.


Ok, so implementing this into a game can be hard. Making a computer human would, as i said in my earlier post, require pregenerated factors. This can be knows as Personality. How the being will react to certain things. Ofcourse, this will have to be coded, but the factors make different beings do different things.

I found this on http://www.scientificpsychic.com/workbook/person2.html

Aggressiveness - our demeanor toward people.
+ friendly, courteous, thoughtful
- aggressive, impolite, tactless

Control attitudes - mechanisms by which we influence others.
+ persuasive, conciliatory, submissive, gentle, yielding
- domineering, punitive, forceful, stubborn

Dependability - factors that affect trust in others.
+ dependable, trusting, honest, truthful
- unreliable, suspicious, dishonest, liar

Egocentrism - our degree of selfishness.
+ generous, humble, forgiving, modest
- greedy, arrogant, resentful, proud

Emotional expression - our ways of expressing feelings.
+ congenial, funny, extroverted, talkative
- inhibited, serious, shy, introverted

Fairness - how we judge others.
+ appreciative, impartial, tolerant
- ungrateful, biased, intolerant

Leadership - how we interact in a group.
+ brave, leader, independent
- fearful, follower, dependent

Physical appearance - how we view ourselves physically.
+ attractive, stylish, tidy
- ugly, disheveled, untidy

Regard for Rules - obedience for the laws of society.
+ ethical, honest, law-abiding
- unethical, dishonest, criminal

Team Spirit - how we fit in society.
+ social, family-oriented, patriotic
- antisocial, loner, anarchist

You could then give each personality attribute a value of 0-10 (where 0-5 are "negative" and 6-10 are "positive"), or something.

I know this really dosn't help us in our quest for learning AI, but it does give an example of personality attributes. This could be applied to animals allso, taking in account ofcourse, that some might not be so important for animals as they are for humans.


I suggest a hybrid approach.

While most machine learning algorithms (neural networks, genetic algorithms) do work for some cases, they have their limitations:

-It takes a long time for them to learn stuff, as the learning process is slow and gradual

-They tend to sometimes get stuck on "local maximum"s - in English, that means they find something that increases their survival rate by a little bit, but it prevents them from finding better solutions.

-Sometimes, they can connect things that don't really connect!

The story has been told (I forget the details) that the military tried using a neural network to identify enemies and friendlies. They did this by presenting images of the vehicles to the system.

Unfortunately, the images of the friendly vehicles were taken on one type of terrain, the enemy vehicles another.

The AI picked up the terrain, not the characteristics of the vehicles, and failed when tested.

This is an unconfirmed story, but illustrates an important point: Be very careful to ensure your AI is learning the right stuff wink1.gif.

-They take up a LOT of CPU power. Even a small neural network can eat up CPU very fast.

-They can't find complex solutions. This is partially due to the fact that we can't create very large neural networks, and partially due to the fact that we still don't have a complete understanding of how real brains work.

So - personally, I'd use programming to solve complex problems with the learning algorithms handling simpler tasks.


I'm trying to make a NN plugin right now!


OMG i forgot the best AI i've seen from here. Counter-Strike has a really nice AI. When the die taking a certain route to plant a bomb or something. The learn from it a go another way. And also when you hide behind a crate and kill them. Next time they encouter that area they check behind the crate before moving. Sometimes they kill me by shooting through the crate. it's really cool does anyone know who programed the 3rd party AI for CS ?


("CobraAI")
While most machine learning algorithms (neural networks, genetic algorithms) do work for some cases, they have their limitations:

-It takes a long time for them to learn stuff, as the learning process is slow and gradual

-They tend to sometimes get stuck on "local maximum"s - in English, that means they find something that increases their survival rate by a little bit, but it prevents them from finding better solutions.

To solve the first problem usually sandboxes are used, where the AI can freely run for several steps testing itself against different situations. Then, with the set of parameters found it can be inserted into the game.

The second problem is easilly solved in genetic algorythms: one of the basis of this aproach is that they use mutations in order to try new things and search for original solutions. Basicly it consists on giving a random value to some of the properties of the IA (it can be a characteristic, an action, etc...). Sometimes youÂ´ll find that this value is worst than the previous ones you had. In this case donÂ´t worry as the algorythm will get rid of it in the next generation. This solves the local maximumÂ´s problem wink1.gif

Anyway, when talking about learning AIs sometimes the problem ends up becoming an optimizacion problem. Genetic algorythms and neural networks are great for this. But there are other options you can use which really make the AI learn. For example, rule sets.

Example: supose you are making a fighting game and you want to create an AI that watches the attack combinations the player uses and learns these tactics.
In our example weÂ´ll be checking for combinations of 3 attacks, and weÂ´ll have these possibilities:
1- punch->punch->punch
2- punch->punch->kick
3- punch->kick->punch
4- punch->kick->kick
5- kick->punch->punch
6- kick->punch->kick
7- kick->kick->punch
8- kick->kick->kick
With these, we can create a set of rules like the following (pseudocode):
CODE

1-if ((prevAttack1=punch)&&(prevAttack2=punch)){
nextAttack=punch;
}
2-if ((prevAttack1=punch)&&(prevAttack2=punch)){
nextAttack=kick;
}
3-if ((prevAttack1=punch)&&(prevAttack2=kick)){
nextAttack=punch;
}
4-if ((prevAttack1=punch)&&(prevAttack2=kick)){
nextAttack=kick;
}
5-...


OK, now what we do is the following: the player attacks with a punch and then with a kick. The rules that satisfy these conditions are rules 3[b] and [b]4 (this is called forward chaining, that is, look for the rules that satisfy the conditions). Now, the AI should choose one of these 2 rules: this is called the conflict resolution stage. One thing commonly done is giving a weight to each rule, so the AI chooses the one with the highest weight. Suppose rule 3 has a higher weight than rule 4: the AI thinks that the player will attack with a punch next as rule 3 states and so it does whatever we told it to do in this situation: crouch to dodge the punch, take a step backwards to avoid it, counterattack with a low kick...Note that the player still hasnÂ´t executed his attack, nor the AI its counterattack, we are planning on advance what we will do and trying to guess what the player will do.

In the next step, the player executes his attack and the AI executes what we planned. Now, we start the method again from the beggining but with one difference: first of all, we see what attack the player really used. If it really was a punch attack we increase the ruleÂ´s 3 weight because it succeeded. But if the player used a kick our guess failed...we then decrease the weight of rule 3 and increase simultaneously the weight of rule 4. This way, when the next situation happens again in the future the AI will have learned that after attacking with a punch and a kick the player usually uses a kick attack again and is prepared to counter it or whatever.

This way, we force the player to continuosly change his tactis because the more he uses one tactic, the faster the AI will learn to predict it and the tactic will become useless.



A fighting game would never use the above approach for one simple reason: The player would beat it without even realizing it.

Consider, for example that a number of very silly people out there include words like terrorist and bomb in their emails nowadays. The reason? They think that since the gov't can monitor emails, the filtering system will pick up those sorts of terms. Then, when the emails are examined and found to be "safe," the computer will adjust... Ultimately, these people assume that they will become essentially censor-proof, since the gov't computers will learn to ignore everything from them.

These people are sadly a bit off the mark. But, if the gov't was using the above method, it would all work exactly like that. So, imagine that a player is learning the moves in a fighting game. At first, they use a very limited set of moves almost exclusively. The computer builds up a defense against these moves, and the player finds it harder and harder to win with the same techniques. So ultimately, the player starts learning new moves. At this point, the computer is so heavily weighted towards the player's tendencies that it is utterly unable to defend itself. And as the player learns the complete moveset, they will presumably continue to revisit the original moves that they learned about as often as any other moves. So any slight tendancies that the computer is supposed to pick up on are lost because of the initial immense weight that represents the learning curve.

Additionally, we have to assume that the player is trying to fool the computer. It would be too easy if the computer could simply be conditioned like that. You'd just do the same move several times in a row, have them all blocked, and then switch moves.

But there are some ways to tweak that. One it to give the weight a weight. Instead of just adding 1 every time the same move occurs, add a variable weight amount. The weight can vary based on all sorts of things. For example, if the player keeps doing a move and it keeps working, increase the weight to learn that pattern faster. and the weight could start out small and vary with the overall weight of the pattern that it's being added to in order to account for a learning curve.

The best, though, is to ignore the whole 'frequency of moves' thing altogether. That method assumes you have a very stupid player. A more skilled player selects moves based on the situation at hand, and your AI should do the same. The weighting system is nice, but it should be weighing factors like distance to player, current posture of self, etc. Consider these 2 scenarios:
1) I stand away from my opponent and execute a move that requires a lengthy block or counter sequence. The computer predicts my move and reacts accordingly. I step forward while the computer is trying to block and execute a different attack, catching the computer off balance. But it sure did predict my attack! Didn't do it much good without paying attention to the range of the predicted attack and the distance I was standing at.
2) The computer leaps into the air to attack. I begin a combo to execute a counter. More often than not, when I execute this move I finish with a low kick. The computer says "Bah, his low kick will miss me completely. What an idiot." But this time, I end the combo with a high kick that catches him square in the face. Then I resume my usual tactics that end in low kicks. He never outweighs one with the other.

In both scenarios, the situation is far more important than the odds; in the first I'm trying to fake out the computer, and in the second I'm breaking my usual pattern because of a certain circumstance. So again, weighting is good, but it's really a matter of what you weigh and when you weigh it.


("tsa05")
A fighting game would never use the above approach for one simple reason: The player would beat it without even realizing it.

("AI for game developers @ David M. Bourg and Glenn Seeman")
In this chapter we're going to study rule-based AI systems. Rule-based AI systems are probably the most widely used AI systems for both real-world and game AI applications

The example I wrote is simple because I wanted to focus on the method. DonÂ´t give too much importance to the weight thing: I chose that because itÂ´s the simplest way of solving the conflict resolution stage, but you can create any system you want. You can use weights that get calculated taking into account many other factors (like the distance, speed of the hit, life gauge, player and foeÂ´s characteristics, bayesian techniques...) or even use techniques that donÂ´t depend on weights. Some of the methods most used for the confict resolution stage are:
("ai-depot.com")
First Applicable: If the rules are in a specified order, firing the first applicable one allows control over the order in which rules fire. This is the simplest strategy and has a potential for a large problem: that of an infinite loop on the same rule. If the working memory remains the same, as does the rule-base, then the conditions of the first rule have not changed and it will fire again and again. To solve this, it is a common practice to suspend a fired rule and prevent it from re-firing until the data in working memory, that satisfied the ruleâ€™s conditions, has changed.
Random: Though it doesnâ€™t provide the predictability or control of the first-applicable strategy, it does have its advantages. For one thing, its unpredictability is an advantage in some circumstances (such as games for example). A random strategy simply chooses a single random rule to fire from the conflict set. Another possibility for a random strategy is a fuzzy rule-based system in which each of the rules has a probability such that some rules are more likely to fire than others.
Most Specific: This strategy is based on the number of conditions of the rules. From the conflict set, the rule with the most conditions is chosen. This is based on the assumption that if it has the most conditions then it has the most relevance to the existing data.
Least Recently Used: Each of the rules is accompanied by a time or step stamp, which marks the last time it was used. This maximizes the number of individual rules that are fired at least once. If all rules are needed for the solution of a given problem, this is a perfect strategy.
"Best" rule: For this to work, each rule is given a â€˜weight,â€™ which specifies how much it should be considered over the alternatives. The rule with the most preferable outcomes is chosen based on this weight.



I think that correojon is going along the right lines. I think the best way is to use a memory base using different .ini and that the computer can build up a database of all the information it gains. I've made the following script for an AI to store the id of an object that it collides with that is bad.

CODE
if file_exists("bad places.ini")=false
   {file_text_open_append("bad places.ini")
   file_text_write_string(1,"[bad places]")
   file_text_close(1)}

ini_open("bad places.ini")
if not(ini_key_exists("bad places",string(other.object_index)))
   {file_text_open_append("bad places.ini")
   file_text_writeln(1)
   file_text_write_string(1,string(other.object_index)+"=danger")
   file_text_close(1)}
   
ini_close()


It just writes the object_index of a bad thing it hits into a file for later referance.

The only difference between this and what the player does is that the player checks that the health has gone down, and then decides what to do. This is possible to program, but would be inefficient and would have little advantage.

What do people think of this code, does anyone else have any ideas of how to do learning with GML



i just thought of something. how would it be if an AI, before actually executing an event, attemps to see what would occure if it performed the action? for example, if it had to make a choice of shoot or dodge, which would be to it's advantage? using a bit of probability to calculate the chance of avoiding the fire from the enemy, and the chance of shooting the enemy before getting shot itself, and still having time to retreat. it would then be a simple ratio, go for the ratio with the larger number. which would you rather have? 10 to 1? or 1 to 10? if it get's hit, even if it does decide to dodge, then it will lessen the window of that side of the proportion for future use. giving the other side of the proportion a better chance.



i just thought of something. how would it be if an AI, before actually executing an event, attemps to see what would occure if it performed the action? for example, if it had to make a choice of shoot or dodge, which would be to it's advantage? using a bit of probability to calculate the chance of avoiding the fire from the enemy, and the chance of shooting the enemy before getting shot itself, and still having time to retreat. it would then be a simple ratio, go for the ratio with the larger number. which would you rather have? 10 to 1? or 1 to 10? if it get's hit, even if it does decide to dodge, then it will lessen the window of that side of the proportion for future use. giving the other side of the proportion a better chance.
*



While that is exellent AI, it has nothing to do with actual learning.

I think this learning buisness is hard, scince all we can come up with, are examples of saving previous experiences. And yet, we havn't made a mechanism to actually tell the different experiences apart, or look at similarities.

This is where learning gets hard. The memories we have it our minds, we are able to reflect upon the current situation. Thus, even if the two situations aren't exactly the same, we can still easily compare them, all to our advantage.

To make a script that can see patterns that aren't exact is very hard.

Now for a new learning AI example:

In one of the earlier post we were talking about RTS learning AI. Well, lets say that we don't, as in that example, learn from what the player is doing, but from our own mistakes as most learning applies to.

Therefore, what makes learning advantageous is that you can recall a situation much like the current one.

Lets say the enemy builds a bunch Metatanks, medium attack units, but also a Chronoblaster, which is a powerfull special character which is very effective against infantry.
Now the computer spots the Metatanks with its scout drone, and it sees nothing but Metatanks. Scince the computer is oviously not "bright", it concludes that its enemy, the player, must have *only* Metatanks. Wrong.
The computer, thinking it's smart, builds alot of Antimatter Gunners, anti-tank infantry. It advanced on the enemy, but is eventually beaten because of the Chronoblaster, which easily slaughters his infantry. The computer now saves the incident to a file,
"I lost a game, where he had Metatanks, and I Antimatter Gunners, because he, without me knowing, had a Chronoblaster. Next time, I should try to not solely build infantry as he might have a Chronoblaster hidden again."

This is how to implement learning AI, not based on the players actions, but the computers own faults. The hard part ofcourse being that it is hard to make the computer recognize memories that are even remotely close to the current scenario. A second hard thing would be for the computer to actually find its own faults.

A trivial topic.




This is how to implement learning AI, not based on the players actions, but the computers own faults

I, who spends large amounts of time over in the Intelligent chatbots topic, disagree. If but I think my reasons carry over here as well.

Basic Premise: The computer doesn't have faults.
A computer will do whatever it is told to do. It will do it with perfect repeatability and precision. Unless you're running Windows--then it's anyone's guess. This means that if you tell the computer to select a strategy, it will follow your guidlines for selecting. And if you tell it to make its own guidlines, it will follow your guidlines for making guidlines for making a selection. And so forth. At some point, you the programmer put all of your foresight into the selections that the computer would need to make and gave it rules. That's all it boils down to. Thus, the only way to really learn is to get new input from the programmer, not from some "learning algorithm."

Let me make a quick point to this effect first by looking at that last example.

the computer spots the Metatanks...it concludes that its enemy, the player, must have *only* Metatanks...It advanced on the enemy, but is eventually beaten because of the Chronoblaster, which easily slaughters his infantry..."I lost a game, where he had Metatanks, and I Antimatter Gunners, because he, without me knowing, had a Chronoblaster"

Notice how in this example, the computer deduced that the problem lay not in the fact that "he" had Infantry, but rather in the fact that the other guy had something that he couldn't see. That's a big deduction, but it's reasonable. Problem is, it get out of hand--"Next time, I should try to not solely build infantry as he might have a Chronoblaster hidden again." How in the devil does he figure that? It's not how real armies function, certainly. "We better pull in 1.5 million troops, helicopters, tanks, gunboats, cruise missiles, guerilla fighters, ninjas, and a couple of Hydrogen bombs. After all, you never know what the enemy might be hiding this time..."
Now, I know that's not quite how it was meant. But my point is this: At the end of the day, you have to make a rule. You have to tell the computer how to go about figuring out the "so what" of an experience. And if the algorithm only looks at what the computer may have missed, well first, the programmer has to define "what may have been missed" so that the computer will know what to look for. And second, heaven forbid that the player simple wins. Just gets lucky. Or is very skilled. The computer goes on assuming that something was missing in its own game plan.

Anyways, the main problem is a very common one in cognitive science--the problem of attribution. This is back to the whole 'computer is faultless' thing. See, it's like this: The computer has no problem with losing. It has a strategy that says "if you see such-and-such, the defense is so-and-so." It executes that defense flawlessly. Job well done. It's you and me that complain. We say that the computer did it wrong, made a mistake, etc. This is because we assume that the computer wants what we want. After all, we did give it all sorts of strategies to fight us with. But the reason that the computer seems ineffective is simply that we beat it. That, for all of its built-in strategies, we found an effective tactic. (we could stop here--in many games, this is how the "boss battle" works. You find the attack pattern, discover a weakness, exploit like mad. But then, we wouldn't be here if that was enough, right?)

To develop new strategies, to learn as it were, you need new input. You must pay attention to the user. You see, learning is judged not by the international learn-o-meter, but by the person opposing the AI. That means that even if a sloppy, ineffective strategy works for whatever reason against the player, that's what you go with. Regardless of elements lacking in the computer's overall plan. See, learning is a human thing. As is intelligence. So, it's really pretty meaningless for the computer to examine its own tactics and formulate a "better" strategy, even if guided by circumstances created by the player (as yours technically is, PhazeDK). All you end up with is something that the computer "thinks" is pretty intelligent. We want something that people judge to be smart. Pay attention, instead, to what the player is doing. We can assume that the player, who has no pre-set cognitive rules, is playing the game "right." Because at the end of the day, they are, no matter what. Whatever the player is doing, that is just as much "playing the game," and more, as anything the AI predicts that the player will do. Since we want to match up against a human intelligence, we need the computer to "think" like a human, and that means presenting the human with challenges and observing how they cope--successfully or unsuccessfully--and then adopting similar tactics through mimickry of partial situations.

So, in response to: "A second hard thing would be for the computer to actually find its own faults." I quite agree. In fact, it's futile. The computer isn't human. It can never find it's faults like a human. Instead, use the human--lose the battle, win the war. Let the human's drive to win fuel the strategy. And crush the puny mortals!!!
oops.



guys google robocode. its a java robot ai game and you will find loads of clues on ai tht adapts to varing situation.
yes its in java but u will easily learn the conversion to gm (there are way to many geniuses on this forum anyway dry.gif )



Ok, so implementing this into a game can be hard. Making a computer human would, as i said in my earlier post, require pregenerated factors. This can be knows as Personality. How the being will react to certain things. Ofcourse, this will have to be coded, but the factors make different beings do different things.

I found this on http://www.scientificpsychic.com/workbook/person2.html

Aggressiveness - our demeanor toward people.
+ friendly, courteous, thoughtful
- aggressive, impolite, tactless

Control attitudes - mechanisms by which we influence others.
+ persuasive, conciliatory, submissive, gentle, yielding
- domineering, punitive, forceful, stubborn

Dependability - factors that affect trust in others.
+ dependable, trusting, honest, truthful
- unreliable, suspicious, dishonest, liar

Egocentrism - our degree of selfishness.
+ generous, humble, forgiving, modest
- greedy, arrogant, resentful, proud

Emotional expression - our ways of expressing feelings.
+ congenial, funny, extroverted, talkative
- inhibited, serious, shy, introverted

Fairness - how we judge others.
+ appreciative, impartial, tolerant
- ungrateful, biased, intolerant

Leadership - how we interact in a group.
+ brave, leader, independent
- fearful, follower, dependent

Physical appearance - how we view ourselves physically.
+ attractive, stylish, tidy
- ugly, disheveled, untidy

Regard for Rules - obedience for the laws of society.
+ ethical, honest, law-abiding
- unethical, dishonest, criminal

Team Spirit - how we fit in society.
+ social, family-oriented, patriotic
- antisocial, loner, anarchist

You could then give each personality attribute a value of 0-10 (where 0-5 are "negative" and 6-10 are "positive"), or something.

I know this really dosn't help us in our quest for learning AI, but it does give an example of personality attributes. This could be applied to animals allso, taking in account ofcourse, that some might not be so important for animals as they are for humans.
*




Don't you think that organizing those into groups of positive and negative are a bit wrong? I mean, I don't think the computer should know what is "right" and what is "wrong" because a person formulates their own personal opinions on the issue.

And social is not necessarily better than anti-social... and are you saying that orphans aren't family oriented?



Off-topic personal request... Since it seems to be happening just about ever other post in this topic... Please, when someone posts a long, space consuming message and you want to add a 3 line response, don't quote the entire message. It's just...silly. Try something like @PhazeDK, about positives and negatives: instead.

On topic: I just posted a very lengthy bit about what sort of behaviors game AI should try to adopt and why. And all I get is "guy google robocode". Can't I get an intelligent, thought-out "I think you're right/wrong?" This is, after all, the Experts' forum. Saying "go look somewhere else for hints" is not taking part in this discussion, particularly when the resulting search leads me to a horribly built wiki that explains nothing and clearly lacks the benefit of a spell-checker/intelligent spelling-monkey.


hi ill just post my thoughts about the original post sorry if you changed your ideas along the way but there were to many large posts and quotes to read everything so probly missed some stuff when i looked over it


When playing a game a of chess, the computer evaluates it's opponent's moved based SOLELY on what it's creator has programmed it to do. This, in short, is the downfall of chess AI. While a human opponent may learn it's mistakes, a chess board will continue making it.


first off ALL progs do what ever they do depending solely on how it was programmed AI is just calculations based on how it was programmed

and i would not say this is a downfall such as chess AI has come to grandmaster level haveing drawn and beaten
karpov
12th World Champion, 1975 - 1985
FIDE Champion 1993 - present

I would say chess AI doing what it was programmed to do, is not a downfall with the fact that it can beat the best human player alive

also there ARE chess progs that dont continue playing the same moves if they lose



this is obviously very, very advanced, especially for most people who use game maker.
uum that sounds somewhat insulting to many ppl and also there are very many ppl who use gm who are vaery advanced in many areas


another post about creating standard chess AI (which is not as complicated as they made it seem

It would take a long time to devise your own hand crafted list of moves, but that is how it must be done

um I would have to disagree with your idea that chess AI is not very complicated
IBM spent many years with full time ppl working on the chess AI for deep blue(prog that beat karpov)

also chess AI dose not have to be a list of moves and part of what intelligence is is not haveing to have a specifice responce already prepared do you with reall intelligence which is what AI is trying to replicate need to already have a pre made exact response to every problem that happens to you in life

what the advanced chess AI does is evalulate possible moves and there consequences a set amount of moves or time limit from when the last move was made

AI is often focused around a database for its starting but to be AI it needs to be able to calculate answers to problems that occur that are not already defined by a database

also it is inpracticle to make a database of all possible moves in chess because it is infinit

and even to calculate the practicle moves would be much to large as

in the first turn in chess (white then black each move once)there is already 400 possible combinations and each turn results in many more combinations of moves and chess games in tournaments can easily last over 100 turn each resulting in far to many calculations it would even with the best computers take years to get enough moves out to make it useful and then the files would be so large it would be inpracticle to use

to do a chess AI or about any ai you need not just a database but a way to react to unforseen problems/situations

sorry for typos



First off, I'll add that I'm PhazeDK. I found that my first account was still active, and i don't know why i thought it wasn't.

@piggy1
You might have gone back a little too far in the posts tongue.gif...



On topic: I just posted a very lengthy bit about what sort of behaviors game AI should try to adopt and why.  And all I get is "guy google robocode".  Can't I get an intelligent, thought-out "I think you're right/wrong?"  This is, after all, the Experts' forum.  Saying "go look somewhere else for hints" is not taking part in this discussion, particularly when the resulting search leads me to a horribly built wiki that explains nothing and clearly lacks the benefit of a spell-checker/intelligent spelling-monkey.
*



Coming right up... now... bare with me guys, i just woke up, but i'll do my best.

When you say "The computer doesn't have faults", I, in some sense, must agree. The computer doesn't know better than to think it's right. But, as also stated by other posters, since we have faults, so does the computer. Therefore our AI will also have faults. (When in my previous post, I was talking about the computer, I was referring to the opponent player, not the hardware or anything).

Now, if we were perfect, we wouldn't need the computers to learn at all, right? We could just make the perfect AI, with no possible way to beat it. The other player would then have to learn from the computer until the game would become a constant "tie".

We aren't perfect. The AI isn't perfect. So when we distribute an unperfect AI, and we don't want to keep updating it, how do we improve it? We let it learn.

Of course a computer (again, the AI itself) cannot learn all by itself. It needs to compare it's actions with the actions of someone else. So when i said "This is how to implement learning AI, not based on the players actions, but the computers own faults" it is still comparing the players play style, with it's own. What I was trying to say was that some of the previous examples, the computer would learn directly from the player, copying his moves. While that is learning, it isn't very yealding learning, unless you have absolutely no knowledge of how the do things by yourself (as in a chatbox AI, if theres a word you don't understand, you'll have to let the use explain, but in a RTS, the computer should by default have a good AI)

Have you ever heard this:

"Dumb are those who do not learn from their mistakes. Intelligent are those who do learn from their mistakes. Wise are those who learn from others mistakes."

Well that is the basic prrinciple of learning. Comparing previous scenarios with other scenarios. Of course, when you do it right, you'll also learn. But definitely not as much as when you do it wrong.

The sentence "I lost a game, where he had Metatanks, and I Antimatter Gunners, because he, without me knowing, had a Chronoblaster" was of course just explanatory. I know that this is alot harder to code, than just to write a sentence, and i can also see how hard it would be for a computer (again, the AI) to learn.

(tsa05)
See, learning is a human thing. As is intelligence. So, it's really pretty meaningless for the computer to examine its own tactics and formulate a "better" strategy, even if guided by circumstances created by the player (as yours technically is, PhazeDK).


Wrong. Intelligence is learning, reasoning, and comprehending. Why should this only apply to brains? Our brains, as other posts have mentioned, is but another form of a computer. Therefore, it is possible, wether we in the GM community can do it or not.

Thus, a computer, with insanely long scripts or not, is able to learn. Learning is very simple in itself. As is intelligence. The theory behind is easily understood. It's comparing previous scenarios with current. Though it might be hard to code, it sure as hell is possible. Some of the GM examples eirlier actually DO compare previous situations to present. But just not very well.

Again, a trivial topic. tongue.gif

And to just set things straight:

(shornickel)
  Don't you think that organizing those into groups of positive and negative are a bit wrong? I mean, I don't think the computer should know what is "right" and what is "wrong" because a person formulates their own personal opinions on the issue.

And social is not necessarily better than anti-social... and are you saying that orphans aren't family oriented?


I think the positives and negatives are seen, as if you were an outsider. If you were to meet the man in person, would you rather that he was aggressive and violent, than friendly and nice? Would you rather want him a sociable group player, than a total loner? Would you want him to say what he means, or be shy and say nothing at all?

They aren't supposed to judge wether you are a "good" or "bad" person, just how well you are socially.



smile.gif Pretty good reply. But I think that you misunderstood a few things I posted. I'll explain them a bit better.

But, as also stated by other posters, since we have faults, so does the computer. Therefore our AI will also have faults

Incorrect. smile.gif A very funny transitive attribution; it demonstrates that I did not make my point clearly enough.
Let me start by saying that I understand your point: If we program AI, we have faults in our strategies, thus the AI will have those faults. So to quote myself:

And if the algorithm only looks at what the computer may have missed, well first, the programmer has to define "what may have been missed" so that the computer will know what to look for.

I hope you see that I already get this problem--that was part of my whole point. You can't program an autotelic AI because we aren't perfect.
But my point was that the AI is perfect in its execution of whatever strategy it knows. See? So all those faults you refer to aren't computer faults, they're human faults.



See, learning is a human thing
Wrong.

Wrong. smile.gif I assume that I don't need to define attribution, but please pay close attention to the term. You've misunderstood my argument. In fact, it almost sounds like you think I am suggesting that computers can't learn. Which would make me quite a hypocrite, considering how I spend my free time. (Well, most of it. Otherwise I'm playing Ninja Gaiden. Gotta stay young.)

The whole thing with intelligence is that it isn't a phenomenon that can be measured by any sort of physical device (the learn-o-meter reference). Intelligence is a concept. The determination that some things have intelligence and others do not is a uniquely human determination. To save myself the page space, just look at something like the Turing Test for Intelligence. There's a reason that Intelligence is considered to be subjective. (If you like, I can put together a fairly large list of papers published on the subject--I don't know if that sort of thing is really wanted around here, but let me know). So, you see, when I say that Intelligence is a Human thing, I'm quite in earnest. I don't mean that only humans can learn. I mean that intelligence only exists in the perceptions of humans. We can make an intelligent machine, sure. But to a squirrel (I hope none are reading this), that machine hasn't got any intelligence. Nor do humans, for that matter.

The reason I pointed out that intelligence is a human thing is to remind all of us that at the end of the day, the computer's performance will not be measured objectively. It will be measured "humanly," if such a term existed in that context (I don't mean humanely). So, it doesn't matter if the computer AI can identify missing elements in its gameplan and formulate new strategies to cover those possibilities. What really matters is that the computer can identify elements of humanness and imitate them. 'Cuz that's how we really recognize intelligence. And since the computer's opponent is a human, what better way to sample human tactics?

Hope that's clearer--I'm more accustomed to lengthy, ongoing dialogues with professors, where they already know where I'm coming from and vice versa. So when I say things like "human thing," I don't even think about how it may be misinterpreted. smile.gif



Well, tsa05, you did make some things alot clearer. Although this discussion is also going a little bit away from our main topic tongue.gif...

What we need is a good example of learning. I can't quite come up with a simple, working example. Maybe you can, seeing that you are a college student, and i've yet to turn 15 tongue.gif.

Either way, it *should* be possible. But then again. What the heck do i know. Especially now ^^. I'm kinda tired... 'Night.


In at least two AI examples brought up so far there is an existing theory of strategy that is worth mentioning. I'm talking about the Chronoblaster/Gunner example and the earlier top down GM shooter where each player moves one square at time. Both of these seem to boil down to the idea of 'matrix games'.

Since people here probably have much more computer sci and math background than me I may be running the risk of patronizing you guys by triyng to explain what matrix games are, but I'll do it anyway.

A Matrix game is a two player game where each round the players secretly choose a move and simultaneously reveal their move. The moves available are finite and do not change each round, but players may have different moves. For example, player 1 might have moves A, B, C, D and player 2 might have moves E, F. For each pair of moves there is a corresponding point value. A:E might be worth +2 points meaning that when a round ends with A:E player 1 gets 2 points. A:F might be worth -7 points meaning that when a round ends with A:F player 2 gains 7 points. The goal is to get more points than your opponent. The name 'matrix game' comes from visualing a matrix whose rows are indexed by the first player's moves, columns by the second player's moves, and whose entries correspond to the point value.

A 'Strategy' in a matrix game is a probability distribution you give your actions each turn. For example, choosing 'A' 50% of the time, 'B' 25% of the time, 'C' zero percent of the time, and 'D' 25% of the time is a strategy. One usually analyzes matrix games from the perspective "I want to maximize my worst possible outcome", i.e. if my opponent knew my strategy (but not of course exactly which move I will make) and tailored his strategy according, what is the best I could do? The justification is partly because matrix games are completely solvable if you assume this, but more importantly I am fairly sure this perspective is equivalent to saying you are seeking the best strategy without considering decieving/bluffing your opponent. (The equivalence isn't immediately obvious though).

The short version of the story is that matrix games are easy to solve. You can find both player's 'optimal strategies' (the technical term for a strategy which maximizes your minimum payoff) without too much computation.

In the top down shooter example, each player's move is which direction to choose. If you both end up shootng each other or you both miss then that has neutral point weight. If player 1 shoots player 2 while not getting shot then that move has a positive point weight, and vice versa. In the chronoblaster example each player's move is choosing what their army composition is. Matrix entries correpsonding to where the first player chooses even a small amount of chronoblasters and the second player chooses lots of gunners will have a strong negative point value, increasing with the number of gunners. (Understand that the point values and the first player's available moves should have built in that he already has a large number of metatanks). The computer's decision of balancing how many gunners to make is very easy with the matrix game framework, and doesn't really relate to learning at all.

Hmm it is surpisingly hard to find web references about matrix games given that there is a video game company with the name "Matrix Games" and there are many games based off the "Matrix" movie. Here is one reference:

http://www.cs.brown.edu/courses/cs195-3/notes/game_theory.pdf


@dosboot,

We never said it wasn't easy to make AI for any of the examples, all we're trying to do is make the AI learn. Who cares if they are good or not, that's not the point. Its whether they can learn or not. And this topic isn't about matrix games.

Also, the examples are the only steps towards learning AI we have here at GMC, but you're very welcome to come up with some better, which is very doable.


Of course since this is going to be used in a game, the scope is going to be much more limited and replicating human intelligence exactly need not be a goal.

The goal is to make a good AI, and one that "acts" human, not neccesarily an intelligent one. What you need to do is to give a set of characteristics you want it to have, and then to implement them.

In addition to the learning stuff, it may also be prudent to make something like an "expert system" which basically mimics the behavior of an experienced player. Find some good players of the game, have them record their games, and have the AI mimic their strategies.

In games with large numbers of units or players, it may even be prudent to create a hierarchal system, with one system making high-level strategies for the overall field, and each unit having the ability to make its own decisions based on its current situation. Some strategy games do this.


Just wondering, how many other people are actually trying to make learning AI in gamemaker here,no matter how basic? By that I mean using GML! I feel like the only one sad.gif .

I really think that people should stop discussing ways of making an Einstein-like AI that has a mortal fear of its life and might realise that its own existence is worthless etc, and start working with a few basics.

In this topic (until now) no-ones even made an example that can detect movement patterns or that learns when its in danger, let alone an example of clever tactics and statergies. So is anyone else using whats being said and making something that learns, because it seems to me very little progress is being made here? I'm not saying people can't do it, just many aren't.

My problem
I'm working on an AI that learns Pac-man at the moment that knows where the blocks are, learns hitting ghosts is bad, knows that hitting coins is good and stores this information in a .ini file. Its all going well except I'm having trouble following up this knowledge with actions without making it seem to hardcoded. I've tried collision lines to detect where things are but making it move more than one step at a time is proving tricky becuase it then doesn't know where to stop.

Does anyone have any ideas of how to make the AI move more dynamically and to think a few steps ahead?


... you could use, say: if place_free(x+sp,y+abs(char.dir)){x+=sp;y+=char.dir}

but it would need a variable in 'char' called 'dir' determining the direction tongue.gif


Pacman Learning

This is a very simple learning game. Pacman begins by knowing nothing, and then it collides with other objects and learns if they are good or bad

As this is the first version it only uses two variables to test if an object is good or bad: health and score.

The goodbad script decides wether something is good or bad, and the other scripts write the objects index into a .ini for later recall.

A collision circle is used to identify objects within a certain range (this can be changed easily), unfortuately I can't find away to make it prefer moving a way from an enemy instead of going towards a point, so thats why sometimes it doesn't work. The first room is just empty so it makes it easier to move around, and the original map is also included, I can't quite make it work on that map.

The script are very versatile and are not specific towards this game.

http://www.savefile.com/files/8405659

Please let me know what you think and any improvements that could be made


The link isn't working for me.


The problem must at your end, it works fine for me on a couple of computers




Pacman Learning

This is a very simple learning game. Pacman begins by knowing nothing, and then it collides with other objects and learns if they are good or bad

As this is the first version it only uses two variables to test if an object is good or bad: health and score.

The goodbad script decides wether something is good or bad, and the other scripts write the objects index into a .ini for later recall.

A collision circle is used to identify objects within a certain range (this can be changed easily), unfortuately I can't find away to make it prefer moving a way from an enemy instead of going towards a point, so thats why sometimes it doesn't work. The first room is just empty so it makes it easier to move around, and the original map is also included, I can't quite make it work on that map.

The script are very versatile and are not specific towards this game.

http://www.savefile.com/files/8405659

Please let me know what you think and any improvements that could be made
*


Couldn't you just make it work on the second map, by using the drag and drop of avoiding solids to reach the pills/coins? (can't remember which)



Hmm, I have some ideas. Before I start, don't tell me that a 3D array is impossible in GM because it is perfectly possible. Just do this:

Script set_array_var(var, ind1, ind2, ind3, value):
execute_string(var+ind1+'['+ind2+','+ind3+']='+value);

Script get_array_var(var, ind1, ind2, ind3);
execute_string('return '+var+ind1+'['+ind2+','+ind3+']');

Note that when using these functions, you have to put the var name as a string, e.g. get_array_var('blah',1,2,3); as opposed to get_array_var(blah,1,2,3);. Anyway, on to the actual interesting stuff.

First part:

The AI has a big lookup table (three dimensional array) showing on the first dimension the AI's situation, on the second dimension the AI's desire, and on the third dimension each possible action. All values are initially set to zero.

Now the computer starts by looking at the row of the lookup table corresponding to its situation and desire. It then picks the highest value of the values in this row, and if there is a tie it picks a random one of these.

It then performs the action it picks. If it succeeded (i.e. the result matches the desire), the value for that item in the lookup table increases. If it failed (i.e. the result doesn't match the desire), the value for that item in the lookup table decreases.

Second part:

The AI starts with two axioms: Winning is good, and losing is bad. If an action results in a win twice in a row, the AI adds a new rule: doing that particular action in that particular situation is good. If an action results in a loss twice in a row, the AI adds a new rule: doing that particular action in that particular situation is bad.

Third part:

The programmer will have to define what the fundamental actions are. For example, for a platformer's AI: move left, move right, jump, shoot forwards, shoot upwards, crouch, use super weapon, etc.

If the AI has already tried and failed at each of the fundamental actions to do something, it will try a random combination of actions to try and achieve it. As soon as it finds the combination of actions that works, it adds a new row to the lookup table with the action it thought up and increases its value for its current situation and desire. For example, in a platformer where you have to do a run-up to jump over a gap, computer tries:
move left- Didn't work.
move right- Didn't work.
etc, then it goes to complex actions:
move left, move right- Didn't work.
move left, jump- Worked.
Action "move left, jump" stored in lookup table and value increased for current situation and desire.

Of course, this wouldn't work too well in a platformer as huge action combinations appear in platformers. But in a tile-based game like Fire Emblem, it would work much better.

Fourth part:

If an AI tries an action and ends up in the exact same state it started in, it marks this action/situation pair as useless. For example, in the hypothetical platformer game:
AI is standing on some ground and there is nothing in the way. It tries the combination "move left, move right" and notices that it is now in the exact same state. So it marks this action/situation pair as useless and will never do it again.

Fifth part:

Now suppose the AI has arrived at a boss battle in our platformer. It already knows that:
- in order to beat the boss, it must deplete its health to zero.
- in order to deplete the boss's health, it can either shoot or use its super weapon.
- shooting normally will not affect the boss's movement; using the super weapon causes it to flinch (which is good).
- it can only use the super weapon, say, 3 times per boss battle.

So it realizes: "I can either shoot or use my super weapon. However, I may need to make better use of the super weapon later on (e.g. character is cornered) and it has limited uses. So I should shoot normally right now."
How it realizes this is straightforward. It plays through the boss once using the super weapon all 3 times right at the start, and fails because the character gets cornered. It then plays through the boss again without using the super weapon, and fails because the character gets cornered. It then tries using the super weapon at random times, and notices that using the super weapon stops the character being cornered for a short while. So it finally decides that it should wait until the character is cornered before using the super weapon.

Sixth part:

This is more like an extension of the fifth part, but whatever. Assume the AI has these rules established:

- Losing lives is bad.
- Losing health continuously causes the loss of one life.
- Therefore, losing health is bad, but not as bad as losing lives.
- Losing ammo continuously may cause the loss of health (it cannot fire at enemies).
- Therefore, losing ammo is bad, but not as bad as losing health.

Now, assume the AI is in a situation where it either has to lose health, or to fire an ammo-limited weapon. It should pick the latter.

Seventh part:

Assume the AI already established the rules in the sixth part. Now, assume that the AI has established the following rules as well:

- The goal is to get to the end of this room in the shortest time.
- Backtracking causes loss of time.
- Therefore, backtracking is bad.
- If health is lost, the goal may become impossible.
- If time is lost, the goal does not become impossible.
- Therefore, losing time is better than losing health.
- Therefore, backtracking is better than losing health.

Normally, the AI will not backtrack. However, suppose the AI is confronted with an enemy and cannot possibly kill it (e.g. the enemy is invulnerable, or the AI doesn't have enough ammo). The AI now thinks this:

- Continuing would cause the enemy to damage me. Therefore, continuing in this situation is bad.
- Staying still would cause the enemy to damage me. Therefore, staying still in this situation is bad.
- Backtracking would cause loss of time, but not loss of health.
- Therefore, the best thing to do is to backtrack.

Later, when the AI has successfully moved into a position where the enemy cannot reach it, and the AI can now get past the enemy, the AI's thoughts will change to:

- Continuing would allow the goal to be reached.
- Staying still would cause loss of time.
- Backtracking would cause loss of time.
- Therefore, the best thing to do is to continue.

That's all I have to say for now, and that was a very long post, so I hope you read it all! smile.gif


@bobxp

Really good examples. I must admit though, that i didn't read it all. If you make a game where what you've written here happens, i'll clap. For a long time i tell you! Because that would be awesome.

WoW is a feeling,
Phaze, Denmark


Hmmmm... This got me thinking a bit.

Babies when born don't know that they have to eat or drink until they do eat and drink and the receptors in their brain give them a good feeling which tells them that eating and drinking is good for them, so they carry on eating and drinking.

Babies when born also don't know that smashing their head into a metal post is bad until they do it and they feel pain (a bad feeling) , this bad feeling then tells them that doing this action is not good, so they don't carry on smashing their heads into metal posts.

Do you see how this is unfolding?
The way to learning is all about GOOD and BAD.

This could be put into programming. At the start of a game an AI has a local variable called varCurrent_Feeling; When an action is executed (e.g. the AI eats or AIActionEat()wink1.gif varCurrent_Feeling = 5; this value would then be stored in the feeling variable for that action. When the AI comes to having the chance of eating again the AI will first check the eat feeling variable. This AI would therefore have learnt that eating is good and it will want to do it again when it gets the chance.


By the way, I may be able to implement some of my ideas in .NET2. I don't know if I can do it in GM, though.


i know that i am reeeaallly out of place in this section, but I just have to mention that you guys are figuring out stuff that scientists have researched for years, just using a 30 dollar program. wow.



Hmmmm... This got me thinking a bit.

Babies when born don't know that they have to eat or drink until they do eat and drink and the receptors in their brain give them a good feeling which tells them that eating and drinking is good for them, so they carry on eating and drinking.

Babies when born also don't know that smashing their head into a metal post is bad until they do it and they feel pain (a bad feeling) , this bad feeling then tells them that doing this action is not good, so they don't carry on smashing their heads into metal posts.
*


This is'nt really true, we are "preprogrammed" with certain reflexes, and we are preprogrammed with certain knowledge. If you submerge a baby in water, it will hold its breath, even if it has'nt leraned to do so. It will try sucking on almost anything until it finds food. Not all mammals know where to get milk when they are newborn, but they know they want it. To make a learning AI you ned to give it a basic framework upon which it can improve.



Maybe true, but giving AI a too large of a predefined framework would limit on the amount it could learn.


I was playing around with the thought today of having a robot creature learning to grab an object of diffrent weight and shape.

The robot would have a set of controls to operate from (conditions), and the effects these would have. Basically the global idea of the robot is to get his claw as near the object as possible, and then initializing a variable to show him that it has been grabbed and succeded.

There would be 8 moves at the robots proposal:

Walk forward, Walk backward
Rotate Bicep CW, Rotate Bicep CCW
Rotate Forarm CW, Rotate Forarm CCW
Open Claw, Close Claw

This couldn't be so hard could it? The robot would simply do something under a time, first randomly, and then after he has used his controls, he will analyze how much closer the claw got and write it in his data base. Then, if the move caused the claw to get further away, he would simply reverse the move on one of the further ones. Also, if he has used all the controls with no success, he could look back in his database and try to do the diffrent actions under another amount of time instead. And over and over again, eventually he would grab the object. So now he has learned!

Perhaps if he has reversed his move a number of time, with same result, he could try another control to see if that is successful.

Wow, this sounds intimidating. I might do this as I get home. Perhaps we could even add senses to it! And if its successful, maybe we could make the robot look for things that are hidden!

Basically, what I wanted to say with this, is that you have to give the learning object a set of controls to work from. When we were babies we also experimented with moving our muscles etc, to see their effect. So you have to tell the AI "use this and that, to accomplish this, but you will have to figure out HOW you should use this and that to accomplish it."



Maybe true, but giving AI a too large of a predefined framework would limit on the amount it could learn.
*


The same is also true if you give it a too small framework. The trick is finding a good balance.



Hmm... So far I have been completely useless to this AI cause. I just want to clear some things up in my mind.
Now, when you say @znorre "To make a learning AI you need to give it a basic framework upon which it can improve.", do you mean that in order for an AI to "learn and apply" it must change and add to it's already existing code? While, at the same time, not changing the basic "framework" in any way? So, say I want the AI to improve it's own framework. This probably wouldn't be possible because once it got rid of it's "framework", there would be no basis or guidlines for it to follow. But is there a way to have it add to, but not negate or cancel, the framework? For example:
I'll say that the "framework" tells the AI that it is not allowed to move the object out of the view, and it will give certain limits, telling it "if and when" it is breaking those limits. Now when the AI breaks those limits it MUST reverse what it has done and mark that action as a "bad" action. So it will not try that action if it will break the rules. These "actions" will be simple code executions (direction=, speed=, x,y, if, timer...), and it will create and test each code as valid or invalid, it will then figure out exactly "what" it is doing and where it will end up, it will then execute the code. If it breaks the "framework" it will have to find a way to reverse what it has done using the values and actions it used, and try and reverse what it has done.
Sorry if this seems a bit hard to understand. I am not good at putting my ideas to words. But what I'm trying to say is, could you have an AI change and manipulate it's own code to find a solution to a problem?


This is truly a great topic for discussion. I hope that I can contribute by saying that the last method described here is the best approach in my opinion.

About 6 months ago, I made up an AI test from an idea I had while making a game. This is how it worked:

I gave the AI 4 actions that it could perform. Move left, Move right, Move up, and Move down. These actions were given to the AI in the form of scripts linked in an array. For example:

CODE

ai.action[1]=move_left();
ai.action[2]=move_right();
etc...


I gave the AI a memory array of the results of performing the above actions. Then I gave the AI a "need". The need was abs(point_distance(ai.x,ai.y,mouse_x,mouse_y));
This need was calculated seperately and given to the AI. So the only thing the AI knew was the value of the need it was given. I then preprogrammed the AI so that it's goal was to reduce it's need to 0 (It wouldn't be hard to make it's goal(s) dynamic).

When the game begins, the AI scans through it's actions to see how many actions it can perform. The AI then scans through it's action memory to see if there is an action it can perform that it does not know what the result will be. If it finds an action with no action memory, it performs the action and records the results. Thus learning about it's self.

The AI then looks at it's need. It performs the actions above until it discovers which actions affect it's need, and which actions affect it's need in the desired way.

Now the AI has learned what actions do what, and what actions are needed to satisfy it's needs. In this case, the AI's need is to be where the mouse is. So this AI follows the mouse perfectly.

SUMMARY (for those who don't want to read it all happy.gif ):

The AI learns what actions it can do, and what actions affect it's needs. In my example, the AI learns to follow the mouse.

Reading this topic has inspired me to expand upon my test, I'll let you all know what happens!





Now, when you say @znorre "To make a learning AI you need to give it a basic framework upon which it can improve.", do you mean that in order for an AI to "learn and apply" it must change and add to it's already existing code? While, at the same time, not changing the basic "framework" in any way? So, say I want the AI to improve it's own framework. This probably wouldn't be possible because once it got rid of it's "framework", there would be no basis or guidlines for it to follow. But is there a way to have it add to, but not negate or cancel, the framework? For example:
I'll say that the "framework" tells the AI that it is not allowed to move the object out of the view, and it will give certain limits, telling it "if and when" it is breaking those limits. Now when the AI breaks those limits it MUST reverse what it has done and mark that action as a "bad" action. So it will not try that action if it will break the rules. These "actions" will be simple code executions (direction=, speed=, x,y, if, timer...), and it will create and test each code as valid or invalid, it will then figure out exactly "what" it is doing and where it will end up, it will then execute the code.
*



Not exactly but close. Things like a maximum speed, inability to move through solids and inability to move outside the game window are not part of what i call the basic framework. They are part of the game physics. What i mean by framework is some preprogrammed (but self changeable) way of behaving. In my simple example that preprogrammed behavior is moving randomly. The AI should be able to change it, but it's neccesary to have it, and it's a good idea to have it intelligent enough, because if it's not doing anything it can't learn anything. If the framework is smarter than just random movement, you will save a lot of learning-time because it won't have to learn such things as "don't jump off cliffs" or "don't shoot your team-mates" the hard way. On the other hand, you should not program anything but the obvious into the AI, because if you give it too much preprogrammed knowledge you limit what it can learn, and that's one of the reasons for making learning AIs: You want them to find tactics you did not know.




The AI learns what actions it can do, and what actions affect it's needs. In my example, the AI learns to follow the mouse.


Where is your example?


I made a quick AI off of Schreib's idea. The AI has a number of arms, and it randomly chooses which one to rotate, then using weights it chooses whether to rotate the arm forward or backward, the AI then rotates the arm and edits the weights based on what happens. The AI is preprogrammed to know that greater distance to the object is bad and less distance is good, and chooses what to do based on that. In addition it's kind of cool to watch wink1.gif You can choose the number of arms, and if you click you can move the goal object around.

http://www.rocketsoft.gm-school.uni.cc/uploads/Arm%20AI.gm6


~Abyss




This is truly a great topic for discussion. I hope that I can contribute by saying that the last method described here is the best approach in my opinion.

About 6 months ago, I made up an AI test from an idea I had while making a game. This is how it worked:

I gave the AI 4 actions that it could perform. Move left, Move right, Move up, and Move down. These actions were given to the AI in the form of scripts linked in an array. For example:

CODE

ai.action[1]=move_left();
ai.action[2]=move_right();
etc...


I gave the AI a memory array of the results of performing the above actions. Then I gave the AI a "need". The need was abs(point_distance(ai.x,ai.y,mouse_x,mouse_y));
This need was calculated seperately and given to the AI. So the only thing the AI knew was the value of the need it was given. I then preprogrammed the AI so that it's goal was to reduce it's need to 0 (It wouldn't be hard to make it's goal(s) dynamic).

When the game begins, the AI scans through it's actions to see how many actions it can perform. The AI then scans through it's action memory to see if there is an action it can perform that it does not know what the result will be. If it finds an action with no action memory, it performs the action and records the results. Thus learning about it's self.

The AI then looks at it's need. It performs the actions above until it discovers which actions affect it's need, and which actions affect it's need in the desired way.

Now the AI has learned what actions do what, and what actions are needed to satisfy it's needs. In this case, the AI's need is to be where the mouse is. So this AI follows the mouse perfectly.

SUMMARY (for those who don't want to read it allÂ  happy.gif ):

The AI learns what actions it can do, and what actions affect it's needs. In my example, the AI learns to follow the mouse.

Reading this topic has inspired me to expand upon my test, I'll let you all know what happens!
*


Well, Mr. Mindreader, you've seemed to figure out how my AI worked that I finished two days ago! biggrin.gif wink1.gif It works just as you say. It has six moves, and it picks the move with the most points. Then, after the move has been done, it analyses, and if the move got it closer to the goal, it got +1 one points, otherwise -12 points. And if both moves were under -100 in points (that is, none of the moves help), it will try some other section of moves, such as raising the claw, grabbing etc.



Unfortunately, my example was lost (as well as a whole bunch of other stuff...) when my hard drive crashed a couple months back. I'll make up another example and post it here.

I'm glad to see the concept in action! Correct me if I'm wrong, but these successes demonstrate a learning AI. Limited learning, but learning nontheless.

Edit: I'd like to see your example abyssal, but the link is down. sad.gif
Posted by: Robbi-985 Jun 15 2006, 05:42 PM



The AI learns what actions it can do, and what actions affect it's needs. In my example, the AI learns to follow the mouse.


Where is your example?


I made a quick AI off of Schreib's idea. The AI has a number of arms, and it randomly chooses which one to rotate, then using weights it chooses whether to rotate the arm forward or backward, the AI then rotates the arm and edits the weights based on what happens. The AI is preprogrammed to know that greater distance to the object is bad and less distance is good, and chooses what to do based on that. In addition it's kind of cool to watch wink1.gif You can choose the number of arms, and if you click you can move the goal object around.

http://www.rocketsoft.gm-school.uni.cc/uploads/Arm%20AI.gm6


~Abyss
*



Nice example. smile.gif I just couldn't resist editing it a bit. There's nothing really too wrong about it, I just wanted to use the whole arm system to do more.

I put 6 balls in the room and a holder for each ball, far away from the ball. The arm's job is to go to the closest ball and take it to the closest holder... and repeat it 'til it's done. wink1.gif It shows a little console in the top-left showing what it is currently trying to do (like 'pick up ball 6')
The arm is made up of more segments which are shorter, so it bends in a neater way (I know, this isn't really necessary, I just wanted to try it out to see what it looked like).
As it carries the balls, the balls leave trails of different colours so you can see their paths, how they got from the start to their holder.

You can move balls which haven't been taken to a holder by clicking and dragging like before (had to change it a bit though or ALL the balls went to the mouse >_< )
You can speed the graph scrolling speed up by holding SHIFT. I tried to comment on bits I added in code.

Anyway, it kind of shows off Abyss's AI and how the arm does not take long to adjust to its new target (usually) considering it only knows the distance to its target.
The line graph is to show the way the distance to the target gradually decreases over time. I know what I did isn't really helping discuss AI but it kind of shows a use for it..... or something like that.
Anyway, if anyone wants to see:
The http://www.rocketsoft.gm-school.uni.cc/uploads/Arm%20AI%20985.gm6.



we should all start working on an AI that can learn, we could make the ultimate AI, for like a TDS or platform game......



we should all start working on an AI that can learn, we could make the ultimate AI, for like a TDS or  platform game......
*



That would be so cool, even the pro's games don't do that properly



Hey, I've been following this topic for a while, and I was interested in that arm thing, so I created something very similar, but simpler.
It uses the points system. There is a ball. It chooses the action with the highest amount of points. If it gets closer, it adds a point to that action. If it gets further, it subtracts 20 points, so it will do a different action. It doesn't go diagonal yet, but I'm thinkin about adding that.

All it is is a ball that learns to go to the mouse. Here is the link::::::::http://www.rocketsoft.gm-school.uni.cc/uploads/SimpleLearningAI.gm6


Nice example. Now someone just has to make it do something a little harder and eventually you could turn it into a very nice AI.
~Ptolemy


Yeah, let's think of more complicated stuff it could do. Like combining up and right gets closer than just going right, so it gets makes a new action for upright. somethin like that.


Hey, i have been following this thread for a little while, and i dont really want it to go dead or anything, so here is my idea.

The way you guys have been going over this, you're just basicly telling the ai "if it works, do it." Now this is all well and dandy but there is a big problem on it, it keeps doing whatever it might find, meaning it has a much lower limit to what it can learn than what you guys have been aiming for. Now think about this, learning comes about from human error and/or trying new things, computers usually have no use for either, thus meaning they cant learn, or at least, cant learn what you dont program them to learn. But what if you program human error into the computer? It might sound like messing up on purpose, but its not, at least from my viewpoint.

Are you following me so far? Good.

Now can anyone tell me three things that lead to human error, or at least trying new things? Thats right, frustration, tiredness, and boredom.

What if you made a code that has the computer lower its own chances to notice something happening each time it loses? thats right, you would simulate frustration!

and if you set the computer to lower it's reaction time as the game progresses? again, correct, a simulation of tiredness

and finally, what about setting a computer to gradually raise its chances to completely ignore what it has learned, just to get some more excitement and danger in the game? once more, you are correct, if you said your computer now has a short attention span (boredom)

I could come up with a code to do this, but im a bit too lazy, and my attention span is even shorter than a very bored computer tongue.gif
at least i'm pretty sure it would be easy to write the code, especially with gamem- huh? is that a quarter on the floor? ooh, shiny biggrin.gif


I think you forgot about necessity--the one factor that by itself has lead to far more progress than any of the characteristics mentioned. It's far more useful in a game, too. Just think, you sit down to play your favorite FPS, and you come around a corner and !bang! nothing happens. The enemy has decided to take a nap, or otherwise simply not shoot you. 'Cuz he was bored. Not so good.


that isnt exactly what i meant.
if you have been reading the thread, you should know that everybody is trying to get the ai to distinguish right and wrong, without it being pre-coded, making it take the safe path, im just saying that it should have a chance to try something else when it doesnt know what will happen, so say the ai finds a good place to kill you from, and with the ideas already shown, it doesnt do anything, it just keeps on doing what it's doing, and never really learns past that point.

but with my idea, if it gets bored of ambushing you in the same place, it might come looking for you, even though it knows it stands a better chance of getting killed that way. when it sees you, it turns out it is on a cliff above the player, perfect headshot, it eventually gets bored with that also, so it decides to explore, it finds a vantage point that gives a perfect shot to where he just was. So it waits, another player kill
another time, it might decide to follow the player, keeping his gun aimed at the player's head, not only does it kill the player as soon as the player turns around, but finds another ambush spot or two

and as for the frustration, after enough kills, the computer gets cocky and misses a perfect shot, getting itself killed

tiredness, he finds a spot, it gets real excited and shoots too soon, alerting the plyer to its position, likewise, it does the same spot for too long, it gets tired and shoots too late, dead again

do you see what i mean?

basicly my idea would just keep it more random, while still learning what works best


I just thought of the absolute BEST method to create AI from stratch (i.e. an IQ level starting at absolute zero) so that the bot actually learns everything by itself (including how to walk etc. so you now know what i mean from scratch). And then eventually it will become smart enough (hopefully) to finish levels etc without a path, but more like a choice made by the actual bot.

I'll test this theory, but im 80% sure that i can do it. I'll get back to you guys once i have thought this through.

Its going to need a very big array index and alot of complex maths based on propositional functions.

This means that no matter where you place this bot, in any game, it will be able to adapt to the environment, the game and be able to actually do stuff, so its not limited to just 'one' game.

Wish me luck. (by the way this is for a platform game)


EDIT: Okay found a few cons/pros.

Cons:
1) The learning progress would be extremely slow (because AI starts at zero
2) Would require LOTS of RAM (There are ways of getting pass this, e.g. using HD, but that would slow it down a bit), but dosn't matter because i've just hought of a third alternative.

Pros:
1) Eventually, the bot would learn so much that it would never die and always finish a level in record time (speculation?)
2) Like i said before the bot would be able to manage in any game environment and learn to adapt
3) Bot would also learn how to be efficient, and master complex skills using multiple controlls (depending on the environment) as well as solve problems (spec?).


Of course, the real question is, whats more important, making AI that learns (very slowly and ill explain why in a sec) or just create AI. Creating AI that learns is not only very hard (make that extremely hard, but still possible) but it takes alot of time for the bot to learn becasue bots obviously (and im talking about computers here) are too stupid to know the difference between different objects.

Creating AI on its own is very easy to make, however it cannot learn as all of the brain you basically programmed yourself. This can make it rather smart, and quick because it dosnt need to learn, but you the programmer must tell it everything it needs to know, however the bot would litterally *NOT WORK* if you were to place him in another game as its brain is only limited to one game and thats the game you taught it to play in.

So while creating a bot that learns is good in the long run, the question is, do we need it to learn, or should we just give it all the required info in order to complete a task?

Keep in mind, that when were talking about creating AI that learns there are 2 ways of doing so. Give it your own AI plus make it learn (which is considered cheating in my opinion) or make it absolutely clueless and learn everything itself, so all the programming is merely there just to get the bot some arms and legs and the rest is for the bot to figure out.

Of course you could make the bot generate its own code, but that probably isnt the best option. Best option would be to make it store every inteligence in a database rather than rely on custom codes.

Perhaps i've said too much.


Hey, I've been following this topic for a while, and I was interested in that arm thing, so I created something very similar, but simpler.
It uses the points system. There is a ball. It chooses the action with the highest amount of points. If it gets closer, it adds a point to that action. If it gets further, it subtracts 20 points, so it will do a different action. It doesn't go diagonal yet, but I'm thinkin about adding that.

All it is is a ball that learns to go to the mouse. Here is the link::::::::Simple AI


I don't want to sound rude, but to me it looks like the ball dosn't learn anything. It just moves based on how the points are added/subtracted. I'ts a good start, but its kind of cheating. How does the ball know that its supposed to move towards the mouse?

So in my opinion, you've already taught the ball to move closer to the mouse by looking at the distance so it hasn't learnt anything, and dosn't need to.

Same goes with the arm example. Its programmed in a way so that it extends the arm based on the distance etc. Not exactly learning. Its just plain 'AI'.

But remember that even with an IQ of zero, you still have to teach your bot how to act and respond. Most importantly, you have to give it the ability to actually make it 'alive'. If you bot is so dumb that it never even bothers to try walking it will never learn, so you will need to give it some brain so that it realises that it must do somthing, just dosn't know what or how and so thats where the learning stage begins. The correct word here is persuasion(sp?). You bot must be programmed in a way so that it 'wants' to learn. The rest is up to the bot.

Is it possible? Definately. On a scale of 1 to 10 (1 being peice of cake) , creating AI would be able 2-3 on the difficulty scale.

Making a bot learn would be around 9-10. (depends on the game too dont forget)

But before you can think about doing any of this, first you need to make a game (to test it on obviously).



If it starts from scratch (meaning 0%), how does it:
1. Know how it's supposed to win?
2. Learn? It needs to make mistakes or successes to learn, and if it does not know anything, it can't do anything...

You always need to have at least a tiny bit of knowledge to learn anything.

Secondly, please rememember that our priority isn't to create true learning AI, but to create some sort of semblance of learning, to be used in games.



If it starts from scratch (meaning 0%), how does it:
1. Know how it's supposed to win?
Thats what the bot finds out

2. Learn? It needs to make mistakes or successes to learn, and if it does not know anything, it can't do anything...
Part true and false. Yes it would need to know alittle (like i explained above) but the rest will be clear to the bot

You always need to have at least a tiny bit of knowledge to learn anything.
Basically what i just said. The only knowledge the bot will have is the knowledge in 'how to learn' somthing.

Secondly, please rememember that our priority isn't to create true learning AI, but to create some sort of semblance of learning, to be used in games.
Yeah i know, i just thought it would be pretty cool to watch a computer bot actually learn everything for him self. But maybe its just me smile.gif



Yeah i know, i just thought it would be pretty cool to watch a computer bot actually learn everything for him self. But maybe its just me


Certainly not just you wink1.gif . And if we get the bot to learn anything for itself, it seems it would be easy to edit it to start with more information making it suited for games, and true learning AI could have its place in a game as well.

Can't wait to see your example /\lex


~Abyss




Can't wait to see your example /\lex


lol, this might take a while so don't wait smile.gif

Ill start by making the actual game, with one huge level with different enemies, physics etc and then work on the bot.

Incase your wondering, it will be a commander keen game smile.gif



whoops, internet screwed up sorry.


(/\lex @ Jun 21 2006, 02:47 PM)
You always need to have at least a tiny bit of knowledge to learn anything.
Basically what i just said. The only knowledge the bot will have is the knowledge in 'how to learn' somthing.
*


How can it learn that winning is good? any AI needs to be preprogrammed with at least one goal, be it winning, getting a high score or avoiding death. unless winning implies directly or indirectly reaching a preprogrammed goal, it can't learn any new goals, or anything at all, for that matter.




Edit (17. aug 06):
you still have to preprogram a "reward" connected to winning
\/\/\/\/\/



Perhaps i didn't quite explain myself properly, It dosn't know winning is good until it wins. In otherwords, its like putting a mouse in a maze. The mouse has no idea what its doing there. Once it gets to the end, it then realises that its goal all along was to get to the other end. The next time the mouse goes into that maze it will already know its goal.

Okay that didn't make sence. Ill just make this and then you'll understand.


2 things.
When a mouse is put into a maze, it doesn't know the goal, but it still tries to do stuff until it is rewarded. Thus, it "learns the goal"--that which caused a reward (not really, but I'll get there). I don't think you really understand how complex this is. It's called operant conditioning, and any good psychology book will inform you that there's many different ways to reinforce pairings. Despite our understanding of these factors, it is still difficult to thoroughly train an animal. And this is where your proposed method begins to fall apart. You see, even though we can train a mouse to "find it's way through a maze," it hasn't really learned the goal--that is the nature of animal intelligence. Depending a bit on which paradigm of learning you follow the details of the process differ, but basically, the animal seeks to attain a satisfied state. It gets through the maze faster and faster, but not because it has realized the goal. Instead, its limited capacity for remembering sensory data is enhanced with each successful time it reaches food. So it follows the path through the maze more quickly in order to more quickly satiate itself, and it is aided in recalling the correct procedure because of its success (it's a brain chemistry thing).

Now, train that mouse to circle strafe avoiding obstacles while shooting at a player. You see, very few cognitive scientists are willing to say that humans operate on the stimulus-response (S/R) principle that animals appear to use. The idea is certainly out there, and it is discussed, but never given full merit--Look up David Seamon on Space Ballet for a very nice overview of this argument.

My point is, humans have something that is still not understood by science that we refer to as consciousness. We have an innate ability to externally reflect, and there seems to be a nearly magical filter that allows us to reflect upon only what is going to produce a collective of "logical" results. You are developing along a different paradigm--you give the bot a compulsion to learn (like the mouse's hunger), but then it will only develop an animal intelligence. And so, back to square one, where we resort to proposing all sorts of random fluctuations for the sake of randomness smile.gif

Second point requires too much text, so I'll be lazy and only gesture at it. -->You've forgotten to consider the affordances of the game. You say that in the long run, the general learning bot is better for dropping into a game, since any specific Ai only works in the game it was designed for. I will first clarify--the game type it was designed for... But yours will be just as limited. I see what you were thinking: It will learn and adapt itself to a game. So, at the outset, in all game types it is pretty poor (I wouldn't pay for a game that offered to learn how to be a game after I bought it!). But eventually, it will adapt, and so every type of game, and in fact every individual game, will have an Ai that is uniquely suited to its goals. But you forget that ever-important compulsion thing. The bot only has certain things that it can do in its environment (these are called affordances), and if you are going to motivate the bot to learn, it must do so within its affordances, or else there is no "Ai." 9For example, if the bot ignores the fact that it has no affordance that lets it walk through walls, then it will wander (because of its compulsion to try things) and it will break all of the rules. So in the end, you will still have to create individual Ai engines for each different game because the affordances of each game will be different, and the bot must be given those in every case in order to adapt.



So in the end, you will still have to create individual Ai engines for each different game because the affordances of each game will be different, and the bot must be given those in every case in order to adapt.


I managed to figure out a way past that, which is why i know its going to work wink1.gif

Posted by: Bytestream Game Studios Jun 28 2006, 03:44 PM


i'm just tweaking an example of this im working on and ill upload it when im finished.

The challenge, of coarse, is how to make the bot learn without programming it in


I would assume you could apply each actions, IE shooting, tossing a grenade, moving, or reloading to a number which is obtained through consequence, for instance, if the unit fire one shot, it would increse the desire to shoot in the future, as the consequence would be the oponent being hurt. If, say, the unit is in a cirtain position maked by a node and if an enemy deals a great amout of damage to them, they mark that area as risky, then if they kill an oponent there that node is marked as valuable. Eventually the entire map would be made into a large gradient of good/bad zones, each with their values in defence and oportunity.



Eventually the entire map would be made into a large gradient of good/bad zones, each with their values in defence and oportunity.
*



There is something similar to this already except of being shoot at they collide increasing and decreasse the value of where they were and what direction they were standing at.

(Znorre)
http://www.geocities.com/snorrevestli/ai_2_005.zip


Controls:
Enter: Change room speed. 1-9999
M: DisplayMode. Choose which players AI to display.
R: "Reward". How much the value of a move increases when a player hits its enemy.
P: "Punishment". How much the value of a move decreases when a player gets hit by its enemy.


This is on the second page about half way down



Hey brainwash d I like your idea but I think curiosity should be added. Most of everything I do is out of total curiosity. This could be led to by boredom most often. So what i think is that you should implement your ideas with sub catagories so that if your ai is bored there is a higher chance of curiosity (trying new things) and if it frustrated it will most likely do something it knows will work to get rid of some frustration, and the same should go for tiredness. But mabe with frustration there could be a slight chance of doing something it is not sure will work. I Believe if this were to be used correctly and each emotion or action would effect its knowlage as well as the other emotions or actions you would have an intelligent learning ai.
Posted by: Bytestream Game Studios Jun 29 2006, 06:54 AM

^^
it sounds like a good idea but the ai is always limited to pre-programmed emotions. For instance, it can't get angry and smash something if the creator hasnt programmed anger + smash into it.


So then you would have to have you game all set out except for the A.I. and then go over every possibility for each emotion and put them each into the code. The A.I. only needs to learn to do what is possible inthe game or what the creator wants it to be able to do. So if that was implemented with something like if anger is true then the object would do one of these actions with a chance of however many total actions are possible and have each action lead to a chance of changing emotion. Or even have the emotion variables set to values instead of true or false and have different actions for the amount of the emotion, so that way you could have all of the emotions communicating with each other to figure out what to do and after each succesfull action the individual action would be given a value and the A.I. would use the actions with the lowest values more often depending on what the emotion is. This method would take months of coding though depending on the extent of actions the A.I. is limited to. So if you have an interactive game were the A.I. can open doors and use objects you need a serious ammount of code. But the A.I. could be professional quality if fully programed for each emotion and action possible. It would be nice if wee could track down some expert programmers from an unpopular game studio who has at least a good idea on how to make regular A.I. and throw our ideas at them to see what they think.
Posted by: Bytestream Game Studios Jun 30 2006, 07:35 AM


that method in itself is not really learning as such. It is only responding to preprogrammed actions and events.

My idea of a learning AI would be one in an rts game where their is no definate set of events. The method discussed earlier where the AI decides why it lost and adjusts to it seems a lot more like learning than the whole emotion thing.


Just to sort things out, the only way you can create the ultimate learning AI without any specialise programming to a game, is to randomly do keyboards events until it works out when something has been killed and stuff like that. Obviously thats pretty much impossible so specialising to a genre at least is the most achiavalble



Can some one make a basic top down multiplayer shooter, that we can try to implement this type of AI onto?
*



Try Phara about your idea. Thats what I am going to do now because I have an idea about ensuring it won't be in the same place twice in a row.

Razard



Earlier today I was getting cold and there was a wall heater by so what I did was I took a chair and sat right at the heater to warm. So I was thinking why not make the ai reconize weather in a RPG so if it is cold the ai put on a winter clothes and if it is hot it put on summer clothes. In an RTS on the other hand it check all the routes into an oppenents base. Then it see where all of the oppenents towers are and where his troops are. If it sees that all of his ways into his base is heavily defended then it would seperate it's forces into 2 and attack 2 places at once. That way if the oppenent is not very good one side will get through and attack key structures such as militry training facilities and then help the other force.. If a base is attacked from within then your base will be destroyed easily because then you would have to defeat the force inside before you can focuse your forces on the outside force wich would have broken through by the time the inside force is defeated. Or the force that broke through could go inside the enemies base and fortify it's position. Then once both forces are defeated the original attacker if smart enough could have built another attack force and then finished off the oppenent whilst he was still recovering from the last attack. You can do this effectivily in Warcraft 3 with the Orcs because of Troll witchdoctors healing wand. So if a person were to create a Warcraft 3 clone they can do this effectivily.

Razard


I see what your saying but to have an A.I. there has to be some pre programmed code or you wouldnt have an A.I. at all. If you were to give it the bare minimun it would still need code to distinguish between things it can and cant do and be able to record and read data. This would require allot of programing. An A.I. cant do what it wasnt programmed to do, if it could you would have a miracle worthy of worship! But without a certian gametype in mind this is all the further I can expand upon this matter. If someone can come up with a basic game to be edited I think that a large group of people should think up all of the rhings to go into the test such as interactive objects, weather, damage and so on and get a pretty good idea of what the A.I. will react to/learn from and try out different methods untill a solid working method is found.


The human brain is complex in many ways but is also simple. The simple part being that we all are preprogrammed to react to certain things such as getting hurt our brain gets a message from our nervous system so we go ow and try to cover if we start bleeding because that perticular part of our body starts getting cold and start to feel what dieing feels like so we are trying to stay alive do cover our wound and get first aid almost immediatily. If we can replcate that in game maker then what will happen that commerical game producers will try and buy the code from us so that they can have a revolution in their game and therefore a lot more game s can be produced by us by leaving the source code in the forum. The SIM's tried to replicate that without to much success. If we create a SIM's clone with that we will be able to finally make money out of Game Maker games. If we can make a RTS that alows us to make the AI Not just first work on collecting resources for the first about 10 minutes then work on building troops. What separates us from animals is not only our ability to think but it is our ability to adapt in our current situation. The reason why there are police is because we social, greedy creatures that want to not lose what they such as friends, valuable items that once gained, we try to hold onto life because in our hearts we know that we lose everything including our very soul if we die. Imagine that we had no greed then wouldn't be the need for police, the army, national guard, security forces and then our ability to adapt would be almost completly lost. The reason that will never happen is because everything dies. Death is the thing that none of us can defeat because it has made us mortals. I may sound like a nerd but almost everyone I know thinks I am one so it doesn't matter but if we became like the Jedi from Star Wars then we would lose most of our greed so then we would be able to live in peace. If we make AI in a RPG that attacks outher AI and has feuds and can't live in peace then we would be starting to create Ai that learns from its mistakes because like humans it would have cops to stop he feud. The feud would then stop from a later generation. If the RPG also has greedy people in it then it would also be a start to replicating the human brain.

This is just my opinion.

Razard


It sounds good in theory but that is what we are trying to figure out how to do, it is already clear that is the type of thing that needs to be done. I dont think money has anythinhg to do with topic either and if the master coders of the major game companies wanted A.I. sutch as that they are fully capable of creating it. The sims is not a verry good game to use that type of A.I. in because its mainly about controll. Thats why it was so popular because its like a little slave for your entertainment and you could make it do whatever you want. If the A.I. were to react without you telling it to the game would get aggrivating and you would feel as if you are losing controll of your "slave" and probably stop plaing. Is the idea good? Yes. Is that the proper game to use the A.I. in? No.



It sounds good in theory but that is what we are trying to figure out how to do, it is already clear that is the type of thing that needs to be done. I dont think money has anythinhg to do with topic either and if the master coders of the major game companies wanted A.I. sutch as that they are fully capable of creating it. The sims is not a verry good game to use that type of A.I. in because its mainly about controll. Thats why it was so popular because its like a little slave for your entertainment and you could make it do whatever you want. If the A.I. were to react without you telling it to the game would get aggrivating and you would feel as if you are losing controll of your "slave" and probably stop plaing. Is the idea good? Yes. Is that the proper game to use the A.I. in? No.
*



Yes I absolutly agree but when in free think mode. I think that if a person creates more realisticsims in the sense that they get hurt and that they do what you tell them to but with more realism. Then you be createing a way of replicacting the human brain. When that is done then we would have to try and perfect it. Once that is done then you could create a true virtual human on cyberspace.

Razard



If anyone knows where I can get any Warcraft 3 sprites. I can probaly make my theory a reality. So if you know of a site please either pm me or post. I have checked google and it deosn't help at all.

Thanks.

Razard



If anyone knows where I can get any Warcraft 3 sprites. I can probaly make my theory a reality.
*




Warcraft 3 doesn't use sprites they use 3d models. But theres a warcraft 3 site with alot of resources http://www.wc3sear.ch/.



Sooo, Warcraft 3 sprites are the key to creating successful AI. Who'd have thought... I fail to see why your theory requires Warcraft 3 sprites to be made a reality. Or maybe that was just a subtle bump and sprite request?



Sooo, Warcraft 3 sprites are the key to creating successful AI.  Who'd have thought...  I fail to see why your theory requires Warcraft 3 sprites to be made a reality.  Or maybe that was just a subtle bump and sprite request?
*



The reason why I need Warcraft 3 sprites is because it is the only RTS that I can think of right now with medics. What I mean is that orc ai if done correctly can make exacelent ai especialy with napeleon tactics. I just remembered I can use Battle for Wesnoth sprites. Thanks tsa05 for helping me remember. I will probably be back in about 2 days with the basic engine. That way we can work on the RTS aspect of it.

Razard




Or maybe that was just a subtle bump and sprite request?
*



That was not a subtle bump or a sprite request tsa05. I just saw a way to help us to start creating our ai.

Razard




Warcraft 3 doesn't use sprites they use 3d models. But theres a warcraft 3 site with alot of resources http://www.wc3sear.ch/.
*




Thanks Ryknu.

Razard



Hay you can reply to multiple people with only one post instead of a post for each reply. Nothing can really be tested untill a basic engine is created and I'm not to good with making definate working engines so we need a victim...ahhem I mean a skilled programmer to get something going. Any takers on the idea? I'm thinking overhead mabe? Or anything that might provide an adeqite ammount of things to be expanded upon.


I will try to start a TopDownShooter engine without any AI, so people can try to implement their own AI into it. It won't have very good graphics, but that doesn't matter right now.


Thank you. If graphics are needed i can get some of my work to use but the engine would be verry mutch appreciated.


I am to lazy right now to make a Warcraft 3 clone so what I am going to do is I am just ggoing to use the MFC engine. For the simple reason I don't want to reinvent the wheel. It is actually an advanced version of MFC because I've always wanted to make an all space RTS. So don't expect the same ship sprites. I am not so certain about buildings. Just pm me every now and then to remind because I get bored easily then I going to work on my other games.

Razard


TERMINATOR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! TERMINATOR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

lol.



TERMINATOR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! TERMINATOR!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
*



What on earth does that mean.

Razard



okay, I am almost done, I am just making the enemy have the same limits as the player, such as turning speed, and maximum speed, and health. So far, all you do is move around and shoot, and the enemy doesn't do anything because I didn't program any AI in it. I will post it on here soon.
Posted by: Bytestream Game Studios Jul 3 2006, 06:18 PM


i have a working rts engine if u want one


Hey, for the top down shooter engine, should I make it so you control your direction with the keyboard, or with the mouse? Anyone have any suggestions?


I think keyboard. It would give more controll and be easier.


(Bytestream Game Studios @ Jul 3 2006, 08:18 PM)
i have a working rts engine if u want one
*



Yes please because I just got home I don't want to work on it right now. Does it by any chance have meducs and towers and walls.

Thanks

Razard



Okay, it's almost done, I just need to fix a few things.


Okay, I am done, here it is: http://www.rocketsoft.gm-school.uni.cc/uploads/TDS%20Engine.gm6
Please tell me if there is anything that is wrong with it, or if something should be changed. I didn't add any AI, that is your job, to test your ideas on it.


Well this is my first post in the experts section so you can just ignore me if you think I am stupid.

They way I see it there are two ways to get what you want. You can have adapting ai or self-creating ai. Self-creating ai is what allot of you are trying to do. I think you may get better results though, with adapting ai.

The difference between adapting ai and self creating ai is this.

Self-creating ai learns by trying new things and saying if they are good/bad (or something like that). Adapting ai starts out with pre-set ai. Then it adapts by changing variables that started out random. See http://www.rocketsoft.gm-school.uni.cc/uploads/AI_learn_ball.gm6 by The Oldy and http://www.geocities.com/snorrevestli/ai.zip by znorre for some examples of adapting ai.

I think that we should go back to trying adapting ai because it may give better results in a game. The player may get bored playing with idiot ai and stop playing the game very quickly before it gets better. Adapting ai can still give the player some decent ai in the beginning but will get more challenging as the player and the ai get better.

EDIT: I forgot to put in that adapting ai was talked about early on. I am not trying to steal others ideas, I am simply saying that we should try it, not spend all of our time on self-creating ai. I also made some grammar changes smile.gif .



i did read the topic to page 6, but i post the link anyway: http://www.a-i.com


Okay, is anyone even trying to add AI to the engine I made for you to practice your ideas on?



Okay, is anyone even trying to add AI to the engine I made for you to practice your ideas on?
*



Yes, hold on it is actually hard to make self-creating AI. So I am going to create adapting AI.

Razard

Posted by: Bytestream Game Studios Jul 6 2006, 05:38 PM


try this little engine i made:

http://www.coolteens.co.za/games/LEARN_AI.zip

CONTROLS:

The bot on the left will shoot randomly either high, middle, or low.

press down to duck under the high bullets.
press left to shield the middle bullets.
press up to jump over the low bullets.

You are on the right

press Q to shoot high
press A to shoot down the middle
press Z to shoot a low one

the bot only knows how to shoot in the beginning.

If you successfully dodge a bullet, the bot will see that as a possibility of dodging and try it next time you shoot at him


That is pretty good. The bot learns from the human. Cool.
Posted by: Bytestream Game Studios Jul 6 2006, 07:54 PM

did u look at it?


Yes, it worked very nicely, eventually, the bot always dodged everything. Good Job.


Well done Bytestream Game Studios.

Razard


Another very good example of learning AI! Good job Bytestream Game Studios!

I believe that we have found the method. Now it is time to expand upon it. So far, the AI learns what we program it to learn (moving toward a goal, dodging bullets, etc...). The next step is to create a system that dynamically learns.

In other words, if we give the AI a new element it learns everything that it can do with the new element (for example, a new weapon) without the programmer having to pre-program a memory system for the element. This would create a truly dynamic learning AI.

This is something I've been working on since I first posted in the topic a page or two back (I decided not to remake my lost example, since better examples had come out). I've been making some limited progress, but the going is very difficult. If we can manage to develop such a system, we'll have discovered the "holy grail" of AI research.

Good luck to everyone on this project!


i had thought of something like that i was going to make a game for it

let me explain:
you fight your ai (like the example) it learns from you
your ai fights an enemy by itself

and so on

so your ai takes note from you then fights on its own and would somehow start learning from itself

ex of what an ai should do after a few games of learning

You shoot a rocket at it (for the first time)
Naturally it moves out of the way
But it doesn't know it explodes when it hits a wall
From then on it does

the next time

You shoot a rocket at it
From its lesson it moves away from the wall and the bullet
It survives

the only major problem of creating ai

how the heck does it write itself so it can read it without us preprogramming its options
Posted by: Bytestream Game Studios Jul 8 2006, 09:26 PM


using the rocket thing as an example:

you could use the following method...


event -> rocket hits wall
what happens next -> rocket explodes
what happens to bot -> gets killed
how to avoid -> step away from wall


thus every action is precoded and only the events need to occur for the ai to "learn". But it isn't really learning since you have already taught it to move away from the wall and you have told it how to dodge the rocket properly.

learning from the human is a step up yet it is still not really learning.

The most dynamic ai i can think of is one for an rts game.

The ai can actually analyse its game and pinpoint obvious weaknesses and causes of his lost based upon obviously bad events (such as losing 10 men while killing two).


BGS has the right idea. This is very similar to some popular game AI techniques. This kind of system is ideal for training AI for fighting games, although it is hardly limited to them. By using a combination of probability and tracking what moves and countermoves lead to winning situations you can apply this technique to almost any game. It can even be expanded to play strategy games like Tic-Tac-Toe, Poker, and even Chess (although the number of possible moves and countermoves in Chess makes this approach highly impractical).

A pioneering British scientist by the name of Donald Michie first developed this technique in 1960. He created a Noughts and Crosses (Tic-Tac-Toe) AI called MENACE (Matchbox Educable Noughts And Crosses Engine) using nothing more than matchboxes, glass beads, and random trials. You can read some about it here:
http://www.adit.co.uk/html/menace_simulation.html

Posted by: Bytestream Game Studios Jul 9 2006, 05:28 PM


im intending on applying the ai methods from this topic in an rts game im making.

The idea is that you create a new ai and give it a name such as Bob. you then play against Bob and he'll learn a few things (training).
You can also play allied with Bob against other computers or humans.

There is obviously a "glass ceiling" as such that the ai cannot go beyond in terms of skill, but generally, a better player will be able to train a better ai.

Perhaps the player can even have the option to manually train the ai from a screen of events and actions. i.e the player sets the ai to train 5 warriors and two archers and then attack the left flank, etc.

what do u think?


I think it will be a pretty good game.


well im working on a basic ai

im thinking it will write the last couple of steps that occured before its death so it can try to avoid them again


That is a good idea Cdaghostie. I hope it works for you.


(Bytestream Game Studios @ Jul 8 2006, 11:26 PM)
learning from the human is a step up yet it is still not really learning.
*



I MUST WARN THIS IS AN EXTREMELY LONG POST!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

That is not true. First it learns from the human it fights. The game is multiplayer so then you put your bot against another person. Then the bot does the same thing to it's oppenents. So that way even though it is predefined it still is unpredictable.

So you have a competition then the bots that wins is pitted to the next round and so on and so forth. There will always be new players and restarts so that way it doesn't ever have the same fighting style twice in a row. If that does happen then it would be predefined and then you would know the bots owner is cheating and therefore disqualified and can never enter again.

On the other hand what you do is you make different styles like defensive or offensive then it improves that ai instead of the all the coding. On the other hand still it could keep the same coding if it keeps on winning. So a human player could be new to the game and press any buttons to win, then when they go into the next round they would see there tactic and attack when all of the bots stamina is fiished, then on the next round the ai would do the same and change it's coding.

The way to truly perfect replicating a human brain though is to make feelings for the ai only for each different character and that it only changes some of it's coding to make it's own style like what players can do in Prince of Persia: the two thrones. If that can be done then you are forced to create your own fihgting style to teach your ai so that it can be almost undefeatable. When it is finally defeated then you create new fighting styles to beat your oppenent to defeat the champion.

If the champion stays on to long then one of the creater's finds out his style and counters it perfectly. That way you keep the game interisting to make people play it for a long ime until you bring a better version out. Once players get bored of it you have already created a new version binging in prizes for only the best of the best to get like a free uprade in something.

Once people won't play it any more you bring out a sequal that brings new elements in.A few such games series is Prince of persia and hitman. A game like that can be 2d but the graphics will have to be execellent and players must have freedom in the game. If players have enough freedom they will play it because it will help them feel like thay are able to do things in wich thay weren't able to have in any other games they have played.

If the human finds it anoying there will be a special tourmenent for them to fight in. That way they can feel proud. The only problem with my AI post is that it can't replicate the human brain and therefore it can not be proper learning AI. The reason is because a human can take the data they can see and immediatly see how to defend themselves, So if you create t so that it cann see the last piece of coding as it happens then the ai would be able to fight for a longer time. I will post an example soon. Sorry I can't think of anything else right now.

Edit: Take bet on soldier for isnstance, I personally have never pllayed the game but if you mix that with prince of persia or assassins creed or all three you could get a brand new game using only BoS and PoP and then you could add splinter cell or assasssins creed to make the sequal.If you mix a game like Rise of Nations: Rise of Legends and Age of Empires 3, you would get a game that is oldschool with execellent graphics. If a company or person puts the gameplay of AoE 1 and the graphics of Rise and Fall then you would have a game that is simple but complex. If you have played RoN:RoL then you would know of a good attempt to receate simple gaming.


Razard



Wow that is a long post. I think it would be cool to see bots fighting each other in tournaments, and learning the other's tactics.


ok yeah that was a stinking long post

same thought here

only i've thought of four problems as i've been thinking of creating the ai

how many instances and steps to take into account before death
and what variables- im thinking all bullets that are near, its relative location to walls for now

won't it "die of old age" when it trys to execute to much code with too many ifs - this is bound to happen anyway

how to execute the saved code so it doesn't slow gm (right now i think of im going to have to use the execute_file())

how to set it to go out and attack(complicated exp.)



how many instances and steps to take into account before death
and what variables- im thinking all bullets that are near, its relative location to walls for now

*



Fine so we make it like Bytestream Game Studios example and add my coding to it. What you do though is add diffeent aspects like moving around, etc.



how to execute the saved code so it doesn't slow gm (right now i think of im going to have to use the execute_file())

*



We can add an anti lag system to the game.


Razard

Posted by: Bytestream Game Studios Jul 11 2006, 06:57 PM


Razard:

My example was only a simple demonstration. We should design a new one here, and then put it together using the same method but decide on every aspect first so it can be coded accordingly.

I was also thinking of a new system based on the following:

script_aiaction

CODE

var random1;
var random2;
var random3;

var action1;
var action2;
var action3;

random1 = ceil(random(10));
random2 = ceil(random(10));
random3 = ceil(random(10));

if (random1 = 1) then (action1 = "jump");
if (random1 = 2) then (action1 = "shoot");
if (random1 = 3) then (action1 = "grenade");
if (random1 = 4) then (action1 = "duck");
if (random1 = 5) then (action1 = "block");

if (random2 = 1) then (action1 = "jump");
if (random2 = 2) then (action1 = "shoot");
if (random2 = 3) then (action1 = "grenade");
if (random2 = 4) then (action1 = "duck");
if (random2 = 5) then (action1 = "block");

if (random3 = 1) then (action1 = "jump");
if (random3 = 2) then (action1 = "shoot");
if (random3 = 3) then (action1 = "grenade");
if (random3 = 4) then (action1 = "duck");
if (random3 = 5) then (action1 = "block");




external data file:

CODE


if the action is successful, we increase the amount of times it has been successfull in the data file.

The next time it chooses an action to perform, it will have a better chance of choosing the successfull action again. For instance:

successfull jumps: 5
successfull blocks: 2
successfull ducks: 3

the ai now has a 50% chance of jumping, 20% chance of blocking and a 30% chance of ducking.



and what says the jury?

BGS.



and what says the jury?

The jury says we find the defendent a genius in the field of ai and we hope he provides an example to suit his findings and then we shall make a game out of it.

The jury (aka (Razard) )


Who here has played War World:Tactical Combat you will notice the the ai is never standing still except at the the beging and it will randomly choose its's equipment. So if you make a 2d version of that combining that with my prevous idea. You will get a cool game.

Razard


here's a concept of "learning" AI that could be implemented into a game style such as mortal kombat or street fighter:

-the AI character always faces the human character(a given, i know)

-the AI is pre-programmed with basics such as types of attack, movement, block ect.

-the AI randomly moves towards/away from the human character

-the AI randomly chooses a mode of attack when within a certain distance of the human character

-the AI randomly chooses a mode of block, counter attack, dodge when the human character is attacking within a strikable distance of the AI character

-the AI records every event of movement, attack, block, ect., and positioning of both the AI and the human character relative to each other

-the AI records every succesful hit, block, etc. made with/from each type of attack, block, ect. by both the AI and the human character

-the AI re-calls an event of movement, block, ect. when the human character presents the AI with a certain type of hit, etc., and executes the code for the block, movement, counter-attack, ect. that has the greatest value of success in the AI's favor


now, as for why i put learning in the "" at the beginning, there is a problem with this and with other suggestions of the "learning" AI dilema. everything it does at all times is basically all pre-programmed. sure, its pre-programmed with the basic movements so it can generate and record all possible succesful variables in its favor then execute the best option for the situation it is presented with. but i dont think i would call that completely learning. its programmed to execute given variables randomly and then you have to program it to execute those variables again when the AI is presented with a certain situation so the end result will side the AI's favor.

so i would call all this "randomly pre-determined AI that eventually turns into a fully pre-determined AI"

just my thoughts on the subject tho. whistle.gif

Edit: you may as well just save your time and just write the AI to execute certain variables when presented with a certain situation right from the start instead writing up code for it to take its time getting to that result


Not sure if this was linked in the past 10 pages or not, but there is a game that shows off pure learning AI at http://nerogame.org/


few links b4 they get lost:

ENGINES
http://www.rocketsoft.gm-school.uni.cc/uploads/TDS%20Engine.gm6- aaron7215's rts engine
people may use to create an ai

INFO LINKS
www.a-i.com - site about their creation of an ai - link added by ozakigames
http://www.adit.co.uk/html/menace_simulation.html - link added by xot
www.nerogame.org - 31 mb ai game - link added by GearGOD
EXAMPLES (GM6.1 Editables)
http://www.coolteens.co.za/games/LEARN_AI.zip

back to my post:
execute_file() doesn't work in the step i figured it wouldn't but it just hangs up the game
im going to upload my Cube AI editable since the AI part is non existant and BGS Studios's code example would work perfectly since i made scripts for every action
(so the ai can call them)


I just had a brainwave. My brainwave is that what you do is you take my long post and then combine it with WW:TC because WW:TC does sort of what we are talking about. What we do is we also combine that idea with nerogame (Thanks to geargod some of us now know about it) and then you would have adapting AI wich in my opionin is better than AI that learns because AI that learns will one day be predictable so adapting AI will not be predictable.

Razard
Posted by: Bytestream Game Studios Jul 15 2006, 08:13 AM


^^^^
agreed.
I havent played WW:TC.

BGS.


The reason why AI that leans would be predictale is because once it finds it's perfect strategy it would always use so what adapting AI would react immmediatly where as AI that learns doesn't the only way to make learning AI unpredictable would to combine the two different types of AI.

Razard


(Bytestream Game Studios @ Jul 15 2006, 10:13 AM)
^^^^
agreed.
I havent played WW:TC.

BGS.
*



In will post a folder allowing you to play the full the game if you want to so that way you can see one of the games I like to play. It is only 514mb to download and play.

BGS.

Razard

aka(the jury)

Posted by: Bytestream Game Studios Jul 15 2006, 09:06 AM

is it a free game then?


No

Razard


I gotcha. (got confused for a second)

So basically its like this:
AI Learns and it adapts

ex. It knows it can kill you with weapon a. but it doesn't have weapon a. so it would look for a power up so its weapon was stronger

(something like that)

also what is ww:tc?

id sure like to dl to see what it is but id be here a while with AOL dialup

also, im kinda lost what is getting our GMAI added to it




Guys, are we talking about super smart ai or learning ai. What are you trying to do? Just because the ai learns doesn't mean it has to learn the right things.

Example: Person walks down street, sees red rock, then sees cloud. Next day same thing happens. Therefor person may think clouds come when they see a red rock. They still are learning. Just not getting smarter.

(This is how superstitions form)


Hi all. Cdaghostie WW:TC is War World: Tactical Combat. We are talking about both z4000.

Razard


Ok razard, I was just trying to point out that this was about making learning ai, not just ai. The main topic was drifting away and I wanted to bring it back.


Sorry z4000, I didn't quite understand what you meant. I wasn't being sarcastic because your reply sounded like I was being sarcastic.

Razard



Guys, are we talking about super smart ai or learning ai. What are you trying to do? Just because the ai learns doesn't mean it has to learn the right things.

Example: Person walks down street,  sees red rock, then sees cloud. Next day same thing happens. Therefor person may think clouds come when they see a red rock. They still are learning. Just not getting smarter.

(This is how superstitions form)
*


That is the process I think we are talking about. Because if the next day, clouds form, and there is no red rock, they will learn that a red rock does not cause clouds. That process is exactly the same way humans learn, and that seems to have worked well for us.



Timewarp, think about this.

If person feels sad when someone hits them, person may think sad=i got hit.
If someone robs person and person feels sad. Person may get rid of sad=i got hit and replace it with sad=i got robbed. Then person will get hit and feel sad. It will loop and person will never learn to avoid anything.

(in that example, think of sad as variable bad.)


If a person could only remember one cause per emotion, nobody would be intelligent.

Think about how we work. To put it in a programming kind of sense:

emotion_sad[1]=punch;
emotion_sad[2]=rob;
...

Thus we learn what events cause a particular emotion. If that's emotion is bad, we learn ways to avoid these events. If an unknown event comes along that causes an undesirable emotion, we add that event to our list of events to avoid.



Timewarp, think about this.

If person feels sad when someone hits them, person may think sad=i got hit.
If someone robs person and person feels sad. Person may get rid of sad=i got hit and replace it with sad=i got robbed. Then person will get hit and feel sad. It will loop and person will never learn to avoid anything.

(in that example, think of sad as variable bad.)
*


That is not the way it should be implemetend, though. Emotions are not an action, therefore, no effect should be attributed to it. It should be based on a cause and effect basis. Example; the computer hits something, and something hits back. Therefore, it can determine that a counterattack will follow an attack. If the computer then decides to attempt to block any attack, then, it could determine that an attack equals a blocked counterattack, and that cound be given a positive value. The AI could always make an attempt to keep its 'score' positive.




If a person could only remember one cause per emotion, nobody would be intelligent.

Think about how we work. To put it in a programming kind of sense:

emotion_sad[1]=punch;
emotion_sad[2]=rob;
...

Thus we learn what events cause a particular emotion. If that's emotion is bad, we learn ways to avoid these events. If an unknown event comes along that causes an undesirable emotion, we add that event to our list of events to avoid.
*



I f a similar event to the bad events looks like it is happening then the AI should try and aviod.

Razard



Well, there's also a factor that heavily interact with learning:
personality.
For example : Lazy persons don't WANT to learn.
I have made several games with enemies that have a own personality.
Personalities are just a set of random defined variables.
For example : A ship moves towards his enemy when he spotted it,
and when it's out of his view range he goes seek it again.
The 'range' it uses for recognition is in normal games always simply predefined.
I NEVER do that. I let it randomly pick a fitting values.
Play with this alot, and you get 'random thinking' enemies.
If you really want 'living' programs that learn,think .etc,
it's just a case of making alot of variables, possibly random ones.


GameMakeMan brings up an interesting point. Randomization can give the player the impression of a more dynamic opponent, a more life-like, less robotic opponent. The AI might not be any smarter, but the player will find it less predictable and more natuaralistic. Randomization also inserts the chance of the AI making a bad decision. We want our enemies to be fallable, we like it when they make a mistake and we see a chance to take the advantage. An enemy that never makes a mistake is not fun. In games, fun is at least as important as smart AI.


GameMakeMan idea is very good. I think we should work more on the learning part though before we make it realistic. I will try in my spare time to make an example.



GameMakeMan idea is very good. I think we should work more on the learning part though before we make it realistic. I will try in my spare time to make an example.
*



maybe Mark Overmars should make work of it in GM 6.2?



It would be interesting to see if someone had the energy to write a simple game (say tic-tac-toe) that actually learned from playing, rather than from following its programmers ideas. It is not such a complex task, really. But it'll take a lot of playing for it to become really good.






Hi guys,
A.I. is quite cool thing wink1.gif
ok, just for your consideration:
not long ago a friend of mine made a fuzzy-logic pattern for "squad intelectual control" and add a "random-pattern" type squad.
Applying AD&D ruled for inflicting DAMage he put equal squads but of different control type together...
(It was his pre-Diploma for Cpp project but I don't think GM will react differently.) So, about 2/3 out of 1000 "battles" (~75%) RaNDom RuLeZ! He said it was a tend- average statistical data. And it didn't really matter simultaneous, turn-soldier or turn-based calculation was implemented. He used both built-in RND function and Normalized one- the same.
When he added XP status- it got even worse - random beserking!

I think that somethimes people like when "smart" cpu go bruteforce or act a bit stupid (like them) because it's fun and games ought to be such wink1.gif
cheers


Stages of intelligence(not just for an AI)
1.Randomization (Just have a reaction)
2.Learning(Learning which reactions work and don't)
3.Thinking(Thinking about your options and current advantages)

Most games have steps 1
Most games don't have step 2 because the enemy doesn't "live long enough"
Some games have step 3 (Halo 3-read thier thing of the week-)

offtopic:
I'm currently leaving the GMC for DS Dev. I can't do two projects and once and I won't try. I'm porting Cube AI. I've learned you get everything you ever wanted from Game Maker (Speed, low ram usage), but you lose all those pre-built functions. Maybe an AI will come to us there and I could port it back. I'll be back, I always have.


About 10 out of the 243 topics here have been useful. Read through it before posting something that's been posted 15 times already.

With Game Maker, we currently have very limited AI possibilities. You could go with the evolution route, where randomly genereated variables are given to each bot, and the ones that accomplish their goal, such as getting close to an object, are respawned with only slightly altered variables. The bots who didn't get anywhere close to their goal would be erased and new bots with completely random variables are made. Eventually, the bots will almost always reack their goal.

This is a very fast method, but can only be used in certain situations.


A nice example of this would be 10 creatures using the A algorithm to find a piece of food. Different variables would be assigned giving different "weights" to the different objects, terrain, enemies, and locations around them. Eventually, after 10-15 generations, they will probably almost all be able to find the food with ease.

There's also using an immense array that memorizes huge numbers of variables about something's surroundings and adapts to them (I think this is using "neural networks"). An example would be if a man gets shot, it will memorize all the objects around him, and if he gets shot again, he'll memorize everything again, and compare it with the previous times he's been shot. If there were 20 objects near him each time he got shot, and 3 of them were there BOTH times he got shot, then he might avoid those objects from now on. Eventually, he'll narrow it down to the 1 or 2 that shot him.

Not only is this a very time-consuming process to impliment into a game, but it can also be very costly when many AI instances are used at the same time. BUT, it can make some of the most intelligent things you'll ever see if you spend a long time making it. Using Game Maker to do this is probably a very bad idea.



Basically, any advanced learning AI is going to require a fairly large to gigantic array. Either of these, combined with a bit of Fuzzy Logic, will make very realistic AI. Anyone else have any suggestions? I might try making some evolution AI, but http://nn.cs.utexas.edu/NERO/about.html is a good example of this.



With Game Maker, we currently have very limited AI possibilities.


Quite frankly, I don't agree at all. AI is not built into any language. Its' desigend by someone using the language. There is nothing stopping you from creating advanced AI in GM, except the competence and energy of the programmer.


Basically, any advanced learning AI is going to require a fairly large to gigantic array.


That's a very narrow view on it. I can assure you it is very well possible to build AI with no arrays at all. (list, stacks, trees, they can all be made good use of. And as a matter of fact you can build structures of objects, using nothing but individual variables pointing at objects). And what would the problem be with actually using arrays, anyway?

AI is a complex topic, but I don't think that GM itself is really the limitation to what level of AI is achieved in GM games. Maybe the execution speed of good AI algorithms in GM would eventually become a limitation, but I think the real limit is set by the programmers writing the games.

My 2 cents




list, stacks, trees
They basically are arrays. Mark put these in so that users could have more functional, yet faster functions for data handling. There is nothing that cannot be done with an array.





list, stacks, trees
They basically are arrays. Mark put these in so that users could have more functional, yet faster functions for data handling. There is nothing that cannot be done with an array.
*



An array (as well as any other datatype) is defined by the operations you can perform on them. An array is an array. A stack is a stack. And a tree is a tree. A list is hence NOT an array.

Implementation of these is another thing



OK... I made an example and will upload it soon (not on the same computer). So far the adapting ai plays against random ai in a shooter. The variable the ai is learning, speed. If the ai wins, it saves the speed in a .txt file. If the ai looses, it slightly modifies the .txt file randomly.

This may seem like it is very simple (it is) but it works very nicely. After one or 2 trys
the learning ai almost always wins.

I am going to edit my example before I upload it. I will try to make the ai play a player instead of a random ai. I will try to make the ai also save the differnce between the direction it was going and the direction to the player.



An array (as well as any other datatype) is defined by the operations you can perform on them. An array is an array. A stack is a stack. And a tree is a tree. A list is hence NOT an array.

These are all arrays. The only difference is that they are all compiled and made to do specific functions. None of them would be of great use in AI. A list can be used to sort huge numbers of arrays, but AI doesn't need to sort anything. It needs to memorize things, and unless you only want 2 or 3 variables, you will have a very large array.

20 variables, 20 AI, when each one dies another one replaces it. That's 400 variables being exchanged every few steps, changes and logic being performed depending on the values, saving the values in a seperate file for later use... processing power stacks up VERY quickly.


Quite frankly, I don't agree at all. AI is not built into any language. Its' desigend by someone using the language. There is nothing stopping you from creating advanced AI in GM, except the competence and energy of the programmer.

I believe that you can make the most advance AI in the world using GM, but it will run at about 1 frame every hour. AI requires a lot of logical functions, and although GM performs these in only a few nanoseconds, that isn't NEARLY fast enough to make anything more than fair AI. C++, Assembly, Delphi, they're all hundreds of times faster than GM because GM isn't compiled, and it never will be.

The only way to do this well would be to use a DLL to do nearly all the functions, but because the AI involves so much stored information, a very large number of variables would have to be switched between GM and the DLL, which will slow things down. If used with DLL's, you could make a decent AI, but by itself, GM cannot make advanced learning AI.

However, if someone would prove me wrong on this, I would be more than happy.


OK... I made an example and will upload it soon (not on the same computer). So far the adapting ai plays against random ai in a shooter. The variable the ai is learning, speed. If the ai wins, it saves the speed in a .txt file. If the ai looses, it slightly modifies the .txt file randomly.


I think it would be nice if you'd finish this. I you make it, we can keep modifying it with more and more variables until it works well. Start simple and work your way up. If only one bot is used, and no "teamwork" betweeen multiple bots, we could make a very nice learning bot. However, with only one guy, I doubt we could get it to the point of "stunning".





An array (as well as any other datatype) is defined by the operations you can perform on them. An array is an array. A stack is a stack. And a tree is a tree. A list is hence NOT an array.

These are all arrays.



An array is an array. A stack is a stack. A stack is by definition not and array, because then it would not be a stack. A datatype is defined by the operations on it. Read a book on the topic if you don't believe me.

You can implement them in various ways, however. Like implementing a list using arrays. Or the other way around. In GM; it is probable that array is the actual implementation that is used as a base for the others. But that is a different thing.

And of course these datatypes can be used for AI. I don't see the point in saying neither that they cannot, or that they are the only way.

And about GM being too slow, I agree that obviously will put a limit. But with today's HW, you can still do quite a lot. Good AI was achieved on computers with a fraction of the performance of a modern PC.




A datatype is defined by the operations on it.
Yes, but the basic functions are arrays. That was my point entirely. But, we digress. Back to the ontopic discussion.





A datatype is defined by the operations on it.
Yes, but the basic functions are arrays. That was my point entirely. But, we digress. Back to the ontopic discussion.
*



We should go back on topic, I agree. But if you have time, have a look at this (which is the first I found doing a quick search)

http://users.evtek.fi/~hannuvl/material/algorith/linlist.doc

it sort of illustrates my point.


As for on topic:

which ever datatypes or algorithms used, I don't think that GM as such puts extremely sever limitations on what AI you can implement.

Speed, yes. But LISP (which is one of the most midely used languages in AI) is (normally) interpreting, and hardly very efficient as a language, when it comes to speed, either.

Suitability as a language. Well...it has its shortcomings, for sure, but none that would really stop you from implementing AI. Certain things might be more cumbersome than you wish, though

One of the key things is maybe that it is not good enough to put an enemy into a game that is supposed to learn over time, and have it start from scratch. By the time it has learnt something, the game is no longer being played. So, I think the AI has to be allowed to "train" in advance, or even be manually initialized to something reasonable

I am vaguely considering making a little program that lets two algorithms play tic-tac-toe against each other, and learn by experience. By letting instances play and evolve based on success, it can be made playing well before actually playing against human opponents. I have done similar things before (in C though. And LISP as well). It is just that I do not have the time, really.



http://host-a.net/getfile.php?usern=labmonkey&file=learningai.zip
(extract all files)

Only use my example for this topic unless you ask for permission.

I understand that this is very basic but it works well and is a good example of adapting ai.

If you are too lazy to look through the code (I would be) then this it what it does.

The square moves at speed 7 and changes to a random direction every 20 steps.
The circle starts moving at a speed of 2 and starts changing direction to moving at the square every 20 steps. If the circle looses, it changes it's variables untill it wins.

This method of learning may be slow if you have it working on 100 variables but it does work. But then again, a game with 100 variables may take the player some time to learn.

With this method, the ai may also learn the wrong thing, but humans do that too (superstitions) and it will give fun to the game.




I understand that this is very basic but it works well and is a good example of adapting ai.

Sorry to sound mean, but I don't see much learning invovled with this AI. It just changes something randomly when it looses, but it can lose even when it has the optimum speed, so it could easily become crappy again.

Here is something I made which uses not only a learning system, but also a unique sight system. A blue circle tries to get to the green square, and although it may take a while at first, within two minutes it should reach the square within 5 seconds. It uses a grid where every 32x32 square is assigned a score depending on it's usefulness, and if the square being looked at is good, the circle will move toward it. When it reaches the goal, the last 15 grids the circle was on will have an increased score the next time. Right now it's VERY basic and unoptimized, but a good example in my opinion.

http://www.freewebs.com/cory_lehan/learner.gm6

Watch it for about 2 minutes, and then if you want to see how it works, press spacebar. I think the sight system is kinda fun, but too random. Sometimes it will pass it's goal right up. Multiple goals can also be made, and you can make your own rooms to try it out.

Just please don't waste your time trying to understand the code. It's very unorganized and even I have trouble reading some parts.



well, making ai capable of learning would take a lot of patience and hard work. but we shouldn't look too far into making the code learn more but further into how we humans learn or react on solving problems. well, at least that's what i think.


(Wow, this really is one huge topic!)
I read on one of the many previous pages that someone said:


Making learning AI is simple:
CODE
if headshots>10 then scrDuck()

Then it would be pre-programmed!
Which is not what we're trying to do...

Something like that.
Well. EVERYTHING must be 'pre-programmed'.
How will it recognize a problem?
From a pre-defined array list for example.
How will it learn it's cause?
Predefined variables of it's surrounding.
How will it learn how to avoid them?
Predifined scripts.
It is innevitable.
Not ALL of it has to be pre-defined, but at least 75% has to.
But because there is a chance this has been said already I'll say something else too.
This might just sound cheesy but look closely at this quote from I-Robot:

"...random variables,unecpected segments of code,
which can be interpret as creativity,personality.
Even the possibilty of dreams!"

Okay, ignore the "dreams" part. That's just movie-crap.
But the rest is true!
It can/has been done.




Well. EVERYTHING must be 'pre-programmed'.
How will it recognize a problem?
From a pre-defined array list for example.
How will it learn it's cause?
Predefined variables of it's surrounding.
How will it learn how to avoid them?
Predifined scripts.
It is innevitable.
Not ALL of it has to be pre-defined, but at least 75% has to.


I think you are missing the point.

If you can just define the rules for the world, and what is a "good" result, as oppsoed to a "bad" result, the AI is to be working in, you can have it "learn" things that you yourself did not know. Which is in some sense the whole beauty of it.

Genetic algorithms work rougly like this:

1) it creates a number of data structures representing "individuals" with a certain "behavoiur"

2) it lets all this "individuals" try to solve the problem the AI is supposed to solve

3) some fails, some fails badly, and some may perform well. The best ones are kept, and a bumch of new ones are created (e.g. by making combinations of the already good ones, or by applying some random minor changes)

4) it goes back to step 2

eventually, some of the individuals will be very good at solving the problem. As a matter of fact, they may well have solved a problem that you as a the programmer could not solve yourself.

The problem the programme is facing is not solving the actual problem, but
a) specifying the rules that apply
cool.gif specifying how to calculate how good an individual is
c) specifying how the individuals combine or evolve into new indivduals, in a way such that they have a reaonsable chance of keeping the good characteristics of its ancestors, while at the same time being different.

Neither a, b or c is easy, and it is likely that you end up with a completely useless set of individuals, randomly chaning to even more useless ones, if you do it wrong. But it is often easier than solving the inital problem


I have done a few excercises in this area, and I can dertainly assure you that the program size is extremely small in comparison to the data it is creating, changing, and evolving, into something that actually works

I am not saying this kind of method necessarily works in a game. By the time it is any good, the guy who bouoght the game is maybe dead. Or at least tired of the game. But if you (the designer) apply it outside the game, and let it "train" or "evolve" until it is reasonably good, you have somthing which may be more of a natural AI than you would ever be able to hardcode . And it can still be made to evolve further as it is being in the game.

I would not advice anyone to start tryint this for a fighting game in GM; though. You'll kill yourself (well, not Yourself, but...yourself). But for tic-tac-toe, checkers or any other fairly simple game, it is definitely within reach. Chess...well...that'd take quite some effort. But I would not consiedr ot impossible.



@jesterdaze:
i like your way of thinking.

more concretely for others, we want to apply this AI for a game don't we? so we don't need to bother reproductivity, evolution of physical... what we need is an AI that learns how to fight! so we have to do only few things for it to work. not to say that they are easy though.

1st of all - explain the rules to the AI. This is the base of your AI, what are his ressources, what he is able to do and not. this will include waypoints set in the level so he's not lost in it and when he gets the order to "move", he don't start moving in just any direction, he goes to waypoints. there is no need for our AI to learn the level... so we can show him. the idea of a learning AI is to make it more human-like, not to make him a complete human.

2nd - the goals - kill, have health high, don't die. the AI knows it can shoot.... but what the **** is shooting? experiment! ... ooh! it makes holes in the wall! good... but that doesn't bring me to my goal. oh that thing is moving, let's shoot it! .... it died! and gave me 1 kill! oh wait! that's my goal! so i need to shoot the moving thing that holds the same thing i killed him with!

You don't need to code a learing curve for precision! You need to set it to be harder the more it is set difficult by the player.

so basically, a new AI will shoot everywhere (in a fps), move anywhere, jump at anytimes, shoot on weapons, try to pick up players, use health pack when out of ammo.

3rd - the AI knows what are the tools he have. the AI knows what are his goals. now he needs a structure to determine when to use. there is the big part of the AI... keeping track of what he did and taking decisions according to it. this will include player in sight, player facing me, my weapon, his weapon, my health, his health, position in the map, with weigh to each of these decisions if you want to control the AI. so let's say i want an AI that runs when not a full health, his health will be more critical when "choosing" what to do next. another funny example, you want the computer to chicken away when you use the weapon you killed him the most with, which means that when you have this weapon, he can't defeat you so he'll run and find something else, health, weapon, position, to best defeat you.

then again, goals are to be prioritized too! if picking the flag is more important in this match, the AI will care less about killing and more about surviving and getting the flag back to his base! one of these goals can also be protect a friend, get your flag back, keep your flag from being captured so you don't need to get it back, but still with a greater weight on capturing the enemy's flag.

there maybe things that i forget but this is the BASE of a "learning" AI, doing more than this would be useless since our application don't supporte doing more.


Just a comment to my own post:

Whe advicing not to try, I refer to the full genetic algorithm thing for like a FPS. I am quite convinced you need to take a lot of "shortcuts", to come to a reasonably behaving enemy within reasonable time.

I am not saying you should not try AI in general for a FPS. Of course you should!


Alot of you people speak the same way as sites to "learn paranormal actions" like telekinesis.
For the ones that don't know what I'm talking about, compare :

Focus on the object, "feel" that you're "connected" to it, and then move it like it's part of you.


..let the AI recognize what the problem is and how to solve it..

It's the most easy tutorial ever. It's a "just do it" turorial.
We don't need to know WHAT to do anymore.
We KNOW how we humans think. The question is, HOW to make AI think/learn too..
I'M not missing the point jesterdaze. YOU are.




Alot of you people speak the same way as sites to "learn paranormal actions" like telekinesis.
For the ones that don't know what I'm talking about, compare :

Focus on the object, "feel" that you're "connected" to it, and then move it like it's part of you.


..let the AI recognize what the problem is and how to solve it..

It's the most easy tutorial ever. It's a "just do it" turorial.
We don't need to know WHAT to do anymore.
We KNOW how we humans think. The question is, HOW to make AI think/learn too..
I'M not missing the point jesterdaze. YOU are.
*



We may talk about different things, but I am not missing the point I am making.

It is easy to get into existential or almost religious discussions on this topic, or into discusisons about free will and the soul, etc, and think I refrain from continue the thread, therefore.



Well I do agree with what GameMakeMan says, the fact is, I didn't have time to post all my toughts because I had to go to sleep.

I wanted to make my post more concrete. Say an exemple of how implementing this to GML. But my time is small for the moment I can only explain my idea and not program it. It will come, don't be afraid tongue.gif

I'll explain the 1st level of my "learing" AI because again, I don't have much time, I have to go to work (but I finished it at work). Let's say it's a top-down shooter where you can pick-up weapons and health and plating.

Put objects called "obj_waypoint" in your room, these will be the important part of the level since we don't want our AI to learn the whole level but to adapt to enemy strategies. They don't need to have a sprite (you can just put one if you want to be more organized when creating your levels, but then, put it invisible, or visible only in debug mode). These will be for the AI not to be lost in a new level. You need to put simple code in their create event so that they make a link to the next waypoint if it has an uninterrupted straight path to it. Will be used for pathfinding and quick way of knowing where to go in the level, which way to pick to get there fast.

For its possible actions, a simple array will be used. ai_possibleaction[0]=<script here>. The script is in fact a script that make the AI look for the closest waypoint and asks the next node to get to desired position. Also requires path finding. Create another variable named something like ai_possibleactionnumber=1. This way, the AI knows that there is only 1 thing he can do, move to the next waypoint to get to desired location. Desired location is a (x,y) coordinate and the script calculates the path using the closest waypoint that has an uninterrupted straight path to it. This still is not a learning AI, since it only finds his path through the level.

Two dimensionnal arrays. They are really useful. Say you want to associate one goal (1st dimension) with multiple actions (2nd dimension) and set it this action helps for this goal. There are too much things to think here to be able to really discuss of a learning AI on a forum. What is needed to really come up with something good is a team working together in the same room on 1 AI that can "learn".

Data structures! Find a way to make them profitable because they are VERY powerful when you come to think of processing that old information is not good anymore because let's say the player changed tactics.

My concept of a learning AI is far more complicated that what I just explained. But if you want to work that AI with a game, there are so much things that don't need to be learned, which you can just teach the AI like you just teach a dog to do tricks. The difference is, the AI won't do mistakes unless you teached him to do some. That is another thing. You don't need to make the AI learn to aim. You want it to learn that walls offer cover, enemies don't shoot when facing off you, this weapon deals more damage than this one, this enemy with this weapon is deadly for me.


Note: i only read the 1st page before wirting this so don't flame me if this has already been said.

IF a working version of this AI comes about (say for chess or something) coudn't it be used to tailor the game to the player?

say everytime the player looses the square thought of a good normally would be maked as bad, and it would be changed normally if the player wins (there by making the game eaiser if the player looses and harder if he wins



Note: i only read the 1st page before wirting this so don't flame me if this has already been said.

IF a working version of this AI comes about (say for chess or something) coudn't it be used to tailor the game to the player?

say everytime the player looses the square thought of a good normally would be maked as bad, and it would be changed normally if the player wins (there by making the game eaiser if the player looses and harder if he wins
*



It pains me to read this, not only because it is difficult to understand, but also that it is quite possibly the most common thing said in this thread. This is probably close to the 50th time this has been said.

Posted by: 2% Milk Jul 29 2006, 07:39 AM



Well. EVERYTHING must be 'pre-programmed'.
How will it recognize a problem?
From a pre-defined array list for example.
How will it learn it's cause?
Predefined variables of it's surrounding.
How will it learn how to avoid them?
Predifined scripts.
It is innevitable.
Not ALL of it has to be pre-defined, but at least 75% has to.


I think you are missing the point.

If you can just define the rules for the world, and what is a "good" result, as oppsoed to a "bad" result, the AI is to be working in, you can have it "learn" things that you yourself did not know. Which is in some sense the whole beauty of it.

Genetic algorithms work rougly like this:

1) it creates a number of data structures representing "individuals" with a certain "behavoiur"

2) it lets all this "individuals" try to solve the problem the AI is supposed to solve

3) some fails, some fails badly, and some may perform well. The best ones are kept, and a bumch of new ones are created (e.g. by making combinations of the already good ones, or by applying some random minor changes)

4) it goes back to step 2

eventually, some of the individuals will be very good at solving the problem. As a matter of fact, they may well have solved a problem that you as a the programmer could not solve yourself.

The problem the programme is facing is not solving the actual problem, but
a) specifying the rules that apply
cool.gif specifying how to calculate how good an individual is
c) specifying how the individuals combine or evolve into new indivduals, in a way such that they have a reaonsable chance of keeping the good characteristics of its ancestors, while at the same time being different.

Neither a, b or c is easy, and it is likely that you end up with a completely useless set of individuals, randomly chaning to even more useless ones, if you do it wrong. But it is often easier than solving the inital problem


I have done a few excercises in this area, and I can dertainly assure you that the program size is extremely small in comparison to the data it is creating, changing, and evolving, into something that actually works

I am not saying this kind of method necessarily works in a game. By the time it is any good, the guy who bouoght the game is maybe dead. Or at least tired of the game. But if you (the designer) apply it outside the game, and let it "train" or "evolve" until it is reasonably good, you have somthing which may be more of a natural AI than you would ever be able to hardcode . And it can still be made to evolve further as it is being in the game.

I would not advice anyone to start tryint this for a fighting game in GM; though. You'll kill yourself (well, not Yourself, but...yourself). But for tic-tac-toe, checkers or any other fairly simple game, it is definitely within reach. Chess...well...that'd take quite some effort. But I would not consiedr ot impossible.
*


Umm.......... What the heck were you talking about?
Basically, to make learning AI you need to:
1) Make it know what's going on and
2) How to react to it.
You need it to look for patterns the player is using. If the player always hides behind a wall and waits for the ai to come to it, you should have do something to get the player out of there. Never use fixed values. (<<already said, i know.) You could use some 'catch on' variable (that would be a fixed one) that would dictate how much the ai 'catches on' to what the player does. Like if the catch on variable was a 0-100 thing, 0 would make it very stupid, and a value of 100 would make it impossible to surprise more than once.



Umm.......... What the heck were you talking about?
Basically, to make learning AI you need to:
1) Make it know what's going on and
2) How to react to it.
You need it to look for patterns the player is using. If the player always hides behind a wall and waits for the ai to come to it, you should have do something to get the player out of there. Never use fixed values. (<<already said, i know.) You could use some 'catch on' variable (that would be a fixed one) that would dictate how much the ai 'catches on' to what the player does. Like if the catch on variable was a 0-100 thing, 0 would make it very stupid, and a value of 100 would make it impossible to surprise more than once.
*

[/quote]


I gave up on this thread, and wonÃ¤t start again. We are discussing completely different levels of AI.



yeah.... computers dont learn by themselves, they have to be told what to learn
thats the hard part about ai you have to tell them they have to learn 0_o


all the ways of programing a learning AI were discussed.... it's about time we start thinking about how to make it in game maker. read my posts and elaborate on them. that should at least exclude some novices and advanced users. let's start to concretize this. read my posts, read my posts, and elaborate, elaborate. we need to advance to the next level. we know what is the problem and how to fix it. now let's do it so that the topic can advance!


^
Behold, the conclusion of what I'm saying all the time wink1.gif
He's right. Bring on your .gm6'es of Artificial Intelligence!
I will too when I made a descent one..
So far they have random intelligence and the ability to dodge -.-


The human mind is too complex for it to even comprehend itself. And we can't program what we can't comprehend, right? We think on such an amazing level and we don't notice. One thing that effects it is our comprehension of language. Every thought that passes through our mind is made up of words. If we imagine an image, we can't think about it without using words. So, this leads to the question of how do animals without language think? We can't possibly comprehend how they think, so we can only guess. My guess is that they don't actually think, but they act upon a series of many (amount depending on the species) instincts. These instincts can be re-written by how the being perseives its surroundings. Our mind also works like this. The larger the brain capacity, the better it perseives its surroundings. Our comprehension of language allows us to perseive what we see far better than anything else. And so, this allows us to re-write our own instincts far more than anything else, so we have the widest range of thoughts. Something like most insects cannot re-write its own instincts beyond what is current. And so, this is the level of thinking that most AIs are, because they can set variables for what is current, but they cannot rewrite its primal instincts, because that would require a program to program itself, which is currently impossible. Code cannot act beyond what is already written. Even the most advanced robotics, like that robot in Japan that you can talk to, is not beyond insect-level learning, because it cannot go past what its programmers put into it. It may be able to read expressions and understand speech, but all this is is a series of algorithms, and statements like "if user inputs this value, then proceed with this action".

There are different "level"s of intelligence. Humans are pretty high in all of these levels. There's thinking level, which is the level of which things perseive their surroundings and rewrite their instincts. There's memory level, which ties into the thinking level in most cases. It is how much something can remember, in how much detail it remembers it in, and for how long it can remember it. An example of where it doesn't tie in is in the Japanese robot AI, because it doesn't rewrite its instincts, yet it is programmed to remember the face and voice of the people it meets. AIs are really the only things that have high memory levels and low thinking levels. There's instinct level, which is how well developed the primal instincts are developed. This is kinda low in insects, really really low in bacteria, mid-level for most other life-forms, and high for humans and the Japanese robot. I can't really think of any others, but I'm sure there's more. You can get the main point from what I've written.

So, we cannot program any AI that goes beyond most insects, which is basically not being able to rewrite itself at all. We can, however, write AIs that mimic high thinking levels with high memory levels and high instinct levels.

An AI system that I devised (sorry if a similar system was already posted, its hard to read a 14 page topic, especially with posts as long as these are) would be as follows. There are many, many variables (obviously in an array), each ranging from -1000 to 1000. 0 is neutral, 1000 is positive, and -1000 is negative. Every action the user takes effects at least one or two of these variables. When the AI has to make a decision, it decides by checking the values of all the variables that have anything in common with the action, and the higher the average score, the more likely the AI is to go through with the action. Of course there is a small bit of randomisation involved, but too much and the AI won't be effected by its instincts much. If all the values are 1000 or close to, it will go through with the action without hesitation. If the average amount is about 0, then there's about a 50/50 chance. If all the values equal about -1000, there's a very low chance that this will happen. Of course, some actions will work in reverse, with more values equalling -1000 meaning the action is more likely to happen. This, coupled with memory of the user doing certain special things (using data structures or a similar thing), will make a pretty complicated but realistic AI. The more variables of course, the more realistic.

-Andruth


I've done with FSM and GA now I'm studying neural net.
I'm working on a tictactoe game which uses neural net,I think neural net is the only way to give the AI a real "brain" 'cuz it simulates the human brain.

In the game,you play with the ai,when you see that some moves is not good,you can change the computer moves.That's how you teach it.

@Gamemakesman:AI can really learn without any pre-programming although teaching it is difficult. This is one of the article which I used to learn neural net: http://www.ai-junkie.com/ann/evolved/nnt1.html .You can see how to makes minesweeper which will be more intelligent by time.However,I don't like that way of teaching.


I am already devoloping "artificial lifeforms" which will learn and adapt based on randomly defined variables.
Wich is, ironicly enough an RPG-like battle game with GRANDPA's in them.
Why is that ironic? Because RPG's seem to have dull AI?
No. Because old human beings tend to lose intelligence.
So far with their only limited possibilities it is not very stunning
with only the ability to dodge and recognize it's attackers behavior,
but when the huge script in teh step events is a large tangled web of code relevant from certain variables,
it will be quite an experience to fight those grandpa's.



all the ways of programing a learning AI were discussed.... it's about time we start thinking about how to make it in game maker. read my posts and elaborate on them. that should at least exclude some novices and advanced users. let's start to concretize this. read my posts, read my posts, and elaborate, elaborate. we need to advance to the next level. we know what is the problem and how to fix it. now let's do it so that the topic can advance!
*



^
Behold, the conclusion of what I'm saying all the time wink1.gif
He's right. Bring on your .gm6'es of Artificial Intelligence!
I will too when I made a descent one..
So far they have random intelligence and the ability to dodge -.-
*



Sadly, I doubt many will create a working AI. One of the most commonly said things on this thread is "I am working on an AI which is almost done", but AI is an INCREDIBLY difficult thing to make. You may think it's almost done, but then you realize that your logic wasn't foolproof, and it doesn't work anything like you planned. Many people will get about 90% done, but will give up after they can't solve those one or two problems.


http://www.freewebs.com/cory_lehan/learner.gm6

Watch it for about 2 minutes, and then if you want to see how it works, press spacebar.  I think the sight system is kinda fun, but too random.  Sometimes it will pass it's goal right up.  Multiple goals can also be made, and you can make your own rooms to try it out.

Just please don't waste your time trying to understand the code.  It's very unorganized and even I have trouble reading some parts.
*



I don't want to sound like I'm bragging, because this is a terrible AI example, but I feel that this is the only example with some sort of simulated learning. I'm done with trying, but anyone interested in making AI, watch it, realize it's faults, and THEN make your AI. The only way we'll make a realistic AI is to improve on everyone else's AI, not to make a bunch of little ones that don't accomplish much (like mine).

And DO NOT use my program. Learn from it, but do not use it. This is an extremely poor AI and is very poorly coded.



Hah. Really? I actually just finished it yesterday.
Unfortunately they do not live long enough to actually have the chance to gain much experience, but I assure you. It works.


Ok, here are two links that should help in this:
http://www.gameai.com/

and

http://members.aol.com/daharrell/


The First one is a good site on AI, the second is a site on the AI WHEEL, an actual learning AI.
What is does do that I don't like is define the objects and the interactions the player object can have with them, and what result they will produce, it would be nice to be able to perform any action on an object, and have a non-predefined reaction happen, rather than just being able to do two actions on something(Which is one of the things that killed Adventure games)

Best of luck, hope this helps, or at least sparks some discussion.



Ok, here are two links that should help in this:
http://www.gameai.com/

and

http://members.aol.com/daharrell/


The First one is a good site on AI,


And it has a reference to http://www.nerogame.org/, which is pretty much the kind of thing I was referring to earlier



I was just about to quit on AI when Schreib sent me a message to continue, so I did. I'm using a method of using many collision_triangles to determine which squares are visible and which aren't. Then a target is chosen depending on the scores of the visible squares. Then something similar to the A* Algorithm is used to get to that target.

I found that this is incredibly heavy, and GM isn't powerful enough to run this well, so instead of using DLL's to handle the tough stuff, I've decided to code it completely in C++ and use SDL for the drawing.

Am I allowed to post a link to a C++ program? I'd assume that I would be since it's related to the topic, but I want to make sure that no one would get angry if I did. It isn't finished yet (I'm just starting in SDL, so I still have to work some stuff out), but I think this will be a nice addition to the thread.
Posted by: Dr. Eechmen Jul 31 2006, 10:21 PM

I [attempt to] use "flags" or "markers" for simi-learning ai. This is basically a different version of the "grid" approach. When the AI gets killed they place a "bad" flag, which is avoided with a certain radius, and when it kills something or finds a health/ammo spawn etc, they place a "good" flag, which it is attracted to. If they detect an incoming "harmful" object, they avoid it. The detection can be constant or radar type. So far I've gotten the dodging down, along with the detection, the flags, on the other hand, are giving me a bit of trouble. Personally, I doubt I'll ever get this working, but it's worth trying.


(Dr. Eechmen @ Jul 31 2006, 05:21 PM)
I [attempt to] use "flags" or "markers" for simi-learning ai. This is basically a different version of the "grid" approach. When the AI gets killed they place a "bad" flag, which is avoided with a certain radius, and when it kills something or finds a health/ammo spawn etc, they place a "good" flag, which it is attracted to. If they detect an incoming "harmful" object, they avoid it. The detection can be constant or radar type. So far I've gotten the dodging down, along with the detection, the flags, on the other hand, are giving me a bit of trouble. Personally, I doubt I'll ever get this working, but it's worth trying.
*



This is a much easier method to start with, but much harder to master in my opinion, as you have to know the best place to put the flags, and when there are 50 of them on the screen at the same time, you have to know which ones to get rid of.

My example uses the grid method, but also uses an inverse distance to attract and repel, and the combination isn't good (such as when the scores are 0, it doesn't really go anywhere), which is why I'm reworking it to use a complete grid method and an interesting "sight" method.

I'd be very excited to see a nice example using flags, so keep at it! Maybe send a link to your work and we can help with any bugs.

Posted by: Dr. Eechmen Aug 1 2006, 03:52 AM

Here's a simplified flag example...It's horrible, but it demonstrates my Idea (no moving or killable enemies in this version, since they're not really something that the AI learns about). I'm working on a better one. Mabye a combonation grid and flag would be better...I don't know how, but it seems that It could be more precise...just a thought.

http://www.box.net/public/38bjs6bxnh
http://www.box.net/public/7dh9xod1sj //new version with tracking flags. (it's getting better...)

//edit- Moving eyes+grid+flag-inverse movement=much better ai..mabye we should try this...



(Dr. Eechmen @ Jul 31 2006, 10:52 PM)
Here's a simplified flag example...It's horrible, but it demonstrates my Idea (no moving or killable enemies in this version, since they're not really something that the AI learns about). I'm working on a better one. Mabye a combonation grid and flag would be better...I don't know how, but it seems that It could be more precise...just a thought.

http://www.box.net/public/38bjs6bxnh
http://www.box.net/public/7dh9xod1sj //new version with tracking flags. (it's getting better...)

//edit- Moving eyes+grid+flag-inverse movement=much better ai..mabye we should try this...
*



A nice example. The main problem with it that I see is that it can only turn left. This makes it very difficult to get to some places. It's a very nice start though, maybe we'll be competing to see which method is better. wink1.gif

Although, if I manage to finish the one I'm working on, which is a bear of a project, it will overhaul any AI I've seen made in GM (probably because it won't be made in GM smile.gif)
But because of my entire programming life being spent with GM, I'm programming C++ almost just like GML, such as giving everything a vspeed and hspeed variable. Basically the only difference is that I have to draw every pixel on the screen, so all these drawing functions are scrambling my brain, but I almost have it down. If I ever finish it, it should be done by Wednesday, as most of what I have to do is convert my GML AI code to C++, which should take about 6-8 hours.

Posted by: Dr. Eechmen Aug 1 2006, 01:11 PM

I have a new version of the "Flag" ai, but my file upload server is acting awry...I'll try to get the example posted soon.

//edit-finally...I've uploaded it, here it is.

http://www.box.net/public/dtyu90sq5s


(Dr. Eechmen @ Aug 1 2006, 01:11 PM)
I have a new version of the "Flag" ai, but my file upload server is acting awry...I'll try to get the example posted soon.

//edit-finally...I've uploaded it, here it is.

http://www.box.net/public/dtyu90sq5s
*


I don't want to be downer here but... Your AI has nothing about learning. Yes, it uses flags, but even then, the "good" flags don't even attract the AI. It's a good idea though, the flags, but use it another way. All your AI does is run the preprogramed functions when it encounters the flags. No learning here.

Also, learning AI is much more difficulte to do. 3 updates in less than 1-2 days would not be possible, unless they are simple bug fixes but even then, these bugs can and will be tricky to fix because the AI itself is hard to program.

Posted by: Dr. Eechmen Aug 1 2006, 02:04 PM

Three things- One, the 3 updates were mostly bug fixes and movement changes.
Two, Yes, the good flags do attract the AI.
Three, you're probably right about It not being an AI, I guess it's more of an imitation....though it's the same principle as the grid approach, which is considered an AI only because the AI creates the grid...The AI in a flag method "creates" flags...same idea, to some extent. Also, the grid AI reacts specifically to different characteristics of the grid; same with flags.

//edit-Think of my "AI" as a mine sweeper marking mine fields...or a construction worker marking underground pipes with flags...take your pick.



(Dr. Eechmen @ Aug 1 2006, 01:11 PM)
I have a new version of the "Flag" ai, but my file upload server is acting awry...I'll try to get the example posted soon.

//edit-finally...I've uploaded it, here it is.

http://www.box.net/public/dtyu90sq5s
*


I don't want to be downer here but... Your AI has nothing about learning. Yes, it uses flags, but even then, the "good" flags don't even attract the AI. It's a good idea though, the flags, but use it another way. All your AI does is run the preprogramed functions when it encounters the flags. No learning here.

Also, learning AI is much more difficulte to do. 3 updates in less than 1-2 days would not be possible, unless they are simple bug fixes but even then, these bugs can and will be tricky to fix because the AI itself is hard to program.
*



So could you explain to me a way to make AI learn without using pre-programmed functions? You might think there's a way, but there isn't. It is impossible. That's like saying make an object move to the right without using functions. Somewhere, several functions must be used. Just letting the AI "see" using a cone of a certain distance would be using pre-programmed functions, because you're limiting the AI to do only what you want it to do.

And 5 updates in 1 day is easily accomplished, it just depends on how easily something can be improved. I made my AI example in about 1/2 a day, so I think something can be slightly improved in the same amount of time.

As I said before, I think Dr. Eechmen's AI is improving nicely, and although it needs a lot of work, he's done better than anyone else in this thread. We have a LOT of talkers here, but if I remember correctly, only 4 actors.

If anyone's curious on how mine's coming along, it may take longer than I expected, as I can't just say "draw_text", I have to actually draw each pixel sad.gif . It took me forever to figure out how to set the fps. Expect it done Thursday.

Posted by: Dr. Eechmen Aug 1 2006, 10:07 PM

If you dropped a ball on the ground, would It become intelligent? No.
If you gave a monkey a banana, it would associate you with bananas, and seek you out.
If you shot a monkey, it would associate you with pain, and avoid you.

...Just to explain my point a bit...




So could you explain to me a way to make AI learn without using pre-programmed functions? You might think there's a way, but there isn't. It is impossible. That's like saying make an object move to the right without using functions. Somewhere, several functions must be used. Just letting the AI "see" using a cone of a certain distance would be using pre-programmed functions, because you're limiting the AI to do only what you want it to do.


Yeah, that's like how I exlained. And because of this, there's no real way to acheive true thinking. In order to acheive this, you'd have to have it able to respond to things how it feels like responding to it, even if it's not programmed to do so. Kinda like taking a human child to an alien planet. It wouldn't be "pre-programmed" or adapted to that planet, but it would still make out the differences and respond to it, even though it hadn't evolved to do so on that exact environment. In order to create "learning", one must create adaptability.

Posted by: Dr. Eechmen Aug 1 2006, 11:39 PM

Ideally, we will eventually be able to devise an AI that, though pre programed, "understands" and "evaluates" its environment as time goes on. It will learn what hurts it, and what helps it. It will learn how to avoid danger, and how to accomidate its needs. This, I believe, is everyone's goal for AI. We still have a long way to go, though.

I will post a modified Flag AI once I fix a bug...should be a couple of hours.

//edit

Here it is, version .4!
http://www.box.net/public/vqkgcdz5oa

//edit2 do you guys think that I should continue to develop flag based AI, or if I should start on a hybrid?

//edit3 my eyes are starting to hurt from working with the AI so much wacko.gif , so I'm turning in for the night... here's a minor update from the last one. http://www.box.net/public/cleo2yxt09



So could you explain to me a way to make AI learn without using pre-programmed functions?Â  You might think there's a way, but there isn't.Â  It is impossible.Â 


Read the http://www.nerogame.orglink.

The functions are changing in the learning process. Essentially, you tell the program what it should be good AT, not HOW to be good at it. And the program changes over and over again, until it becomes good at what you wanted it to be good at

The end result is a program that you did not write yourself. It has been created by the program you wrote, yes, but it does things in ways and in orders you never told it to do them. It will find solutions to the problem that you would never have thought of.

Posted by: Dr. Eechmen Aug 2 2006, 02:14 PM


Read the http://www.nerogame.orglink.

The functions are changing in the learning process.Â  Essentially, you tell the program what it should be good AT, not HOW to be good at it. And the program changes over and over again, until it becomes good at what you wanted it to be good at

The end result is a program that you did not write yourself. It has been created by the program you wrote, yes, but it does things in ways and in orders you never told it to do them. It will find solutions to the problem that you would never have thought of.
*



Jesterdaze has a point here, The grids and flags are useful, but they aren't exactly learning. True, the AI "Learns" what helps/hurts it by either placing a flag or marking a grid. This would be good, but we tought the AI how to mark the grid or place the flag. The only AI I've seen here that did this in a way was THE_oldy's ball AI, from Sparkworker's suggestion. We could apply his method to various diferent aspects of the game, such as INI and TXT files that act as scripts that the AI can alter. We have opened the doors that lead to successful AI, but we haven't stepped in yet. I reccomend we do so. I will attempt to cross my flag example with an INI or TXT (which will require a lot of learning on my part, since I haven't worked with these much before... tongue.gif). Good luck to anyone else working on a learning AI.

-Dr. Eechmen

//In my opinion, to do this, we must give the AI "emotions" "enjoyment" and "pain". It will evaluate how events effect its emotions, and determine which heal it, hurt it, make it happy, make it sad, and many more. I say we start with a basic AI, which will only have "heal" and "hurt". Other objects will have the effect on the AI in their collision events. Let's say the AI encounters a harmful object, and gets harmed. It will recognise the harm as pain, and write down the type of object as a hazard, and it will either A) have the instinct to avoid the object or B.) have to learn by trial and eror how to avoid the harmful object. When the AI encounters an object that helps it, it will take note of the object and associate it with good, and It will either seek it out, or learn how to seek it out. This method can be applied to an infinite number of hurt/help-like events. I hope this helped.

//Here's an incredibly basic "Hurt/Help" AI... It needs much improving before it can truly learn, but it does evaluate the effect objects have on it and react to that, it's a start.http://www.box.net/public/nstxj40y55





So could you explain to me a way to make AI learn without using pre-programmed functions?  You might think there's a way, but there isn't.  It is impossible. 


Read the http://www.nerogame.orglink.

The functions are changing in the learning process. Essentially, you tell the program what it should be good AT, not HOW to be good at it. And the program changes over and over again, until it becomes good at what you wanted it to be good at

The end result is a program that you did not write yourself. It has been created by the program you wrote, yes, but it does things in ways and in orders you never told it to do them. It will find solutions to the problem that you would never have thought of.
*



I suppose I said that wrong. What I meant was that you cannot make the AI do something differently than what you told it to do. You might be able to stick some random variables in there and create new functions, but it will never truly think on it's own, it will have to do what it's told to do.

You could even argue that Humans aren't truly independant if you think about it long enough, as most of our brains function the same way, and if someone were to go back in time (which I believe to be impossible), and not interact with anything at all, everything would be "redone" in the exact same way since we would react to everything in the same way.

But this doesn't really have much to do with the current topic. I suppose learning would mean that something changes according to what it believes would be beneficial, whether it is pre-programmed or not.

Update: I hate C++ dry.gif , but it should be done tomorrow.



If you make self-programmable code, the program will be able to do anything it wants- even turn into a virus and spread across the internet.



If you make self-programmable code, the program will be able to do anything it wants- even turn into a virus and spread across the internet.
*



But it could only do so if the programmer allowed it to do so. There is no possible way that a computer can do anything completely independant, it's just the way they work. That's like telling an object in GM to move right without you ever programming anything at all involving speed. Somewhere, you MUST tell it some sort of speed or it won't move anywhere. You couldn't have a program spread viruses across the internet if it doesn't know what the internet is or how it works.

Now, with the new DNA computers coming out, that might be different...

Update on my part: I finally got all the drawing functions down, and I've started on a cool AI sight. When I finish it, I'll make a file showing how it works, it's EXTREMELY fast and very effective. My original method was so slow that I had to move to C++, but this is so fast I might end up making a GM file after I finish the C++ one.

It's gonna take a lot longer than I thought cause I never thought there'd be so many errors, so hopefully it'll be done by Friday night. 584 lines of code and rising, hurray progress!





If you make self-programmable code, the program will be able to do anything it wants- even turn into a virus and spread across the internet.
*



But it could only do so if the programmer allowed it to do so. There is no possible way that a computer can do anything completely independant, it's just the way they work.


The basic building blocks of actions must be defined, yes. But the combining of them is not predefined. Which is exactly how a you work, too. The basic building blocks are admittedly much more complex, but your muscles and limbs pretty much sets the limits to your actions, before you even start thinking about what to do. Everything comes out as combinations of building blocks that any good doctor could list for you. In that sense, you are no more independent than the computer. It's just the way you work.

If this is off-topic, the title of the thread is wrong.

Posted by: Dr. Eechmen Aug 3 2006, 04:45 PM


If you make self-programmable code, the program will be able to do anything it wants- even turn into a virus and spread across the internet.
*



The only way this would be possible, at least, that I know of, would be by using mutations.

Mutation Process.
First Idea (in our case, coding or variable) splits into 2 or more seperate ideas, the most effective for the task at hand is stored (possible written into an ever expanding INI file as a string) then mutations are taken off of this one, and you go through "generations" of ideas until the final result is achieved. If a mutation engine went awry, you could possible end up with a virus spreading through the internet, though this is extremely unlikely. If a mutation engine was worked well enough, you could throw hypothetically it into almost any game, and it would evolve to cope with the environment, and possible beat the game. A mutation engine would be hard to build, so I'm not going to attempt to build this unless urged to by other people. If anyone has any input on mutations, I would be much obliged if it was posted in this forum, as I believe mutations are the best way to make a learning AI.

-Dr. Eechmen

Link to Mutation-related article.

http://www.popsci.com/popsci/science/0e13af26862ba010vgnvcm1000004eecbccdrcrd.html

New AI example...
http://www.box.net/public/abme4pkayj

//edit- I decided to work on a mutation sequence...It's a heck of a lot harder than I expected....I might have a basic one done sometime.... I might actually have to look up some coding from existing mutation functions to base mine off of.



pretty good, but there r some glitches,
if there's a similey face and a frowning face overlapping each other, it'll keep going back and forth, and my box got stuck on a smiley face and got like 600 health and 20 sight
Posted by: Dr. Eechmen Aug 4 2006, 01:49 PM


pretty good, but there r some glitches,
if there's a similey face and a frowning face overlapping each other, it'll keep going back and forth, and my box got stuck on a smiley face and got like 600 health and 20 sight
*



I know...It's in very basic stages of development, I need to work out movement priorities, but I have paused work on it for a bit while I attempt to create a working mutation sequence.



@Dr. Eechmen
I read the article in the link you provided. After going through it, I was seriously amazed!! But I thought of one thing when I came back to the subject. If you really want to create this kind of program, prepare to lag! Do you have any idea of what ammount of processor time it would take? It's unbelievable! The computer must not only perform code, but create new code, test it, make combinations, eliminate bad lines, run your game, all your functions which require processor time. And that's beside the huuuuuuuge ammount of memory that would be needed to deal with all this genetics... And what about the guy who have an integrated video card? LOL

You have to create a simplified version of this... But to be able to do so, you must first fully understand and use the genetic programming. I wish you luck!

PS.: Once you get a computer performant enough to do this, and finish the program itself, have it create a new OS for PC LOL
Posted by: Dr. Eechmen Aug 4 2006, 06:07 PM


@Dr. Eechmen
I read the article in the link you provided. After going through it, I was seriously amazed!! But I thought of one thing when I came back to the subject. If you really want to create this kind of program, prepare to lag! Do you have any idea of what ammount of processor time it would take? It's unbelievable! The computer must not only perform code, but create new code, test it, make combinations, eliminate bad lines, run your game, all your functions which require processor time. And that's beside the huuuuuuuge ammount of memory that would be needed to deal with all this genetics... And what about the guy who have an integrated video card? LOL

You have to create a simplified version of this... But to be able to do so, you must first fully understand and use the genetic programming. I wish you luck!

PS.: Once you get a computer performant enough to do this, and finish the program itself, have it create a new OS for PC LOL
*



Yeah, I know I have little chance of success...though If I do it would be great... And if I do, think of the possabilities.. we could throw the AI into any environment...any game, and it theroretically beat the game. But that's ony part of It... One could use this to invent things, such as in the article... now that would be a really amazing game maker creation.

Too bad I probably won't get it working tongue.gif ...

Dr. Eechmen

Horray! I managed to make a simple mutation-only Genetic programming example. It's full of bugs, but the programming is there, and it works (to some extent, download it and you'll see what I mean) I'll work on refining my mutation for the next release, but until then, here it is.
http://www.box.net/public/acslcebjns

Powered by Invision Power Board (http://www.invisionboard.com)
© Invision Power Services (http://www.invisionpower.com)
